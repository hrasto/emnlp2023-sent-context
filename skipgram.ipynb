{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmenters.structure import StructuredCorpus\n",
    "import segmenters.iterator as it\n",
    "from gensim.models import TfidfModel, LsiModel, LdaMulticore, LdaModel\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import os\n",
    "from segmenters.structure import StructuredCorpus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "import module\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label  train    dev   test  train%   dev%  test%\n",
      "0     s  87748  10656  10808   46.28  44.96  45.60\n",
      "1     b  39901   5415   5933   21.04  22.85  25.03\n",
      "2     ?  33198   4065   3140   17.51  17.15  13.25\n",
      "3     +  15184   2184   2063    8.01   9.21   8.70\n",
      "4     %  13589   1382   1759    7.17   5.83   7.42\n"
     ]
    }
   ],
   "source": [
    "dirs = [\n",
    "    'swda_mfs_100w',\n",
    "]\n",
    "\n",
    "dirname = dirs[0]\n",
    "\n",
    "ctrain = StructuredCorpus.load(f'{dirname}/corpus/train')\n",
    "ctest = StructuredCorpus.load(f'{dirname}/corpus/test')\n",
    "cdev = StructuredCorpus.load(f'{dirname}/corpus/dev')\n",
    "\n",
    "targets_train = [da[1][0] for _, da in ctrain[['default', 'act_tag']]]\n",
    "targets_dev = [da[1][0] for _, da in cdev[['default', 'act_tag']]]\n",
    "targets_test = [da[1][0] for _, da in ctest[['default', 'act_tag']]]\n",
    "\n",
    "targets_counts = dict(zip(*np.unique(targets_test, return_counts=True)))    \n",
    "targets_counts = sorted(targets_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "def simplify_target(tg, most_common=n_labels-1, unk_label='?'):\n",
    "    labels = [lab for lab, count in targets_counts[:most_common]]\n",
    "    return tg[0] if tg[0] in labels else unk_label\n",
    "\n",
    "simple_targets_train = [simplify_target(t) for t in targets_train]\n",
    "simple_targets_dev = [simplify_target(t) for t in targets_dev]\n",
    "simple_targets_test = [simplify_target(t) for t in targets_test]\n",
    "\n",
    "simple_counts_train = list(zip(*np.unique(simple_targets_train, return_counts=True)))\n",
    "simple_counts_train = sorted(simple_counts_train, key=lambda x: x[1], reverse=True)\n",
    "simple_counts_dev = dict(zip(*np.unique(simple_targets_dev, return_counts=True)))\n",
    "simple_counts_test = dict(zip(*np.unique(simple_targets_test, return_counts=True)))\n",
    "freq_table = []\n",
    "for label, count in simple_counts_train: \n",
    "    freq_table.append({'label': label, 'train': count, 'dev': simple_counts_dev[label], 'test': simple_counts_test[label]})\n",
    "freq_table = pd.DataFrame(freq_table)\n",
    "freq_table['train%'] = (freq_table['train'] / freq_table['train'].sum())*100\n",
    "freq_table['dev%'] = (freq_table['dev'] / freq_table['dev'].sum())*100\n",
    "freq_table['test%'] = (freq_table['test'] / freq_table['test'].sum())*100\n",
    "freq_table = freq_table.round(2)\n",
    "print(freq_table)\n",
    "freq_table.to_csv(f'{dirname}/label_freq.csv', sep='\\t')\n",
    "\n",
    "w2v_models=[\n",
    "    '../models-gensim/glove-twitter-25',\n",
    "    #'../models-gensim/glove-wiki-gigaword-100',\n",
    "]\n",
    "\n",
    "n_labels=5\n",
    "n_samples=5000\n",
    "classifiers = {\n",
    "    #'lr': lambda: LogisticRegression(random_state=1, max_iter=2000),\n",
    "    'lrcv10': lambda: LogisticRegressionCV(random_state=1, max_iter=10000, cv=10),\n",
    "    #'svm_rbf': lambda: svm.SVC(kernel='rbf'),\n",
    "    #'tree5': lambda: tree.DecisionTreeClassifier(random_state=1, max_depth=5),\n",
    "    'tree10': lambda: tree.DecisionTreeClassifier(random_state=1, max_depth=10),\n",
    "    #'tree20': lambda: tree.DecisionTreeClassifier(random_state=1, max_depth=20),\n",
    "    #'forest5': lambda: ensemble.RandomForestClassifier(random_state=1, max_depth=5),\n",
    "    #'forest10': lambda: ensemble.RandomForestClassifier(random_state=1, max_depth=10),\n",
    "    #'forest15': lambda: ensemble.RandomForestClassifier(random_state=1, max_depth=15),\n",
    "    #'knn10': lambda: KNeighborsClassifier(n_neighbors=10),\n",
    "    #'knn20': lambda: KNeighborsClassifier(n_neighbors=20),\n",
    "    #'knn30': lambda: KNeighborsClassifier(n_neighbors=30),\n",
    "    #'knn40': lambda: KNeighborsClassifier(n_neighbors=40),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label  train    dev   test  train%   dev%  test%\n",
      "0     s  87748  10656  10808   46.28  44.96  45.60\n",
      "1     b  39901   5415   5933   21.04  22.85  25.03\n",
      "2     ?  33198   4065   3140   17.51  17.15  13.25\n",
      "3     +  15184   2184   2063    8.01   9.21   8.70\n",
      "4     %  13589   1382   1759    7.17   5.83   7.42\n"
     ]
    }
   ],
   "source": [
    "def gensim_corpus_to_numpy(corpus):\n",
    "    x = []\n",
    "    for res in corpus:\n",
    "        vec = np.zeros(num_topics)\n",
    "        for i, val in res: vec[i] = val\n",
    "        x.append(list(vec))\n",
    "    x = np.array(x)\n",
    "    return x\n",
    "\n",
    "def corpus2wv(corpus, w2v, agg=False, dim=10, max_words=20):\n",
    "    x = []\n",
    "    for seq, labs in corpus[['default', 'act_tag']]:\n",
    "        seq = seq[:max_words]\n",
    "        seq_w2v = [w2v.key_to_index.get(ctest.idx_to_word[i], 0) for i in seq]\n",
    "        _x = np.zeros((max_words, dim))\n",
    "        for i, vec in enumerate(w2v[seq_w2v]):\n",
    "            _x[i] = vec[:dim]\n",
    "        if agg: \n",
    "            _x = _x.mean(0)\n",
    "        else: \n",
    "            _x = _x.flatten()\n",
    "        x.append(_x)\n",
    "    x = np.array(x)\n",
    "    return x \n",
    "\n",
    "dictionary = Dictionary()\n",
    "dictionary.id2token = ctest.idx_to_word\n",
    "dictionary.token2id = ctest.word_to_idx\n",
    "\n",
    "w2v = KeyedVectors.load(w2v_models[0])\n",
    "\n",
    "result_table = []\n",
    "for dim in [2, 4, 8, 16, 25]:\n",
    "    for agg in [True, False]: \n",
    "        #x_train = corpus2wv(ctrain, w2v, agg)\n",
    "        x_dev = corpus2wv(cdev, w2v, agg, dim)\n",
    "        x_test = corpus2wv(ctest, w2v, agg, dim)\n",
    "        result = {'dim': dim, 'agg': agg}        \n",
    "        for clf_name, clf_init in classifiers.items():\n",
    "            clf = clf_init()\n",
    "            clf.fit(x_dev[:n_samples], simple_targets_dev[:n_samples])\n",
    "            preds = clf.predict(x_test)\n",
    "            acc = accuracy_score(simple_targets_test, preds)\n",
    "            f1_micro = f1_score(simple_targets_test, preds, average='micro')\n",
    "            f1_macro = f1_score(simple_targets_test, preds, average='macro')\n",
    "            #print(f'{clf_name}\\tacc={acc}\\tf1_micro={f1_micro}\\tf1_macro={f1_macro}')\n",
    "            result[f'{clf_name}_acc'] = acc\n",
    "            #result[f'{clf_name}_f1ma'] = f1_macro\n",
    "            #result[f'{clf_name}_f1mi'] = f1_micro\n",
    "        result_table.append(result)\n",
    "\n",
    "result_table = pd.DataFrame(result_table)\n",
    "result_table = result_table.round(2)\n",
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim</th>\n",
       "      <th>agg</th>\n",
       "      <th>lr_acc</th>\n",
       "      <th>lrcv_acc</th>\n",
       "      <th>tree10_acc</th>\n",
       "      <th>knn20_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>True</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dim    agg  lr_acc  lrcv_acc  tree10_acc  knn20_acc\n",
       "0    2   True    0.45      0.60        0.55       0.60\n",
       "1    2  False    0.60      0.63        0.64       0.45\n",
       "2    4   True    0.54      0.58        0.54       0.59\n",
       "3    4  False    0.59      0.64        0.63       0.33\n",
       "4    8   True    0.64      0.64        0.57       0.63\n",
       "5    8  False    0.61      0.65        0.63       0.46\n",
       "6   16   True    0.66      0.66        0.61       0.64\n",
       "7   16  False    0.65      0.67        0.64       0.64\n",
       "8   25   True    0.66      0.67        0.59       0.65\n",
       "9   25  False    0.66      0.67        0.64       0.64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Form vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = module.init_embedding(nin=len(ctest), nout=4)\n",
    "word_vectors = emb.weight.detach().numpy()\n",
    "np.unique(word_vectors.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim</th>\n",
       "      <th>agg</th>\n",
       "      <th>lrcv10_acc</th>\n",
       "      <th>tree10_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dim    agg  lrcv10_acc  tree10_acc\n",
       "0    2   True        0.46        0.60\n",
       "1    2  False        0.61        0.68\n",
       "2    4   True        0.48        0.59\n",
       "3    4  False        0.61        0.66\n",
       "4    8   True        0.56        0.61\n",
       "5    8  False        0.65        0.66"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corpus2emb(corpus, word_vectors, agg, dim, max_words=20):\n",
    "    x = []\n",
    "    for seq, labs in corpus[['default', 'act_tag']]:\n",
    "        seq = seq[:max_words]\n",
    "        _x = np.zeros((max_words, dim))\n",
    "        _x[:len(seq)] = word_vectors[seq][:, :dim]\n",
    "        if agg: \n",
    "            _x = _x.mean(0)\n",
    "        else: \n",
    "            _x = _x.flatten()\n",
    "        x.append(_x)\n",
    "    x = np.array(x)\n",
    "    return x \n",
    "\n",
    "result_table = []\n",
    "for dim in [2, 4, 8]:\n",
    "    for agg in [True, False]: \n",
    "        emb = module.init_embedding(nin=len(ctest), nout=dim)\n",
    "        word_vectors = emb.weight.detach().numpy()\n",
    "\n",
    "        x_dev = corpus2emb(cdev, word_vectors, agg, dim)\n",
    "        x_test = corpus2emb(ctest, word_vectors, agg, dim)\n",
    "\n",
    "        result = {'dim':dim ,'agg':agg}\n",
    "        for clf_name, clf_init in classifiers.items():\n",
    "            clf = clf_init()\n",
    "            clf.fit(x_dev[:n_samples], simple_targets_dev[:n_samples])\n",
    "            preds = clf.predict(x_test)\n",
    "            acc = accuracy_score(simple_targets_test, preds)\n",
    "            f1_micro = f1_score(simple_targets_test, preds, average='micro')\n",
    "            f1_macro = f1_score(simple_targets_test, preds, average='macro')\n",
    "            #print(f'{clf_name}\\tacc={acc}\\tf1_micro={f1_micro}\\tf1_macro={f1_macro}')\n",
    "            result[f'{clf_name}_acc'] = acc\n",
    "            #result[f'{clf_name}_f1ma'] = f1_macro\n",
    "            #result[f'{clf_name}_f1mi'] = f1_micro\n",
    "        result_table.append(result)\n",
    "\n",
    "result_table = pd.DataFrame(result_table)\n",
    "result_table = result_table.round(2)\n",
    "result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgram, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_train = list(ctrain.derive_segment_boundaries('conversation_no', 'default'))\n",
    "segmentation_dev = list(cdev.derive_segment_boundaries('conversation_no', 'default'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_and_pad_sequence(seq, max_words=20):\n",
    "    seq = seq[:max_words]\n",
    "    res = np.repeat(ctest.word_to_idx['<PAD>'], max_words)\n",
    "    res[:len(seq)] = seq\n",
    "    return res\n",
    "\n",
    "idx_train = np.array([cut_and_pad_sequence(seq) for seq in ctrain.sequences])\n",
    "idx_dev = np.array([cut_and_pad_sequence(seq) for seq in cdev.sequences])\n",
    "idx_test = np.array([cut_and_pad_sequence(seq) for seq in ctest.sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646,     2,    80,     1, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646,     2,    80,     1, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646,     2,    80,     1, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646,     2,    80,     1, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     1, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     1, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     1, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     1, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453])) (array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    5,    50,    11,     6,  5798,   171,     4,    41,     1,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    4,     2,    23,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    5,    50,    11,     6,  5798,   171,     4,    41,     1,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646,     2,    80,     1, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646,     2,    80,     1, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     1, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     1, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646,     2,    80,     1, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([    6,     0,    12,     0, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   53,     7,    53,    35,    13,    69,    79,    14,  2214,\n",
      "           0, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([  158,    61,   646,     2,    80,     1, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   53,     7,    53,    35,    13,    69,    79,    14,  2214,\n",
      "           0, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     1, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   36,     1, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]), array([   16,     0, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453, 13453,\n",
      "       13453, 13453]))\n"
     ]
    }
   ],
   "source": [
    "def make_skipgrams(x: np.array, segmentation:list, context_size:int, limit=10000000):\n",
    "    seg_simple = np.array([-1] + segmentation + [x.shape[0]-1]) + 1\n",
    "    ct = 0\n",
    "    for left, right in zip(seg_simple[:-1], seg_simple[1:]): \n",
    "        for pivot in range(left+context_size, right-context_size):\n",
    "            for i in range(1, context_size+1):\n",
    "                if ct >= limit: \n",
    "                    return\n",
    "                yield (x[pivot], x[pivot+i])\n",
    "                yield (x[pivot], x[pivot-i])\n",
    "                ct+=2\n",
    "\n",
    "context_size=2\n",
    "batch_size=32\n",
    "\n",
    "batches_train = it.RestartableCallableIterator(make_skipgrams, fn_args=[idx_train, segmentation_train, context_size])\n",
    "batches_train = it.RestartableBatchIterator(batches_train, batch_size=batch_size)\n",
    "batches_dev = it.RestartableBatchIterator(batches_dev, batch_size=batch_size)\n",
    "batches_dev = it.RestartableCallableIterator(make_skipgrams, fn_args=[idx_dev, segmentation_dev, context_size, 32*100])\n",
    "batches_dev = it.RestartableBatchIterator(batches_dev, batch_size=batch_size)\n",
    "print(*zip(*next(iter(batches_dev))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceSG(module.ReconstructionModel):\n",
    "    def __init__(self, dim_token, dim_latent, hyperparams={}, emb_layer=None):\n",
    "        super().__init__()\n",
    "        self.emb_layer = emb_layer\n",
    "        self.encoder = module.LSTMEncoder(dim_token, dim_latent, **hyperparams)\n",
    "        decoder_lstm = module.LSTMEncoder(dim_token+dim_latent, dim_token, **hyperparams)\n",
    "        self.decoder = module.TurboSequencer(decoder_lstm, dim_token, dim_latent)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        pivot, neighbor = zip(*batch)\n",
    "        if self.emb_layer is None: \n",
    "            pivot = T(pivot).float()\n",
    "            neighbor = T(neighbor).float()\n",
    "        else: \n",
    "            pivot = T(pivot).long()\n",
    "            neighbor = T(neighbor).long()\n",
    "            pivot = self.emb_layer(pivot)\n",
    "            neighbor = self.emb_layer(neighbor)\n",
    "        pivot = pivot.transpose(0, 1).contiguous()\n",
    "        neighbor = neighbor.transpose(0, 1).contiguous()\n",
    "        lat = self.encoder(pivot)\n",
    "        neighbor_hat = self.decoder.decode(\n",
    "            x_static=lat, \n",
    "            teacher_force_y=neighbor,\n",
    "            #decoding_steps=\n",
    "            )\n",
    "        return neighbor_hat, neighbor\n",
    "    \n",
    "    def loss_fn(self, neighbor_hat, neighbor):\n",
    "        return F.mse_loss(neighbor_hat, neighbor, reduction='mean')\n",
    "\n",
    "dim_latent = 8\n",
    "dim_token = 8\n",
    "emb_layer = module.init_embedding(len(ctest), dim_token)\n",
    "hyperparams = {\n",
    "    'bidirectional': True, \n",
    "    'hidden_size': 64,\n",
    "    'num_layers': 2,\n",
    "    'dropout': .1,\n",
    "}\n",
    "model = SequenceSG(dim_token, dim_latent, hyperparams, emb_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using a test set of 100 batches (BS=32)\n",
      "Epoch 00\tBatch 10\t0.1601s/batch\ttrain_loss = 12.4807\n",
      "Epoch 00\tBatch 20\t0.1565s/batch\ttrain_loss = 10.3378\n",
      "Epoch 00\tBatch 30\t0.1526s/batch\ttrain_loss = 9.2731\n",
      "Epoch 00\tBatch 30\ttest_loss = 7.2454\n",
      "new best loss (inf -> 7.245416)\n",
      "Epoch 00\tBatch 40\t0.1522s/batch\ttrain_loss = 7.7006\n",
      "Epoch 00\tBatch 50\t0.1541s/batch\ttrain_loss = 7.9230\n",
      "Epoch 00\tBatch 60\t0.1630s/batch\ttrain_loss = 8.2300\n",
      "Epoch 00\tBatch 60\ttest_loss = 6.6803\n",
      "new best loss (7.245416 -> 6.680335)\n",
      "Epoch 00\tBatch 70\t0.1520s/batch\ttrain_loss = 8.0776\n",
      "Epoch 00\tBatch 80\t0.1514s/batch\ttrain_loss = 6.8082\n",
      "Epoch 00\tBatch 90\t0.1514s/batch\ttrain_loss = 7.3202\n",
      "Epoch 00\tBatch 90\ttest_loss = 6.4707\n",
      "new best loss (6.680335 -> 6.470734)\n",
      "Epoch 00\tBatch 100\t0.1515s/batch\ttrain_loss = 8.4447\n",
      "Epoch 00\tBatch 110\t0.1503s/batch\ttrain_loss = 7.3512\n",
      "Epoch 00\tBatch 120\t0.1498s/batch\ttrain_loss = 7.3515\n",
      "Epoch 00\tBatch 120\ttest_loss = 6.3187\n",
      "new best loss (6.470734 -> 6.318651)\n",
      "Epoch 00\tBatch 130\t0.1491s/batch\ttrain_loss = 5.7480\n",
      "Epoch 00\tBatch 140\t0.1499s/batch\ttrain_loss = 6.4073\n",
      "Epoch 00\tBatch 150\t0.1505s/batch\ttrain_loss = 7.2177\n",
      "Epoch 00\tBatch 150\ttest_loss = 6.2253\n",
      "new best loss (6.318651 -> 6.225260)\n",
      "Epoch 00\tBatch 160\t0.1518s/batch\ttrain_loss = 8.2300\n",
      "Epoch 00\tBatch 170\t0.1491s/batch\ttrain_loss = 6.2008\n",
      "Epoch 00\tBatch 180\t0.1503s/batch\ttrain_loss = 7.0174\n",
      "Epoch 00\tBatch 180\ttest_loss = 6.1962\n",
      "new best loss (6.225260 -> 6.196220)\n",
      "Epoch 00\tBatch 190\t0.1485s/batch\ttrain_loss = 5.7369\n",
      "Epoch 00\tBatch 200\t0.1488s/batch\ttrain_loss = 5.7520\n",
      "Epoch 00\tBatch 210\t0.1471s/batch\ttrain_loss = 5.3836\n",
      "Epoch 00\tBatch 210\ttest_loss = 6.1758\n",
      "new best loss (6.196220 -> 6.175847)\n",
      "Epoch 00\tBatch 220\t0.1496s/batch\ttrain_loss = 7.7028\n",
      "Epoch 00\tBatch 230\t0.1490s/batch\ttrain_loss = 6.2701\n",
      "Epoch 00\tBatch 240\t0.1478s/batch\ttrain_loss = 5.9573\n",
      "Epoch 00\tBatch 240\ttest_loss = 6.1998\n",
      "Epoch 00\tBatch 250\t0.1479s/batch\ttrain_loss = 5.7880\n",
      "Epoch 00\tBatch 260\t0.1491s/batch\ttrain_loss = 6.5735\n",
      "Epoch 00\tBatch 270\t0.1494s/batch\ttrain_loss = 6.6607\n",
      "Epoch 00\tBatch 270\ttest_loss = 6.1400\n",
      "new best loss (6.175847 -> 6.139989)\n",
      "Epoch 00\tBatch 280\t0.1479s/batch\ttrain_loss = 5.6523\n",
      "Epoch 00\tBatch 290\t0.1483s/batch\ttrain_loss = 6.0389\n",
      "Epoch 00\tBatch 300\t0.1488s/batch\ttrain_loss = 6.8321\n",
      "Epoch 00\tBatch 300\ttest_loss = 6.1299\n",
      "new best loss (6.139989 -> 6.129893)\n",
      "Epoch 00\tBatch 310\t0.1497s/batch\ttrain_loss = 6.5978\n",
      "Epoch 00\tBatch 320\t0.1494s/batch\ttrain_loss = 6.2785\n",
      "Epoch 00\tBatch 330\t0.1496s/batch\ttrain_loss = 7.2438\n",
      "Epoch 00\tBatch 330\ttest_loss = 6.1227\n",
      "Epoch 00\tBatch 340\t0.1484s/batch\ttrain_loss = 6.0229\n",
      "Epoch 00\tBatch 350\t0.1482s/batch\ttrain_loss = 6.2452\n",
      "Epoch 00\tBatch 360\t0.1472s/batch\ttrain_loss = 5.1693\n",
      "Epoch 00\tBatch 360\ttest_loss = 6.1011\n",
      "new best loss (6.129893 -> 6.101122)\n",
      "Epoch 00\tBatch 370\t0.1484s/batch\ttrain_loss = 6.3180\n",
      "Epoch 00\tBatch 380\t0.1482s/batch\ttrain_loss = 4.9093\n",
      "Epoch 00\tBatch 390\t0.1489s/batch\ttrain_loss = 5.5949\n",
      "Epoch 00\tBatch 390\ttest_loss = 6.1072\n",
      "Epoch 00\tBatch 400\t0.1484s/batch\ttrain_loss = 6.5243\n",
      "Epoch 00\tBatch 410\t0.1490s/batch\ttrain_loss = 7.9716\n",
      "Epoch 00\tBatch 420\t0.1500s/batch\ttrain_loss = 8.2632\n",
      "Epoch 00\tBatch 420\ttest_loss = 6.0807\n",
      "new best loss (6.101122 -> 6.080706)\n",
      "Epoch 00\tBatch 430\t0.1477s/batch\ttrain_loss = 5.8374\n",
      "Epoch 00\tBatch 440\t0.1493s/batch\ttrain_loss = 6.1348\n",
      "Epoch 00\tBatch 450\t0.1493s/batch\ttrain_loss = 6.8852\n",
      "Epoch 00\tBatch 450\ttest_loss = 6.0679\n",
      "new best loss (6.080706 -> 6.067921)\n",
      "Epoch 00\tBatch 460\t0.1482s/batch\ttrain_loss = 5.6817\n",
      "Epoch 00\tBatch 470\t0.1486s/batch\ttrain_loss = 6.3655\n",
      "Epoch 00\tBatch 480\t0.1470s/batch\ttrain_loss = 4.9469\n",
      "Epoch 00\tBatch 480\ttest_loss = 6.0585\n",
      "Epoch 00\tBatch 490\t0.1484s/batch\ttrain_loss = 5.8506\n",
      "Epoch 00\tBatch 500\t0.1482s/batch\ttrain_loss = 6.2219\n",
      "Epoch 00\tBatch 510\t0.1479s/batch\ttrain_loss = 5.3244\n",
      "Epoch 00\tBatch 510\ttest_loss = 6.0528\n",
      "new best loss (6.067921 -> 6.052823)\n",
      "Epoch 00\tBatch 520\t0.1484s/batch\ttrain_loss = 5.7172\n",
      "Epoch 00\tBatch 530\t0.1487s/batch\ttrain_loss = 5.4873\n",
      "Epoch 00\tBatch 540\t0.1490s/batch\ttrain_loss = 6.6290\n",
      "Epoch 00\tBatch 540\ttest_loss = 6.0258\n",
      "new best loss (6.052823 -> 6.025759)\n",
      "Epoch 00\tBatch 550\t0.1487s/batch\ttrain_loss = 5.7500\n",
      "Epoch 00\tBatch 560\t0.1492s/batch\ttrain_loss = 7.3592\n",
      "Epoch 00\tBatch 570\t0.1478s/batch\ttrain_loss = 5.5296\n",
      "Epoch 00\tBatch 570\ttest_loss = 5.9947\n",
      "new best loss (6.025759 -> 5.994682)\n",
      "Epoch 00\tBatch 580\t0.1487s/batch\ttrain_loss = 6.4062\n",
      "Epoch 00\tBatch 590\t0.1487s/batch\ttrain_loss = 6.4561\n",
      "Epoch 00\tBatch 600\t0.1490s/batch\ttrain_loss = 7.4930\n",
      "Epoch 00\tBatch 600\ttest_loss = 6.0046\n",
      "Epoch 00\tBatch 610\t0.1491s/batch\ttrain_loss = 6.7018\n",
      "Epoch 00\tBatch 620\t0.1490s/batch\ttrain_loss = 6.1473\n",
      "Epoch 00\tBatch 630\t0.1488s/batch\ttrain_loss = 6.3533\n",
      "Epoch 00\tBatch 630\ttest_loss = 5.9811\n",
      "new best loss (5.994682 -> 5.981148)\n",
      "Epoch 00\tBatch 640\t0.1476s/batch\ttrain_loss = 6.1020\n",
      "Epoch 00\tBatch 650\t0.1486s/batch\ttrain_loss = 6.0364\n",
      "Epoch 00\tBatch 660\t0.1496s/batch\ttrain_loss = 7.2958\n",
      "Epoch 00\tBatch 660\ttest_loss = 5.9348\n",
      "new best loss (5.981148 -> 5.934836)\n",
      "Epoch 00\tBatch 670\t0.1484s/batch\ttrain_loss = 6.5857\n",
      "Epoch 00\tBatch 680\t0.1481s/batch\ttrain_loss = 6.2780\n",
      "Epoch 00\tBatch 690\t0.1480s/batch\ttrain_loss = 5.4695\n",
      "Epoch 00\tBatch 690\ttest_loss = 5.9300\n",
      "Epoch 00\tBatch 700\t0.1478s/batch\ttrain_loss = 5.3639\n",
      "Epoch 00\tBatch 710\t0.1483s/batch\ttrain_loss = 5.4357\n",
      "Epoch 00\tBatch 720\t0.1484s/batch\ttrain_loss = 6.0930\n",
      "Epoch 00\tBatch 720\ttest_loss = 5.9217\n",
      "new best loss (5.934836 -> 5.921683)\n",
      "Epoch 00\tBatch 730\t0.1482s/batch\ttrain_loss = 6.1817\n",
      "Epoch 00\tBatch 740\t0.1485s/batch\ttrain_loss = 5.2831\n",
      "Epoch 00\tBatch 750\t0.1468s/batch\ttrain_loss = 5.3324\n",
      "Epoch 00\tBatch 750\ttest_loss = 5.9054\n",
      "new best loss (5.921683 -> 5.905445)\n",
      "Epoch 00\tBatch 760\t0.1496s/batch\ttrain_loss = 7.5847\n",
      "Epoch 00\tBatch 770\t0.1498s/batch\ttrain_loss = 8.0406\n",
      "Epoch 00\tBatch 780\t0.1494s/batch\ttrain_loss = 7.8396\n",
      "Epoch 00\tBatch 780\ttest_loss = 5.8999\n",
      "Epoch 00\tBatch 790\t0.1487s/batch\ttrain_loss = 7.2627\n",
      "Epoch 00\tBatch 800\t0.1479s/batch\ttrain_loss = 6.0067\n",
      "Epoch 00\tBatch 810\t0.1474s/batch\ttrain_loss = 6.0007\n",
      "Epoch 00\tBatch 810\ttest_loss = 5.9110\n",
      "Epoch 00\tBatch 820\t0.1470s/batch\ttrain_loss = 5.5905\n",
      "Epoch 00\tBatch 830\t0.1473s/batch\ttrain_loss = 5.9732\n",
      "Epoch 00\tBatch 840\t0.1476s/batch\ttrain_loss = 4.9911\n",
      "Epoch 00\tBatch 840\ttest_loss = 5.8813\n",
      "new best loss (5.905445 -> 5.881325)\n",
      "Epoch 00\tBatch 850\t0.1473s/batch\ttrain_loss = 5.5094\n",
      "Epoch 00\tBatch 860\t0.1479s/batch\ttrain_loss = 6.0414\n",
      "Epoch 00\tBatch 870\t0.1491s/batch\ttrain_loss = 7.8113\n",
      "Epoch 00\tBatch 870\ttest_loss = 5.8809\n",
      "Epoch 00\tBatch 880\t0.1470s/batch\ttrain_loss = 5.4752\n",
      "Epoch 00\tBatch 890\t0.1475s/batch\ttrain_loss = 6.1938\n",
      "Epoch 00\tBatch 900\t0.1474s/batch\ttrain_loss = 5.6777\n",
      "Epoch 00\tBatch 900\ttest_loss = 5.8409\n",
      "new best loss (5.881325 -> 5.840917)\n",
      "Epoch 00\tBatch 910\t0.1536s/batch\ttrain_loss = 6.1601\n",
      "Epoch 00\tBatch 920\t0.1497s/batch\ttrain_loss = 6.7283\n",
      "Epoch 00\tBatch 930\t0.1494s/batch\ttrain_loss = 7.5891\n",
      "Epoch 00\tBatch 930\ttest_loss = 5.8466\n",
      "Epoch 00\tBatch 940\t0.1486s/batch\ttrain_loss = 5.1329\n",
      "Epoch 00\tBatch 950\t0.1498s/batch\ttrain_loss = 6.7897\n",
      "Epoch 00\tBatch 960\t0.1495s/batch\ttrain_loss = 5.2795\n",
      "Epoch 00\tBatch 960\ttest_loss = 5.8330\n",
      "Epoch 00\tBatch 970\t0.1469s/batch\ttrain_loss = 5.2758\n",
      "Epoch 00\tBatch 980\t0.1481s/batch\ttrain_loss = 5.4096\n",
      "Epoch 00\tBatch 990\t0.1467s/batch\ttrain_loss = 4.7843\n",
      "Epoch 00\tBatch 990\ttest_loss = 5.8313\n",
      "Epoch 00\tBatch 1000\t0.1496s/batch\ttrain_loss = 4.3390\n",
      "Epoch 00\tBatch 1010\t0.1494s/batch\ttrain_loss = 4.7165\n",
      "Epoch 00\tBatch 1020\t0.1506s/batch\ttrain_loss = 6.1130\n",
      "Epoch 00\tBatch 1020\ttest_loss = 5.8216\n",
      "new best loss (5.840917 -> 5.821564)\n",
      "Epoch 00\tBatch 1030\t0.1503s/batch\ttrain_loss = 5.3760\n",
      "Epoch 00\tBatch 1040\t0.1498s/batch\ttrain_loss = 6.3972\n",
      "Epoch 00\tBatch 1050\t0.1502s/batch\ttrain_loss = 6.2050\n",
      "Epoch 00\tBatch 1050\ttest_loss = 5.8296\n",
      "Epoch 00\tBatch 1060\t0.1485s/batch\ttrain_loss = 6.4630\n",
      "Epoch 00\tBatch 1070\t0.1503s/batch\ttrain_loss = 6.1201\n",
      "Epoch 00\tBatch 1080\t0.1499s/batch\ttrain_loss = 6.1329\n",
      "Epoch 00\tBatch 1080\ttest_loss = 5.8286\n",
      "Epoch 00\tBatch 1090\t0.1503s/batch\ttrain_loss = 5.6595\n",
      "Epoch 00\tBatch 1100\t0.1488s/batch\ttrain_loss = 5.2902\n",
      "Epoch 00\tBatch 1110\t0.1496s/batch\ttrain_loss = 5.1062\n",
      "Epoch 00\tBatch 1110\ttest_loss = 5.7872\n",
      "new best loss (5.821564 -> 5.787161)\n",
      "Epoch 00\tBatch 1120\t0.1508s/batch\ttrain_loss = 5.7915\n",
      "Epoch 00\tBatch 1130\t0.1517s/batch\ttrain_loss = 7.0239\n",
      "Epoch 00\tBatch 1140\t0.1512s/batch\ttrain_loss = 6.9384\n",
      "Epoch 00\tBatch 1140\ttest_loss = 5.8035\n",
      "Epoch 00\tBatch 1150\t0.1517s/batch\ttrain_loss = 6.8202\n",
      "Epoch 00\tBatch 1160\t0.1505s/batch\ttrain_loss = 6.9986\n",
      "Epoch 00\tBatch 1170\t0.1489s/batch\ttrain_loss = 5.1802\n",
      "Epoch 00\tBatch 1170\ttest_loss = 5.7627\n",
      "new best loss (5.787161 -> 5.762727)\n",
      "Epoch 00\tBatch 1180\t0.1502s/batch\ttrain_loss = 6.3117\n",
      "Epoch 00\tBatch 1190\t0.1505s/batch\ttrain_loss = 6.9898\n",
      "Epoch 00\tBatch 1200\t0.1506s/batch\ttrain_loss = 6.6318\n",
      "Epoch 00\tBatch 1200\ttest_loss = 5.7815\n",
      "Epoch 00\tBatch 1210\t0.1501s/batch\ttrain_loss = 6.7247\n",
      "Epoch 00\tBatch 1220\t0.1495s/batch\ttrain_loss = 6.1216\n",
      "Epoch 00\tBatch 1230\t0.1494s/batch\ttrain_loss = 6.6144\n",
      "Epoch 00\tBatch 1230\ttest_loss = 5.7606\n",
      "Epoch 00\tBatch 1240\t0.1490s/batch\ttrain_loss = 4.8517\n",
      "Epoch 00\tBatch 1250\t0.1501s/batch\ttrain_loss = 5.8298\n",
      "Epoch 00\tBatch 1260\t0.1496s/batch\ttrain_loss = 5.1788\n",
      "Epoch 00\tBatch 1260\ttest_loss = 5.7638\n",
      "Epoch 00\tBatch 1270\t0.1474s/batch\ttrain_loss = 5.0660\n",
      "Epoch 00\tBatch 1280\t0.1480s/batch\ttrain_loss = 5.7806\n",
      "Epoch 00\tBatch 1290\t0.1500s/batch\ttrain_loss = 6.1826\n",
      "Epoch 00\tBatch 1290\ttest_loss = 5.7397\n",
      "new best loss (5.762727 -> 5.739674)\n",
      "Epoch 00\tBatch 1300\t0.1536s/batch\ttrain_loss = 6.0213\n",
      "Epoch 00\tBatch 1310\t0.1490s/batch\ttrain_loss = 6.4917\n",
      "Epoch 00\tBatch 1320\t0.1507s/batch\ttrain_loss = 7.9911\n",
      "Epoch 00\tBatch 1320\ttest_loss = 5.7373\n",
      "Epoch 00\tBatch 1330\t0.1479s/batch\ttrain_loss = 5.8570\n",
      "Epoch 00\tBatch 1340\t0.1494s/batch\ttrain_loss = 6.4275\n",
      "Epoch 00\tBatch 1350\t0.1508s/batch\ttrain_loss = 7.2493\n",
      "Epoch 00\tBatch 1350\ttest_loss = 5.7411\n",
      "Epoch 00\tBatch 1360\t0.1506s/batch\ttrain_loss = 5.7070\n",
      "Epoch 00\tBatch 1370\t0.1500s/batch\ttrain_loss = 5.5328\n",
      "Epoch 00\tBatch 1380\t0.1501s/batch\ttrain_loss = 5.8270\n",
      "Epoch 00\tBatch 1380\ttest_loss = 5.7251\n",
      "new best loss (5.739674 -> 5.725050)\n",
      "Epoch 00\tBatch 1390\t0.1487s/batch\ttrain_loss = 5.3248\n",
      "Epoch 00\tBatch 1400\t0.1508s/batch\ttrain_loss = 7.1498\n",
      "Epoch 00\tBatch 1410\t0.1510s/batch\ttrain_loss = 6.3242\n",
      "Epoch 00\tBatch 1410\ttest_loss = 5.7449\n",
      "Epoch 00\tBatch 1420\t0.1511s/batch\ttrain_loss = 7.4514\n",
      "Epoch 00\tBatch 1430\t0.1515s/batch\ttrain_loss = 8.0106\n",
      "Epoch 00\tBatch 1440\t0.1615s/batch\ttrain_loss = 8.3646\n",
      "Epoch 00\tBatch 1440\ttest_loss = 5.7148\n",
      "new best loss (5.725050 -> 5.714792)\n",
      "Epoch 00\tBatch 1450\t0.1499s/batch\ttrain_loss = 5.1771\n",
      "Epoch 00\tBatch 1460\t0.1503s/batch\ttrain_loss = 6.9001\n",
      "Epoch 00\tBatch 1470\t0.1499s/batch\ttrain_loss = 6.6343\n",
      "Epoch 00\tBatch 1470\ttest_loss = 5.7130\n",
      "Epoch 00\tBatch 1480\t0.1502s/batch\ttrain_loss = 7.1676\n",
      "Epoch 00\tBatch 1490\t0.1505s/batch\ttrain_loss = 6.2416\n",
      "Epoch 00\tBatch 1500\t0.1496s/batch\ttrain_loss = 5.7009\n",
      "Epoch 00\tBatch 1500\ttest_loss = 5.7151\n",
      "Epoch 00\tBatch 1510\t0.1494s/batch\ttrain_loss = 6.5189\n",
      "Epoch 00\tBatch 1520\t0.1485s/batch\ttrain_loss = 5.9118\n",
      "Epoch 00\tBatch 1530\t0.1511s/batch\ttrain_loss = 6.7603\n",
      "Epoch 00\tBatch 1530\ttest_loss = 5.7327\n",
      "Epoch 00\tBatch 1540\t0.1485s/batch\ttrain_loss = 6.9159\n",
      "Epoch 00\tBatch 1550\t0.1497s/batch\ttrain_loss = 6.8739\n",
      "Epoch 00\tBatch 1560\t0.1487s/batch\ttrain_loss = 5.9077\n",
      "Epoch 00\tBatch 1560\ttest_loss = 5.6998\n",
      "new best loss (5.714792 -> 5.699819)\n",
      "Epoch 00\tBatch 1570\t0.1493s/batch\ttrain_loss = 6.5467\n",
      "Epoch 00\tBatch 1580\t0.1499s/batch\ttrain_loss = 7.4262\n",
      "Epoch 00\tBatch 1590\t0.1492s/batch\ttrain_loss = 5.9907\n",
      "Epoch 00\tBatch 1590\ttest_loss = 5.6907\n",
      "Epoch 00\tBatch 1600\t0.1504s/batch\ttrain_loss = 6.6840\n",
      "Epoch 00\tBatch 1610\t0.1490s/batch\ttrain_loss = 6.1792\n",
      "Epoch 00\tBatch 1620\t0.1489s/batch\ttrain_loss = 6.0707\n",
      "Epoch 00\tBatch 1620\ttest_loss = 5.6875\n",
      "new best loss (5.699819 -> 5.687507)\n",
      "Epoch 00\tBatch 1630\t0.1499s/batch\ttrain_loss = 5.6684\n",
      "Epoch 00\tBatch 1640\t0.1498s/batch\ttrain_loss = 5.6370\n",
      "Epoch 00\tBatch 1650\t0.1500s/batch\ttrain_loss = 5.7484\n",
      "Epoch 00\tBatch 1650\ttest_loss = 5.6999\n",
      "Epoch 00\tBatch 1660\t0.1489s/batch\ttrain_loss = 5.1680\n",
      "Epoch 00\tBatch 1670\t0.1543s/batch\ttrain_loss = 5.8558\n",
      "Epoch 00\tBatch 1680\t0.1500s/batch\ttrain_loss = 6.7860\n",
      "Epoch 00\tBatch 1680\ttest_loss = 5.6765\n",
      "new best loss (5.687507 -> 5.676527)\n",
      "Epoch 00\tBatch 1690\t0.1517s/batch\ttrain_loss = 5.9070\n",
      "Epoch 00\tBatch 1700\t0.1498s/batch\ttrain_loss = 6.0424\n",
      "Epoch 00\tBatch 1710\t0.1507s/batch\ttrain_loss = 6.5279\n",
      "Epoch 00\tBatch 1710\ttest_loss = 5.6831\n",
      "Epoch 00\tBatch 1720\t0.1502s/batch\ttrain_loss = 5.7520\n",
      "Epoch 00\tBatch 1730\t0.1496s/batch\ttrain_loss = 6.4374\n",
      "Epoch 00\tBatch 1740\t0.1504s/batch\ttrain_loss = 5.7312\n",
      "Epoch 00\tBatch 1740\ttest_loss = 5.6904\n",
      "Epoch 00\tBatch 1750\t0.1514s/batch\ttrain_loss = 5.9793\n",
      "Epoch 00\tBatch 1760\t0.1494s/batch\ttrain_loss = 5.2752\n",
      "Epoch 00\tBatch 1770\t0.1494s/batch\ttrain_loss = 4.7962\n",
      "Epoch 00\tBatch 1770\ttest_loss = 5.6782\n",
      "Epoch 00\tBatch 1780\t0.1503s/batch\ttrain_loss = 5.9996\n",
      "Epoch 00\tBatch 1790\t0.1495s/batch\ttrain_loss = 5.5780\n",
      "Epoch 00\tBatch 1800\t0.1507s/batch\ttrain_loss = 7.0448\n",
      "Epoch 00\tBatch 1800\ttest_loss = 5.6605\n",
      "new best loss (5.676527 -> 5.660549)\n",
      "Epoch 00\tBatch 1810\t0.1507s/batch\ttrain_loss = 6.0262\n",
      "Epoch 00\tBatch 1820\t0.1503s/batch\ttrain_loss = 5.9286\n",
      "Epoch 00\tBatch 1830\t0.1507s/batch\ttrain_loss = 6.4476\n",
      "Epoch 00\tBatch 1830\ttest_loss = 5.6703\n",
      "Epoch 00\tBatch 1840\t0.1610s/batch\ttrain_loss = 5.7811\n",
      "Epoch 00\tBatch 1850\t0.1534s/batch\ttrain_loss = 5.9057\n",
      "Epoch 00\tBatch 1860\t0.1473s/batch\ttrain_loss = 5.8010\n",
      "Epoch 00\tBatch 1860\ttest_loss = 5.6780\n",
      "Epoch 00\tBatch 1870\t0.1510s/batch\ttrain_loss = 5.8100\n",
      "Epoch 00\tBatch 1880\t0.1507s/batch\ttrain_loss = 6.5717\n",
      "Epoch 00\tBatch 1890\t0.1513s/batch\ttrain_loss = 7.1446\n",
      "Epoch 00\tBatch 1890\ttest_loss = 5.6739\n",
      "Epoch 00\tBatch 1900\t0.1499s/batch\ttrain_loss = 5.6700\n",
      "Epoch 00\tBatch 1910\t0.1500s/batch\ttrain_loss = 5.9275\n",
      "Epoch 00\tBatch 1920\t0.1509s/batch\ttrain_loss = 6.0796\n",
      "Epoch 00\tBatch 1920\ttest_loss = 5.6618\n",
      "Epoch 00\tBatch 1930\t0.1499s/batch\ttrain_loss = 5.4203\n",
      "Epoch 00\tBatch 1940\t0.1505s/batch\ttrain_loss = 6.1038\n",
      "Epoch 00\tBatch 1950\t0.1506s/batch\ttrain_loss = 6.5642\n",
      "Epoch 00\tBatch 1950\ttest_loss = 5.6503\n",
      "new best loss (5.660549 -> 5.650344)\n",
      "Epoch 00\tBatch 1960\t0.1501s/batch\ttrain_loss = 6.5846\n",
      "Epoch 00\tBatch 1970\t0.1510s/batch\ttrain_loss = 7.1530\n",
      "Epoch 00\tBatch 1980\t0.1507s/batch\ttrain_loss = 6.8057\n",
      "Epoch 00\tBatch 1980\ttest_loss = 5.6530\n",
      "Epoch 00\tBatch 1990\t0.1515s/batch\ttrain_loss = 5.8519\n",
      "Epoch 00\tBatch 2000\t0.1508s/batch\ttrain_loss = 7.1509\n",
      "Epoch 00\tBatch 2010\t0.1496s/batch\ttrain_loss = 6.9460\n",
      "Epoch 00\tBatch 2010\ttest_loss = 5.6625\n",
      "Epoch 00\tBatch 2020\t0.1495s/batch\ttrain_loss = 5.1172\n",
      "Epoch 00\tBatch 2030\t0.1486s/batch\ttrain_loss = 4.5884\n",
      "Epoch 00\tBatch 2040\t0.1490s/batch\ttrain_loss = 5.3924\n",
      "Epoch 00\tBatch 2040\ttest_loss = 5.6460\n",
      "Epoch 00\tBatch 2050\t0.1479s/batch\ttrain_loss = 4.8731\n",
      "Epoch 00\tBatch 2060\t0.1505s/batch\ttrain_loss = 5.5959\n",
      "Epoch 00\tBatch 2070\t0.1494s/batch\ttrain_loss = 5.2829\n",
      "Epoch 00\tBatch 2070\ttest_loss = 5.6297\n",
      "new best loss (5.650344 -> 5.629683)\n",
      "Epoch 00\tBatch 2080\t0.1496s/batch\ttrain_loss = 5.2675\n",
      "Epoch 00\tBatch 2090\t0.1500s/batch\ttrain_loss = 5.7968\n",
      "Epoch 00\tBatch 2100\t0.1503s/batch\ttrain_loss = 5.4586\n",
      "Epoch 00\tBatch 2100\ttest_loss = 5.6597\n",
      "Epoch 00\tBatch 2110\t0.1500s/batch\ttrain_loss = 4.8971\n",
      "Epoch 00\tBatch 2120\t0.1517s/batch\ttrain_loss = 5.6800\n",
      "Epoch 00\tBatch 2130\t0.1496s/batch\ttrain_loss = 4.2113\n",
      "Epoch 00\tBatch 2130\ttest_loss = 5.6282\n",
      "Epoch 00\tBatch 2140\t0.1495s/batch\ttrain_loss = 5.0355\n",
      "Epoch 00\tBatch 2150\t0.1499s/batch\ttrain_loss = 4.7598\n",
      "Epoch 00\tBatch 2160\t0.1499s/batch\ttrain_loss = 5.0337\n",
      "Epoch 00\tBatch 2160\ttest_loss = 5.6268\n",
      "Epoch 00\tBatch 2170\t0.1502s/batch\ttrain_loss = 6.3845\n",
      "Epoch 00\tBatch 2180\t0.1552s/batch\ttrain_loss = 5.9792\n",
      "Epoch 00\tBatch 2190\t0.1504s/batch\ttrain_loss = 5.8108\n",
      "Epoch 00\tBatch 2190\ttest_loss = 5.6087\n",
      "new best loss (5.629683 -> 5.608689)\n",
      "Epoch 00\tBatch 2200\t0.1481s/batch\ttrain_loss = 5.0584\n",
      "Epoch 00\tBatch 2210\t0.1500s/batch\ttrain_loss = 5.5946\n",
      "Epoch 00\tBatch 2220\t0.1484s/batch\ttrain_loss = 4.5081\n",
      "Epoch 00\tBatch 2220\ttest_loss = 5.6215\n",
      "Epoch 00\tBatch 2230\t0.1502s/batch\ttrain_loss = 5.9758\n",
      "Epoch 00\tBatch 2240\t0.1507s/batch\ttrain_loss = 6.6848\n",
      "Epoch 00\tBatch 2250\t0.1502s/batch\ttrain_loss = 7.7408\n",
      "Epoch 00\tBatch 2250\ttest_loss = 5.6312\n",
      "Epoch 00\tBatch 2260\t0.1491s/batch\ttrain_loss = 6.2744\n",
      "Epoch 00\tBatch 2270\t0.1502s/batch\ttrain_loss = 6.0850\n",
      "Epoch 00\tBatch 2280\t0.1502s/batch\ttrain_loss = 6.3488\n",
      "Epoch 00\tBatch 2280\ttest_loss = 5.6083\n",
      "Epoch 00\tBatch 2290\t0.1491s/batch\ttrain_loss = 6.2269\n",
      "Epoch 00\tBatch 2300\t0.1479s/batch\ttrain_loss = 4.5047\n",
      "Epoch 00\tBatch 2310\t0.1494s/batch\ttrain_loss = 6.0211\n",
      "Epoch 00\tBatch 2310\ttest_loss = 5.6181\n",
      "Epoch 00\tBatch 2320\t0.1493s/batch\ttrain_loss = 5.6454\n",
      "Epoch 00\tBatch 2330\t0.1502s/batch\ttrain_loss = 6.4436\n",
      "Epoch 00\tBatch 2340\t0.1495s/batch\ttrain_loss = 5.5184\n",
      "Epoch 00\tBatch 2340\ttest_loss = 5.6157\n",
      "Epoch 00\tBatch 2350\t0.1507s/batch\ttrain_loss = 5.9550\n",
      "Epoch 00\tBatch 2360\t0.1489s/batch\ttrain_loss = 6.0385\n",
      "Epoch 00\tBatch 2370\t0.1544s/batch\ttrain_loss = 5.1058\n",
      "Epoch 00\tBatch 2370\ttest_loss = 5.6728\n",
      "Epoch 00\tBatch 2380\t0.1499s/batch\ttrain_loss = 5.3567\n",
      "Epoch 00\tBatch 2390\t0.1489s/batch\ttrain_loss = 5.3168\n",
      "Epoch 00\tBatch 2400\t0.1497s/batch\ttrain_loss = 5.6912\n",
      "Epoch 00\tBatch 2400\ttest_loss = 5.5955\n",
      "new best loss (5.608689 -> 5.595487)\n",
      "Epoch 00\tBatch 2410\t0.1507s/batch\ttrain_loss = 7.1569\n",
      "Epoch 00\tBatch 2420\t0.1493s/batch\ttrain_loss = 6.2959\n",
      "Epoch 00\tBatch 2430\t0.1504s/batch\ttrain_loss = 7.3759\n",
      "Epoch 00\tBatch 2430\ttest_loss = 5.5971\n",
      "Epoch 00\tBatch 2440\t0.1504s/batch\ttrain_loss = 8.0414\n",
      "Epoch 00\tBatch 2450\t0.1498s/batch\ttrain_loss = 5.9685\n",
      "Epoch 00\tBatch 2460\t0.1482s/batch\ttrain_loss = 5.4312\n",
      "Epoch 00\tBatch 2460\ttest_loss = 5.6311\n",
      "Epoch 00\tBatch 2470\t0.1504s/batch\ttrain_loss = 5.6947\n",
      "Epoch 00\tBatch 2480\t0.1469s/batch\ttrain_loss = 5.8217\n",
      "Epoch 00\tBatch 2490\t0.1474s/batch\ttrain_loss = 4.9429\n",
      "Epoch 00\tBatch 2490\ttest_loss = 5.6186\n",
      "Epoch 00\tBatch 2500\t0.1491s/batch\ttrain_loss = 5.4208\n",
      "Epoch 00\tBatch 2510\t0.1488s/batch\ttrain_loss = 5.6783\n",
      "Epoch 00\tBatch 2520\t0.1519s/batch\ttrain_loss = 8.5816\n",
      "Epoch 00\tBatch 2520\ttest_loss = 5.6072\n",
      "Epoch 00\tBatch 2530\t0.1490s/batch\ttrain_loss = 6.1870\n",
      "Epoch 00\tBatch 2540\t0.1491s/batch\ttrain_loss = 6.7002\n",
      "Epoch 00\tBatch 2550\t0.1501s/batch\ttrain_loss = 6.8667\n",
      "Epoch 00\tBatch 2550\ttest_loss = 5.5930\n",
      "Epoch 00\tBatch 2560\t0.1504s/batch\ttrain_loss = 7.3692\n",
      "Epoch 00\tBatch 2570\t0.1493s/batch\ttrain_loss = 5.1559\n",
      "Epoch 00\tBatch 2580\t0.1496s/batch\ttrain_loss = 4.9714\n",
      "Epoch 00\tBatch 2580\ttest_loss = 5.5911\n",
      "Epoch 00\tBatch 2590\t0.1493s/batch\ttrain_loss = 5.2966\n",
      "Epoch 00\tBatch 2600\t0.1492s/batch\ttrain_loss = 5.7707\n",
      "Epoch 00\tBatch 2610\t0.1500s/batch\ttrain_loss = 6.3100\n",
      "Epoch 00\tBatch 2610\ttest_loss = 5.5752\n",
      "new best loss (5.595487 -> 5.575216)\n",
      "Epoch 00\tBatch 2620\t0.1492s/batch\ttrain_loss = 5.4352\n",
      "Epoch 00\tBatch 2630\t0.1490s/batch\ttrain_loss = 5.4545\n",
      "Epoch 00\tBatch 2640\t0.1500s/batch\ttrain_loss = 5.9536\n",
      "Epoch 00\tBatch 2640\ttest_loss = 5.5916\n",
      "Epoch 00\tBatch 2650\t0.1495s/batch\ttrain_loss = 5.6647\n",
      "Epoch 00\tBatch 2660\t0.1484s/batch\ttrain_loss = 4.8966\n",
      "Epoch 00\tBatch 2670\t0.1472s/batch\ttrain_loss = 4.1472\n",
      "Epoch 00\tBatch 2670\ttest_loss = 5.5830\n",
      "Epoch 00\tBatch 2680\t0.1492s/batch\ttrain_loss = 5.7862\n",
      "Epoch 00\tBatch 2690\t0.1499s/batch\ttrain_loss = 5.8255\n",
      "Epoch 00\tBatch 2700\t0.1498s/batch\ttrain_loss = 5.5762\n",
      "Epoch 00\tBatch 2700\ttest_loss = 5.5935\n",
      "Epoch 00\tBatch 2710\t0.1500s/batch\ttrain_loss = 5.5389\n",
      "Epoch 00\tBatch 2720\t0.1516s/batch\ttrain_loss = 7.5123\n",
      "Epoch 00\tBatch 2730\t0.1493s/batch\ttrain_loss = 6.1124\n",
      "Epoch 00\tBatch 2730\ttest_loss = 5.5932\n",
      "Epoch 00\tBatch 2740\t0.1502s/batch\ttrain_loss = 5.1991\n",
      "Epoch 00\tBatch 2750\t0.1504s/batch\ttrain_loss = 7.4761\n",
      "Epoch 00\tBatch 2760\t0.1510s/batch\ttrain_loss = 7.3021\n",
      "Epoch 00\tBatch 2760\ttest_loss = 5.6008\n",
      "Epoch 00\tBatch 2770\t0.1491s/batch\ttrain_loss = 4.4125\n",
      "Epoch 00\tBatch 2780\t0.1492s/batch\ttrain_loss = 4.6313\n",
      "Epoch 00\tBatch 2790\t0.1501s/batch\ttrain_loss = 5.6846\n",
      "Epoch 00\tBatch 2790\ttest_loss = 5.5898\n",
      "Epoch 00\tBatch 2800\t0.1493s/batch\ttrain_loss = 5.4390\n",
      "Epoch 00\tBatch 2810\t0.1514s/batch\ttrain_loss = 6.9965\n",
      "Epoch 00\tBatch 2820\t0.1502s/batch\ttrain_loss = 5.9658\n",
      "Epoch 00\tBatch 2820\ttest_loss = 5.5761\n",
      "Epoch 00\tBatch 2830\t0.1500s/batch\ttrain_loss = 5.8908\n",
      "Epoch 00\tBatch 2840\t0.1502s/batch\ttrain_loss = 5.3551\n",
      "Epoch 00\tBatch 2850\t0.1502s/batch\ttrain_loss = 5.4545\n",
      "Epoch 00\tBatch 2850\ttest_loss = 5.5832\n",
      "Epoch 00\tBatch 2860\t0.1508s/batch\ttrain_loss = 6.1071\n",
      "Epoch 00\tBatch 2870\t0.1505s/batch\ttrain_loss = 5.4085\n",
      "Epoch 00\tBatch 2880\t0.1511s/batch\ttrain_loss = 6.2032\n",
      "Epoch 00\tBatch 2880\ttest_loss = 5.5980\n",
      "Epoch 00\tBatch 2890\t0.1499s/batch\ttrain_loss = 6.2021\n",
      "Epoch 00\tBatch 2900\t0.1510s/batch\ttrain_loss = 5.6337\n",
      "Epoch 00\tBatch 2910\t0.1527s/batch\ttrain_loss = 6.1949\n",
      "Epoch 00\tBatch 2910\ttest_loss = 5.6022\n",
      "Epoch 00\tBatch 2920\t0.1494s/batch\ttrain_loss = 4.9586\n",
      "Epoch 00\tBatch 2930\t0.1495s/batch\ttrain_loss = 4.7993\n",
      "Epoch 00\tBatch 2940\t0.1487s/batch\ttrain_loss = 4.7748\n",
      "Epoch 00\tBatch 2940\ttest_loss = 5.6054\n",
      "Epoch 00\tBatch 2950\t0.1494s/batch\ttrain_loss = 5.3060\n",
      "Epoch 00\tBatch 2960\t0.1510s/batch\ttrain_loss = 6.7690\n",
      "Epoch 00\tBatch 2970\t0.1495s/batch\ttrain_loss = 6.2414\n",
      "Epoch 00\tBatch 2970\ttest_loss = 5.5993\n",
      "Epoch 00\tBatch 2980\t0.1510s/batch\ttrain_loss = 6.8654\n",
      "Epoch 00\tBatch 2990\t0.1501s/batch\ttrain_loss = 5.6444\n",
      "Epoch 00\tBatch 3000\t0.1503s/batch\ttrain_loss = 6.0763\n",
      "Epoch 00\tBatch 3000\ttest_loss = 5.5757\n",
      "Epoch 00\tBatch 3010\t0.1510s/batch\ttrain_loss = 5.8580\n",
      "Epoch 00\tBatch 3020\t0.1514s/batch\ttrain_loss = 7.0501\n",
      "Epoch 00\tBatch 3030\t0.1503s/batch\ttrain_loss = 6.1223\n",
      "Epoch 00\tBatch 3030\ttest_loss = 5.5691\n",
      "Epoch 00\tBatch 3040\t0.1525s/batch\ttrain_loss = 6.7045\n",
      "Epoch 00\tBatch 3050\t0.1503s/batch\ttrain_loss = 5.8765\n",
      "Epoch 00\tBatch 3060\t0.1500s/batch\ttrain_loss = 4.6727\n",
      "Epoch 00\tBatch 3060\ttest_loss = 5.5886\n",
      "Epoch 00\tBatch 3070\t0.1491s/batch\ttrain_loss = 4.6187\n",
      "Epoch 00\tBatch 3080\t0.1480s/batch\ttrain_loss = 3.8847\n",
      "Epoch 00\tBatch 3090\t0.1504s/batch\ttrain_loss = 6.0394\n",
      "Epoch 00\tBatch 3090\ttest_loss = 5.6027\n",
      "Epoch 00\tBatch 3100\t0.1507s/batch\ttrain_loss = 5.9210\n",
      "Epoch 00\tBatch 3110\t0.1501s/batch\ttrain_loss = 5.7285\n",
      "Epoch 00\tBatch 3120\t0.1507s/batch\ttrain_loss = 6.1333\n",
      "Epoch 00\tBatch 3120\ttest_loss = 5.5723\n",
      "Epoch 00\tBatch 3130\t0.1514s/batch\ttrain_loss = 6.6582\n",
      "Epoch 00\tBatch 3140\t0.1493s/batch\ttrain_loss = 5.3178\n",
      "Epoch 00\tBatch 3150\t0.1619s/batch\ttrain_loss = 5.0544\n",
      "Epoch 00\tBatch 3150\ttest_loss = 5.5625\n",
      "new best loss (5.575216 -> 5.562455)\n",
      "Epoch 00\tBatch 3160\t0.1499s/batch\ttrain_loss = 5.5825\n",
      "Epoch 00\tBatch 3170\t0.1486s/batch\ttrain_loss = 5.3728\n",
      "Epoch 00\tBatch 3180\t0.1495s/batch\ttrain_loss = 5.1539\n",
      "Epoch 00\tBatch 3180\ttest_loss = 5.5733\n",
      "Epoch 00\tBatch 3190\t0.1507s/batch\ttrain_loss = 6.4266\n",
      "Epoch 00\tBatch 3200\t0.1506s/batch\ttrain_loss = 7.5177\n",
      "Epoch 00\tBatch 3210\t0.1499s/batch\ttrain_loss = 6.2164\n",
      "Epoch 00\tBatch 3210\ttest_loss = 5.5580\n",
      "Epoch 00\tBatch 3220\t0.1499s/batch\ttrain_loss = 6.3812\n",
      "Epoch 00\tBatch 3230\t0.1500s/batch\ttrain_loss = 6.5667\n",
      "Epoch 00\tBatch 3240\t0.1499s/batch\ttrain_loss = 6.1179\n",
      "Epoch 00\tBatch 3240\ttest_loss = 5.5469\n",
      "new best loss (5.562455 -> 5.546914)\n",
      "Epoch 00\tBatch 3250\t0.1507s/batch\ttrain_loss = 6.8784\n",
      "Epoch 00\tBatch 3260\t0.1520s/batch\ttrain_loss = 7.7128\n",
      "Epoch 00\tBatch 3270\t0.1509s/batch\ttrain_loss = 6.4119\n",
      "Epoch 00\tBatch 3270\ttest_loss = 5.5877\n",
      "Epoch 00\tBatch 3280\t0.1491s/batch\ttrain_loss = 5.3438\n",
      "Epoch 00\tBatch 3290\t0.1497s/batch\ttrain_loss = 5.7274\n",
      "Epoch 00\tBatch 3300\t0.1489s/batch\ttrain_loss = 5.4081\n",
      "Epoch 00\tBatch 3300\ttest_loss = 5.5612\n",
      "Epoch 00\tBatch 3310\t0.1491s/batch\ttrain_loss = 5.0691\n",
      "Epoch 00\tBatch 3320\t0.1486s/batch\ttrain_loss = 4.9492\n",
      "Epoch 00\tBatch 3330\t0.1499s/batch\ttrain_loss = 6.0637\n",
      "Epoch 00\tBatch 3330\ttest_loss = 5.5688\n",
      "Epoch 00\tBatch 3340\t0.1493s/batch\ttrain_loss = 4.8616\n",
      "Epoch 00\tBatch 3350\t0.1516s/batch\ttrain_loss = 8.3071\n",
      "Epoch 00\tBatch 3360\t0.1498s/batch\ttrain_loss = 6.4327\n",
      "Epoch 00\tBatch 3360\ttest_loss = 5.5855\n",
      "Epoch 00\tBatch 3370\t0.1493s/batch\ttrain_loss = 5.5120\n",
      "Epoch 00\tBatch 3380\t0.1489s/batch\ttrain_loss = 4.0970\n",
      "Epoch 00\tBatch 3390\t0.1606s/batch\ttrain_loss = 5.0544\n",
      "Epoch 00\tBatch 3390\ttest_loss = 5.5888\n",
      "Epoch 00\tBatch 3400\t0.1489s/batch\ttrain_loss = 5.2434\n",
      "Epoch 00\tBatch 3410\t0.1486s/batch\ttrain_loss = 4.9606\n",
      "Epoch 00\tBatch 3420\t0.1495s/batch\ttrain_loss = 5.6277\n",
      "Epoch 00\tBatch 3420\ttest_loss = 5.5675\n",
      "Epoch 00\tBatch 3430\t0.1498s/batch\ttrain_loss = 5.5509\n",
      "Epoch 00\tBatch 3440\t0.1487s/batch\ttrain_loss = 4.3452\n",
      "Epoch 00\tBatch 3450\t0.1489s/batch\ttrain_loss = 5.4309\n",
      "Epoch 00\tBatch 3450\ttest_loss = 5.5773\n",
      "Epoch 00\tBatch 3460\t0.1490s/batch\ttrain_loss = 5.5496\n",
      "Epoch 00\tBatch 3470\t0.1491s/batch\ttrain_loss = 5.2452\n",
      "Epoch 00\tBatch 3480\t0.1489s/batch\ttrain_loss = 5.1052\n",
      "Epoch 00\tBatch 3480\ttest_loss = 5.5449\n",
      "Epoch 00\tBatch 3490\t0.1497s/batch\ttrain_loss = 5.2182\n",
      "Epoch 00\tBatch 3500\t0.1490s/batch\ttrain_loss = 4.5703\n",
      "Epoch 00\tBatch 3510\t0.1504s/batch\ttrain_loss = 5.9778\n",
      "Epoch 00\tBatch 3510\ttest_loss = 5.5424\n",
      "Epoch 00\tBatch 3520\t0.1509s/batch\ttrain_loss = 6.0911\n",
      "Epoch 00\tBatch 3530\t0.1512s/batch\ttrain_loss = 6.0931\n",
      "Epoch 00\tBatch 3540\t0.1507s/batch\ttrain_loss = 6.3032\n",
      "Epoch 00\tBatch 3540\ttest_loss = 5.5424\n",
      "Epoch 00\tBatch 3550\t0.1496s/batch\ttrain_loss = 4.5425\n",
      "Epoch 00\tBatch 3560\t0.1498s/batch\ttrain_loss = 5.7051\n",
      "Epoch 00\tBatch 3570\t0.1501s/batch\ttrain_loss = 4.8711\n",
      "Epoch 00\tBatch 3570\ttest_loss = 5.7355\n",
      "Epoch 00\tBatch 3580\t0.1499s/batch\ttrain_loss = 5.6266\n",
      "Epoch 00\tBatch 3590\t0.1505s/batch\ttrain_loss = 5.7805\n",
      "Epoch 00\tBatch 3600\t0.1491s/batch\ttrain_loss = 4.7898\n",
      "Epoch 00\tBatch 3600\ttest_loss = 5.5409\n",
      "Epoch 00\tBatch 3610\t0.1510s/batch\ttrain_loss = 5.0743\n",
      "Epoch 00\tBatch 3620\t0.1515s/batch\ttrain_loss = 5.6371\n",
      "Epoch 00\tBatch 3630\t0.1506s/batch\ttrain_loss = 5.2029\n",
      "Epoch 00\tBatch 3630\ttest_loss = 5.5500\n",
      "Epoch 00\tBatch 3640\t0.1499s/batch\ttrain_loss = 4.8008\n",
      "Epoch 00\tBatch 3650\t0.1497s/batch\ttrain_loss = 5.2584\n",
      "Epoch 00\tBatch 3660\t0.1498s/batch\ttrain_loss = 5.3968\n",
      "Epoch 00\tBatch 3660\ttest_loss = 5.5445\n",
      "Epoch 00\tBatch 3670\t0.1620s/batch\ttrain_loss = 6.3977\n",
      "Epoch 00\tBatch 3680\t0.1496s/batch\ttrain_loss = 5.3729\n",
      "Epoch 00\tBatch 3690\t0.1500s/batch\ttrain_loss = 6.0391\n",
      "Epoch 00\tBatch 3690\ttest_loss = 5.5520\n",
      "Epoch 00\tBatch 3700\t0.1493s/batch\ttrain_loss = 5.2953\n",
      "Epoch 00\tBatch 3710\t0.1489s/batch\ttrain_loss = 4.6928\n",
      "Epoch 00\tBatch 3720\t0.1503s/batch\ttrain_loss = 6.3196\n",
      "Epoch 00\tBatch 3720\ttest_loss = 5.5405\n",
      "Epoch 00\tBatch 3730\t0.1522s/batch\ttrain_loss = 7.0138\n",
      "Epoch 00\tBatch 3740\t0.1507s/batch\ttrain_loss = 7.0674\n",
      "Epoch 00\tBatch 3750\t0.1491s/batch\ttrain_loss = 5.5250\n",
      "Epoch 00\tBatch 3750\ttest_loss = 5.5417\n",
      "Epoch 00\tBatch 3760\t0.1497s/batch\ttrain_loss = 5.0379\n",
      "Epoch 00\tBatch 3770\t0.1485s/batch\ttrain_loss = 4.6562\n",
      "Epoch 00\tBatch 3780\t0.1493s/batch\ttrain_loss = 5.3356\n",
      "Epoch 00\tBatch 3780\ttest_loss = 5.5506\n",
      "Epoch 00\tBatch 3790\t0.1495s/batch\ttrain_loss = 5.2637\n",
      "Epoch 00\tBatch 3800\t0.1496s/batch\ttrain_loss = 4.9510\n",
      "Epoch 00\tBatch 3810\t0.1495s/batch\ttrain_loss = 5.1991\n",
      "Epoch 00\tBatch 3810\ttest_loss = 5.5539\n",
      "Epoch 00\tBatch 3820\t0.1499s/batch\ttrain_loss = 5.8987\n",
      "Epoch 00\tBatch 3830\t0.1497s/batch\ttrain_loss = 5.4419\n",
      "Epoch 00\tBatch 3840\t0.1489s/batch\ttrain_loss = 4.5118\n",
      "Epoch 00\tBatch 3840\ttest_loss = 5.5486\n",
      "Epoch 00\tBatch 3850\t0.1495s/batch\ttrain_loss = 5.2748\n",
      "Epoch 00\tBatch 3860\t0.1513s/batch\ttrain_loss = 6.1440\n",
      "Epoch 00\tBatch 3870\t0.1504s/batch\ttrain_loss = 6.3617\n",
      "Epoch 00\tBatch 3870\ttest_loss = 5.5539\n",
      "Epoch 00\tBatch 3880\t0.1506s/batch\ttrain_loss = 5.8203\n",
      "Epoch 00\tBatch 3890\t0.1515s/batch\ttrain_loss = 6.6856\n",
      "Epoch 00\tBatch 3900\t0.1494s/batch\ttrain_loss = 6.1506\n",
      "Epoch 00\tBatch 3900\ttest_loss = 5.5627\n",
      "Epoch 00\tBatch 3910\t0.1496s/batch\ttrain_loss = 5.6728\n",
      "Epoch 00\tBatch 3920\t0.1490s/batch\ttrain_loss = 5.9437\n",
      "Epoch 00\tBatch 3930\t0.1494s/batch\ttrain_loss = 5.4406\n",
      "Epoch 00\tBatch 3930\ttest_loss = 5.5586\n",
      "Epoch 00\tBatch 3940\t0.1492s/batch\ttrain_loss = 5.8808\n",
      "Epoch 00\tBatch 3950\t0.1503s/batch\ttrain_loss = 7.9262\n",
      "Epoch 00\tBatch 3960\t0.1514s/batch\ttrain_loss = 6.7836\n",
      "Epoch 00\tBatch 3960\ttest_loss = 5.5564\n",
      "Epoch 00\tBatch 3970\t0.1499s/batch\ttrain_loss = 6.6704\n",
      "Epoch 00\tBatch 3980\t0.1497s/batch\ttrain_loss = 6.4117\n",
      "Epoch 00\tBatch 3990\t0.1500s/batch\ttrain_loss = 5.4816\n",
      "Epoch 00\tBatch 3990\ttest_loss = 5.5587\n",
      "Epoch 00\tBatch 4000\t0.1486s/batch\ttrain_loss = 4.4043\n",
      "Epoch 00\tBatch 4010\t0.1507s/batch\ttrain_loss = 5.1098\n",
      "Epoch 00\tBatch 4020\t0.1489s/batch\ttrain_loss = 4.7373\n",
      "Epoch 00\tBatch 4020\ttest_loss = 5.5578\n",
      "Epoch 00\tBatch 4030\t0.1493s/batch\ttrain_loss = 5.5249\n",
      "Epoch 00\tBatch 4040\t0.1503s/batch\ttrain_loss = 6.1941\n",
      "Epoch 00\tBatch 4050\t0.1506s/batch\ttrain_loss = 6.7528\n",
      "Epoch 00\tBatch 4050\ttest_loss = 5.5586\n",
      "Epoch 00\tBatch 4060\t0.1505s/batch\ttrain_loss = 6.8312\n",
      "Epoch 00\tBatch 4070\t0.1506s/batch\ttrain_loss = 6.8789\n",
      "Epoch 00\tBatch 4080\t0.1484s/batch\ttrain_loss = 5.1156\n",
      "Epoch 00\tBatch 4080\ttest_loss = 5.5490\n",
      "Epoch 00\tBatch 4090\t0.1501s/batch\ttrain_loss = 5.7659\n",
      "Epoch 00\tBatch 4100\t0.1505s/batch\ttrain_loss = 5.3213\n",
      "Epoch 00\tBatch 4110\t0.1515s/batch\ttrain_loss = 6.3073\n",
      "Epoch 00\tBatch 4110\ttest_loss = 5.5413\n",
      "Epoch 00\tBatch 4120\t0.1514s/batch\ttrain_loss = 6.0063\n",
      "Epoch 00\tBatch 4130\t0.1510s/batch\ttrain_loss = 6.3971\n",
      "Epoch 00\tBatch 4140\t0.1498s/batch\ttrain_loss = 5.9132\n",
      "Epoch 00\tBatch 4140\ttest_loss = 5.5471\n",
      "Epoch 00\tBatch 4150\t0.1498s/batch\ttrain_loss = 5.7558\n",
      "Epoch 00\tBatch 4160\t0.1508s/batch\ttrain_loss = 6.6739\n",
      "Epoch 00\tBatch 4170\t0.1507s/batch\ttrain_loss = 6.2167\n",
      "Epoch 00\tBatch 4170\ttest_loss = 5.5397\n",
      "Epoch 00\tBatch 4180\t0.1506s/batch\ttrain_loss = 6.1978\n",
      "Epoch 00\tBatch 4190\t0.1507s/batch\ttrain_loss = 5.8998\n",
      "Epoch 00\tBatch 4200\t0.1501s/batch\ttrain_loss = 5.8333\n",
      "Epoch 00\tBatch 4200\ttest_loss = 5.5283\n",
      "new best loss (5.546914 -> 5.528267)\n",
      "Epoch 00\tBatch 4210\t0.1506s/batch\ttrain_loss = 6.7390\n",
      "Epoch 00\tBatch 4220\t0.1495s/batch\ttrain_loss = 5.6620\n",
      "Epoch 00\tBatch 4230\t0.1496s/batch\ttrain_loss = 6.2221\n",
      "Epoch 00\tBatch 4230\ttest_loss = 5.5361\n",
      "Epoch 00\tBatch 4240\t0.1501s/batch\ttrain_loss = 5.6215\n",
      "Epoch 00\tBatch 4250\t0.1498s/batch\ttrain_loss = 5.5971\n",
      "Epoch 00\tBatch 4260\t0.1489s/batch\ttrain_loss = 5.2632\n",
      "Epoch 00\tBatch 4260\ttest_loss = 5.5508\n",
      "Epoch 00\tBatch 4270\t0.1510s/batch\ttrain_loss = 7.0925\n",
      "Epoch 00\tBatch 4280\t0.1497s/batch\ttrain_loss = 5.4320\n",
      "Epoch 00\tBatch 4290\t0.1499s/batch\ttrain_loss = 4.6679\n",
      "Epoch 00\tBatch 4290\ttest_loss = 5.5240\n",
      "Epoch 00\tBatch 4300\t0.1498s/batch\ttrain_loss = 5.7159\n",
      "Epoch 00\tBatch 4310\t0.1510s/batch\ttrain_loss = 6.4709\n",
      "Epoch 00\tBatch 4320\t0.1501s/batch\ttrain_loss = 5.7780\n",
      "Epoch 00\tBatch 4320\ttest_loss = 5.5329\n",
      "Epoch 00\tBatch 4330\t0.1494s/batch\ttrain_loss = 5.6315\n",
      "Epoch 00\tBatch 4340\t0.1509s/batch\ttrain_loss = 6.1851\n",
      "Epoch 00\tBatch 4350\t0.1492s/batch\ttrain_loss = 5.1335\n",
      "Epoch 00\tBatch 4350\ttest_loss = 5.5765\n",
      "Epoch 00\tBatch 4360\t0.1502s/batch\ttrain_loss = 5.6684\n",
      "Epoch 00\tBatch 4370\t0.1507s/batch\ttrain_loss = 6.8641\n",
      "Epoch 00\tBatch 4380\t0.1493s/batch\ttrain_loss = 5.7430\n",
      "Epoch 00\tBatch 4380\ttest_loss = 5.5258\n",
      "Epoch 00\tBatch 4390\t0.1491s/batch\ttrain_loss = 5.4464\n",
      "Epoch 00\tBatch 4400\t0.1510s/batch\ttrain_loss = 7.1188\n",
      "Epoch 00\tBatch 4410\t0.1502s/batch\ttrain_loss = 6.0223\n",
      "Epoch 00\tBatch 4410\ttest_loss = 5.5118\n",
      "new best loss (5.528267 -> 5.511764)\n",
      "Epoch 00\tBatch 4420\t0.1519s/batch\ttrain_loss = 7.2941\n",
      "Epoch 00\tBatch 4430\t0.1515s/batch\ttrain_loss = 6.6475\n",
      "Epoch 00\tBatch 4440\t0.1491s/batch\ttrain_loss = 5.1746\n",
      "Epoch 00\tBatch 4440\ttest_loss = 5.5132\n",
      "Epoch 00\tBatch 4450\t0.1513s/batch\ttrain_loss = 5.1157\n",
      "Epoch 00\tBatch 4460\t0.1514s/batch\ttrain_loss = 5.6396\n",
      "Epoch 00\tBatch 4470\t0.1509s/batch\ttrain_loss = 5.6058\n",
      "Epoch 00\tBatch 4470\ttest_loss = 5.5146\n",
      "Epoch 00\tBatch 4480\t0.1501s/batch\ttrain_loss = 4.8988\n",
      "Epoch 00\tBatch 4490\t0.1495s/batch\ttrain_loss = 5.1103\n",
      "Epoch 00\tBatch 4500\t0.1505s/batch\ttrain_loss = 5.1578\n",
      "Epoch 00\tBatch 4500\ttest_loss = 5.5376\n",
      "Epoch 00\tBatch 4510\t0.1489s/batch\ttrain_loss = 4.8600\n",
      "Epoch 00\tBatch 4520\t0.1493s/batch\ttrain_loss = 4.7105\n",
      "Epoch 00\tBatch 4530\t0.1511s/batch\ttrain_loss = 4.8337\n",
      "Epoch 00\tBatch 4530\ttest_loss = 5.5314\n",
      "Epoch 00\tBatch 4540\t0.1493s/batch\ttrain_loss = 4.7738\n",
      "Epoch 00\tBatch 4550\t0.1492s/batch\ttrain_loss = 4.3254\n",
      "Epoch 00\tBatch 4560\t0.1501s/batch\ttrain_loss = 5.6936\n",
      "Epoch 00\tBatch 4560\ttest_loss = 5.5282\n",
      "Epoch 00\tBatch 4570\t0.1491s/batch\ttrain_loss = 4.8915\n",
      "Epoch 00\tBatch 4580\t0.1492s/batch\ttrain_loss = 4.1955\n",
      "Epoch 00\tBatch 4590\t0.1509s/batch\ttrain_loss = 7.0283\n",
      "Epoch 00\tBatch 4590\ttest_loss = 5.5126\n",
      "Epoch 00\tBatch 4600\t0.1509s/batch\ttrain_loss = 5.9172\n",
      "Epoch 00\tBatch 4610\t0.1521s/batch\ttrain_loss = 7.0835\n",
      "Epoch 00\tBatch 4620\t0.1510s/batch\ttrain_loss = 5.3469\n",
      "Epoch 00\tBatch 4620\ttest_loss = 5.5191\n",
      "Epoch 00\tBatch 4630\t0.1509s/batch\ttrain_loss = 6.0756\n",
      "Epoch 00\tBatch 4640\t0.1519s/batch\ttrain_loss = 6.1539\n",
      "Epoch 00\tBatch 4650\t0.1510s/batch\ttrain_loss = 6.0702\n",
      "Epoch 00\tBatch 4650\ttest_loss = 5.5097\n",
      "Epoch 00\tBatch 4660\t0.1526s/batch\ttrain_loss = 7.1037\n",
      "Epoch 00\tBatch 4670\t0.1498s/batch\ttrain_loss = 5.0503\n",
      "Epoch 00\tBatch 4680\t0.1497s/batch\ttrain_loss = 5.4933\n",
      "Epoch 00\tBatch 4680\ttest_loss = 5.5141\n",
      "Epoch 00\tBatch 4690\t0.1499s/batch\ttrain_loss = 5.5306\n",
      "Epoch 00\tBatch 4700\t0.1495s/batch\ttrain_loss = 4.4913\n",
      "Epoch 00\tBatch 4710\t0.1502s/batch\ttrain_loss = 6.1872\n",
      "Epoch 00\tBatch 4710\ttest_loss = 5.5074\n",
      "Epoch 00\tBatch 4720\t0.1512s/batch\ttrain_loss = 5.4311\n",
      "Epoch 00\tBatch 4730\t0.1485s/batch\ttrain_loss = 4.9104\n",
      "Epoch 00\tBatch 4740\t0.1499s/batch\ttrain_loss = 5.9244\n",
      "Epoch 00\tBatch 4740\ttest_loss = 5.5082\n",
      "Epoch 00\tBatch 4750\t0.1502s/batch\ttrain_loss = 5.6109\n",
      "Epoch 00\tBatch 4760\t0.1500s/batch\ttrain_loss = 4.9668\n",
      "Epoch 00\tBatch 4770\t0.1490s/batch\ttrain_loss = 4.3863\n",
      "Epoch 00\tBatch 4770\ttest_loss = 5.5295\n",
      "Epoch 00\tBatch 4780\t0.1505s/batch\ttrain_loss = 5.5468\n",
      "Epoch 00\tBatch 4790\t0.1500s/batch\ttrain_loss = 5.0212\n",
      "Epoch 00\tBatch 4800\t0.1512s/batch\ttrain_loss = 6.3503\n",
      "Epoch 00\tBatch 4800\ttest_loss = 5.5437\n",
      "Epoch 00\tBatch 4810\t0.1506s/batch\ttrain_loss = 5.4156\n",
      "Epoch 00\tBatch 4820\t0.1502s/batch\ttrain_loss = 5.3818\n",
      "Epoch 00\tBatch 4830\t0.1499s/batch\ttrain_loss = 5.6565\n",
      "Epoch 00\tBatch 4830\ttest_loss = 5.5399\n",
      "Epoch 00\tBatch 4840\t0.1510s/batch\ttrain_loss = 5.9796\n",
      "Epoch 00\tBatch 4850\t0.1504s/batch\ttrain_loss = 5.4446\n",
      "Epoch 00\tBatch 4860\t0.1493s/batch\ttrain_loss = 4.6991\n",
      "Epoch 00\tBatch 4860\ttest_loss = 5.5028\n",
      "Epoch 00\tBatch 4870\t0.1495s/batch\ttrain_loss = 5.3129\n",
      "Epoch 00\tBatch 4880\t0.1508s/batch\ttrain_loss = 5.7019\n",
      "Epoch 00\tBatch 4890\t0.1500s/batch\ttrain_loss = 5.5305\n",
      "Epoch 00\tBatch 4890\ttest_loss = 5.4969\n",
      "new best loss (5.511764 -> 5.496924)\n",
      "Epoch 00\tBatch 4900\t0.1502s/batch\ttrain_loss = 5.8975\n",
      "Epoch 00\tBatch 4910\t0.1515s/batch\ttrain_loss = 5.9355\n",
      "Epoch 00\tBatch 4920\t0.1510s/batch\ttrain_loss = 5.8222\n",
      "Epoch 00\tBatch 4920\ttest_loss = 5.5250\n",
      "Epoch 00\tBatch 4930\t0.1518s/batch\ttrain_loss = 7.3772\n",
      "Epoch 00\tBatch 4940\t0.1516s/batch\ttrain_loss = 5.8689\n",
      "Epoch 00\tBatch 4950\t0.1533s/batch\ttrain_loss = 6.0774\n",
      "Epoch 00\tBatch 4950\ttest_loss = 5.5095\n",
      "Epoch 00\tBatch 4960\t0.1511s/batch\ttrain_loss = 6.2150\n",
      "Epoch 00\tBatch 4970\t0.1512s/batch\ttrain_loss = 5.7887\n",
      "Epoch 00\tBatch 4980\t0.1511s/batch\ttrain_loss = 6.5125\n",
      "Epoch 00\tBatch 4980\ttest_loss = 5.5450\n",
      "Epoch 00\tBatch 4990\t0.1497s/batch\ttrain_loss = 6.2129\n",
      "Epoch 00\tBatch 5000\t0.1479s/batch\ttrain_loss = 4.7084\n",
      "Epoch 00\tBatch 5010\t0.1512s/batch\ttrain_loss = 4.5017\n",
      "Epoch 00\tBatch 5010\ttest_loss = 5.5152\n",
      "Epoch 00\tBatch 5020\t0.1495s/batch\ttrain_loss = 5.3910\n",
      "Epoch 00\tBatch 5030\t0.1493s/batch\ttrain_loss = 4.6386\n",
      "Epoch 00\tBatch 5040\t0.1497s/batch\ttrain_loss = 5.5172\n",
      "Epoch 00\tBatch 5040\ttest_loss = 5.5519\n",
      "Epoch 00\tBatch 5050\t0.1487s/batch\ttrain_loss = 4.6300\n",
      "Epoch 00\tBatch 5060\t0.1494s/batch\ttrain_loss = 5.0640\n",
      "Epoch 00\tBatch 5070\t0.1483s/batch\ttrain_loss = 3.8937\n",
      "Epoch 00\tBatch 5070\ttest_loss = 5.5404\n",
      "Epoch 00\tBatch 5080\t0.1496s/batch\ttrain_loss = 4.7356\n",
      "Epoch 00\tBatch 5090\t0.1485s/batch\ttrain_loss = 4.1474\n",
      "Epoch 00\tBatch 5100\t0.1482s/batch\ttrain_loss = 3.7194\n",
      "Epoch 00\tBatch 5100\ttest_loss = 5.5695\n",
      "Epoch 00\tBatch 5110\t0.1500s/batch\ttrain_loss = 5.1200\n",
      "Epoch 00\tBatch 5120\t0.1487s/batch\ttrain_loss = 4.2214\n",
      "Epoch 00\tBatch 5130\t0.1515s/batch\ttrain_loss = 6.6332\n",
      "Epoch 00\tBatch 5130\ttest_loss = 5.5018\n",
      "Epoch 00\tBatch 5140\t0.1505s/batch\ttrain_loss = 5.6792\n",
      "Epoch 00\tBatch 5150\t0.1512s/batch\ttrain_loss = 6.3844\n",
      "Epoch 00\tBatch 5160\t0.1500s/batch\ttrain_loss = 5.9026\n",
      "Epoch 00\tBatch 5160\ttest_loss = 5.5043\n",
      "Epoch 00\tBatch 5170\t0.1507s/batch\ttrain_loss = 6.1446\n",
      "Epoch 00\tBatch 5180\t0.1499s/batch\ttrain_loss = 5.1960\n",
      "Epoch 00\tBatch 5190\t0.1502s/batch\ttrain_loss = 4.8867\n",
      "Epoch 00\tBatch 5190\ttest_loss = 5.5064\n",
      "Epoch 00\tBatch 5200\t0.1502s/batch\ttrain_loss = 5.1577\n",
      "Epoch 00\tBatch 5210\t0.1519s/batch\ttrain_loss = 6.7986\n",
      "Epoch 00\tBatch 5220\t0.1502s/batch\ttrain_loss = 6.2350\n",
      "Epoch 00\tBatch 5220\ttest_loss = 5.5012\n",
      "Epoch 00\tBatch 5230\t0.1504s/batch\ttrain_loss = 6.8784\n",
      "Epoch 00\tBatch 5240\t0.1517s/batch\ttrain_loss = 6.2587\n",
      "Epoch 00\tBatch 5250\t0.1514s/batch\ttrain_loss = 5.1822\n",
      "Epoch 00\tBatch 5250\ttest_loss = 5.5010\n",
      "Epoch 00\tBatch 5260\t0.1502s/batch\ttrain_loss = 5.1230\n",
      "Epoch 00\tBatch 5270\t0.1498s/batch\ttrain_loss = 4.4954\n",
      "Epoch 00\tBatch 5280\t0.1502s/batch\ttrain_loss = 4.8503\n",
      "Epoch 00\tBatch 5280\ttest_loss = 5.5131\n",
      "Epoch 00\tBatch 5290\t0.1492s/batch\ttrain_loss = 4.3120\n",
      "Epoch 00\tBatch 5300\t0.1505s/batch\ttrain_loss = 4.5917\n",
      "Epoch 00\tBatch 5310\t0.1499s/batch\ttrain_loss = 4.2803\n",
      "Epoch 00\tBatch 5310\ttest_loss = 5.5383\n",
      "Epoch 00\tBatch 5320\t0.1503s/batch\ttrain_loss = 4.7706\n",
      "Epoch 00\tBatch 5330\t0.1502s/batch\ttrain_loss = 4.6042\n",
      "Epoch 00\tBatch 5340\t0.1506s/batch\ttrain_loss = 5.8074\n",
      "Epoch 00\tBatch 5340\ttest_loss = 5.5194\n",
      "Epoch 00\tBatch 5350\t0.1505s/batch\ttrain_loss = 5.1907\n",
      "Epoch 00\tBatch 5360\t0.1483s/batch\ttrain_loss = 4.0001\n",
      "Epoch 00\tBatch 5370\t0.1504s/batch\ttrain_loss = 4.8305\n",
      "Epoch 00\tBatch 5370\ttest_loss = 5.5170\n",
      "Epoch 00\tBatch 5380\t0.1494s/batch\ttrain_loss = 4.5550\n",
      "Epoch 00\tBatch 5390\t0.1499s/batch\ttrain_loss = 4.6610\n",
      "Epoch 00\tBatch 5400\t0.1484s/batch\ttrain_loss = 5.1055\n",
      "Epoch 00\tBatch 5400\ttest_loss = 5.5093\n",
      "Epoch 00\tBatch 5410\t0.1513s/batch\ttrain_loss = 6.8628\n",
      "Epoch 00\tBatch 5420\t0.1520s/batch\ttrain_loss = 7.7538\n",
      "Epoch 00\tBatch 5430\t0.1510s/batch\ttrain_loss = 5.8098\n",
      "Epoch 00\tBatch 5430\ttest_loss = 5.5086\n",
      "Epoch 00\tBatch 5440\t0.1503s/batch\ttrain_loss = 4.7047\n",
      "Epoch 00\tBatch 5450\t0.1494s/batch\ttrain_loss = 4.8574\n",
      "Epoch 00\tBatch 5460\t0.1491s/batch\ttrain_loss = 4.9163\n",
      "Epoch 00\tBatch 5460\ttest_loss = 5.5406\n",
      "Epoch 00\tBatch 5470\t0.1510s/batch\ttrain_loss = 7.5012\n",
      "Epoch 00\tBatch 5480\t0.1506s/batch\ttrain_loss = 6.4914\n",
      "Epoch 00\tBatch 5490\t0.1511s/batch\ttrain_loss = 6.3218\n",
      "Epoch 00\tBatch 5490\ttest_loss = 5.5433\n",
      "Epoch 00\tBatch 5500\t0.1507s/batch\ttrain_loss = 6.3207\n",
      "Epoch 00\tBatch 5510\t0.1499s/batch\ttrain_loss = 5.4565\n",
      "Epoch 00\tBatch 5520\t0.1493s/batch\ttrain_loss = 6.0607\n",
      "Epoch 00\tBatch 5520\ttest_loss = 5.5234\n",
      "Epoch 00\tBatch 5530\t0.1505s/batch\ttrain_loss = 5.4401\n",
      "Epoch 00\tBatch 5540\t0.1506s/batch\ttrain_loss = 5.7810\n",
      "Epoch 00\tBatch 5550\t0.1509s/batch\ttrain_loss = 6.3701\n",
      "Epoch 00\tBatch 5550\ttest_loss = 5.5197\n",
      "Epoch 00\tBatch 5560\t0.1493s/batch\ttrain_loss = 6.7281\n",
      "Epoch 00\tBatch 5570\t0.1488s/batch\ttrain_loss = 7.1662\n",
      "Epoch 00\tBatch 5580\t0.1477s/batch\ttrain_loss = 5.2289\n",
      "Epoch 00\tBatch 5580\ttest_loss = 5.5051\n",
      "Epoch 00\tBatch 5590\t0.1498s/batch\ttrain_loss = 5.2155\n",
      "Epoch 00\tBatch 5600\t0.1504s/batch\ttrain_loss = 6.1379\n",
      "Epoch 00\tBatch 5610\t0.1506s/batch\ttrain_loss = 6.3208\n",
      "Epoch 00\tBatch 5610\ttest_loss = 5.4992\n",
      "Epoch 00\tBatch 5620\t0.1509s/batch\ttrain_loss = 6.6683\n",
      "Epoch 00\tBatch 5630\t0.1495s/batch\ttrain_loss = 5.2622\n",
      "Epoch 00\tBatch 5640\t0.1501s/batch\ttrain_loss = 5.1892\n",
      "Epoch 00\tBatch 5640\ttest_loss = 5.5045\n",
      "Epoch 00\tBatch 5650\t0.1501s/batch\ttrain_loss = 5.9486\n",
      "Epoch 00\tBatch 5660\t0.1497s/batch\ttrain_loss = 5.9717\n",
      "Epoch 00\tBatch 5670\t0.1511s/batch\ttrain_loss = 6.8173\n",
      "Epoch 00\tBatch 5670\ttest_loss = 5.5171\n",
      "Epoch 00\tBatch 5680\t0.1494s/batch\ttrain_loss = 4.8222\n",
      "Epoch 00\tBatch 5690\t0.1489s/batch\ttrain_loss = 5.1549\n",
      "Epoch 00\tBatch 5700\t0.1479s/batch\ttrain_loss = 4.5110\n",
      "Epoch 00\tBatch 5700\ttest_loss = 5.5061\n",
      "Epoch 00\tBatch 5710\t0.1492s/batch\ttrain_loss = 5.1810\n",
      "Epoch 00\tBatch 5720\t0.1486s/batch\ttrain_loss = 5.0938\n",
      "Epoch 00\tBatch 5730\t0.1502s/batch\ttrain_loss = 6.2247\n",
      "Epoch 00\tBatch 5730\ttest_loss = 5.5149\n",
      "Epoch 00\tBatch 5740\t0.1489s/batch\ttrain_loss = 5.4457\n",
      "Epoch 00\tBatch 5750\t0.1491s/batch\ttrain_loss = 5.4781\n",
      "Epoch 00\tBatch 5760\t0.1488s/batch\ttrain_loss = 5.0254\n",
      "Epoch 00\tBatch 5760\ttest_loss = 5.4893\n",
      "Epoch 00\tBatch 5770\t0.1493s/batch\ttrain_loss = 4.1899\n",
      "Epoch 00\tBatch 5780\t0.1484s/batch\ttrain_loss = 3.6495\n",
      "Epoch 00\tBatch 5790\t0.1480s/batch\ttrain_loss = 3.6521\n",
      "Epoch 00\tBatch 5790\ttest_loss = 5.5302\n",
      "Epoch 00\tBatch 5800\t0.1481s/batch\ttrain_loss = 4.1575\n",
      "Epoch 00\tBatch 5810\t0.1514s/batch\ttrain_loss = 6.9319\n",
      "Epoch 00\tBatch 5820\t0.1502s/batch\ttrain_loss = 5.0319\n",
      "Epoch 00\tBatch 5820\ttest_loss = 5.5096\n",
      "Epoch 00\tBatch 5830\t0.1501s/batch\ttrain_loss = 6.1329\n",
      "Epoch 00\tBatch 5840\t0.1506s/batch\ttrain_loss = 6.3112\n",
      "Epoch 00\tBatch 5850\t0.1499s/batch\ttrain_loss = 5.1969\n",
      "Epoch 00\tBatch 5850\ttest_loss = 5.5378\n",
      "Epoch 00\tBatch 5860\t0.1499s/batch\ttrain_loss = 6.3392\n",
      "Epoch 00\tBatch 5870\t0.1497s/batch\ttrain_loss = 6.0158\n",
      "Epoch 00\tBatch 5880\t0.1503s/batch\ttrain_loss = 5.0035\n",
      "Epoch 00\tBatch 5880\ttest_loss = 5.5196\n",
      "Epoch 00\tBatch 5890\t0.1491s/batch\ttrain_loss = 5.4411\n",
      "Epoch 00\tBatch 5900\t0.1499s/batch\ttrain_loss = 5.8537\n",
      "Epoch 00\tBatch 5910\t0.1497s/batch\ttrain_loss = 5.1469\n",
      "Epoch 00\tBatch 5910\ttest_loss = 5.5144\n",
      "Epoch 00\tBatch 5920\t0.1496s/batch\ttrain_loss = 6.0926\n",
      "Epoch 00\tBatch 5930\t0.1508s/batch\ttrain_loss = 6.1768\n",
      "Epoch 00\tBatch 5940\t0.1503s/batch\ttrain_loss = 5.9842\n",
      "Epoch 00\tBatch 5940\ttest_loss = 5.4949\n",
      "Epoch 00\tBatch 5950\t0.1496s/batch\ttrain_loss = 5.5494\n",
      "Epoch 00\tBatch 5960\t0.1494s/batch\ttrain_loss = 5.0370\n",
      "Epoch 00\tBatch 5970\t0.1497s/batch\ttrain_loss = 5.0203\n",
      "Epoch 00\tBatch 5970\ttest_loss = 5.5074\n",
      "Epoch 00\tBatch 5980\t0.1487s/batch\ttrain_loss = 4.6768\n",
      "Epoch 00\tBatch 5990\t0.1604s/batch\ttrain_loss = 4.4574\n",
      "Epoch 00\tBatch 6000\t0.1486s/batch\ttrain_loss = 4.3028\n",
      "Epoch 00\tBatch 6000\ttest_loss = 5.4977\n",
      "Epoch 00\tBatch 6010\t0.1491s/batch\ttrain_loss = 4.5556\n",
      "Epoch 00\tBatch 6020\t0.1503s/batch\ttrain_loss = 5.7820\n",
      "Epoch 00\tBatch 6030\t0.1492s/batch\ttrain_loss = 5.1704\n",
      "Epoch 00\tBatch 6030\ttest_loss = 5.4905\n",
      "Epoch 00\tBatch 6040\t0.1499s/batch\ttrain_loss = 5.9314\n",
      "Epoch 00\tBatch 6050\t0.1503s/batch\ttrain_loss = 5.7951\n",
      "Epoch 00\tBatch 6060\t0.1512s/batch\ttrain_loss = 6.6796\n",
      "Epoch 00\tBatch 6060\ttest_loss = 5.4965\n",
      "Epoch 00\tBatch 6070\t0.1488s/batch\ttrain_loss = 4.4018\n",
      "Epoch 00\tBatch 6080\t0.1495s/batch\ttrain_loss = 5.3524\n",
      "Epoch 00\tBatch 6090\t0.1500s/batch\ttrain_loss = 5.3477\n",
      "Epoch 00\tBatch 6090\ttest_loss = 5.4882\n",
      "Epoch 00\tBatch 6100\t0.1490s/batch\ttrain_loss = 4.9977\n",
      "Epoch 00\tBatch 6110\t0.1489s/batch\ttrain_loss = 4.5347\n",
      "Epoch 00\tBatch 6120\t0.1494s/batch\ttrain_loss = 5.2408\n",
      "Epoch 00\tBatch 6120\ttest_loss = 5.4960\n",
      "Epoch 00\tBatch 6130\t0.1503s/batch\ttrain_loss = 5.8909\n",
      "Epoch 00\tBatch 6140\t0.1483s/batch\ttrain_loss = 4.1116\n",
      "Epoch 00\tBatch 6150\t0.1488s/batch\ttrain_loss = 4.2116\n",
      "Epoch 00\tBatch 6150\ttest_loss = 5.4880\n",
      "Epoch 00\tBatch 6160\t0.1478s/batch\ttrain_loss = 3.6568\n",
      "Epoch 00\tBatch 6170\t0.1491s/batch\ttrain_loss = 4.2582\n",
      "Epoch 00\tBatch 6180\t0.1505s/batch\ttrain_loss = 5.3564\n",
      "Epoch 00\tBatch 6180\ttest_loss = 5.4824\n",
      "new best loss (5.496924 -> 5.482391)\n",
      "Epoch 00\tBatch 6190\t0.1501s/batch\ttrain_loss = 5.1832\n",
      "Epoch 00\tBatch 6200\t0.1494s/batch\ttrain_loss = 5.3076\n",
      "Epoch 00\tBatch 6210\t0.1493s/batch\ttrain_loss = 4.4418\n",
      "Epoch 00\tBatch 6210\ttest_loss = 5.5082\n",
      "Epoch 00\tBatch 6220\t0.1496s/batch\ttrain_loss = 3.9772\n",
      "Epoch 00\tBatch 6230\t0.1515s/batch\ttrain_loss = 6.0206\n",
      "Epoch 00\tBatch 6240\t0.1513s/batch\ttrain_loss = 6.7565\n",
      "Epoch 00\tBatch 6240\ttest_loss = 5.4743\n",
      "Epoch 00\tBatch 6250\t0.1504s/batch\ttrain_loss = 5.2042\n",
      "Epoch 00\tBatch 6260\t0.1504s/batch\ttrain_loss = 5.6121\n",
      "Epoch 00\tBatch 6270\t0.1506s/batch\ttrain_loss = 5.8644\n",
      "Epoch 00\tBatch 6270\ttest_loss = 5.4792\n",
      "Epoch 00\tBatch 6280\t0.1511s/batch\ttrain_loss = 6.5133\n",
      "Epoch 00\tBatch 6290\t0.1505s/batch\ttrain_loss = 5.9128\n",
      "Epoch 00\tBatch 6300\t0.1520s/batch\ttrain_loss = 6.3801\n",
      "Epoch 00\tBatch 6300\ttest_loss = 5.4820\n",
      "Epoch 00\tBatch 6310\t0.1507s/batch\ttrain_loss = 5.5776\n",
      "Epoch 00\tBatch 6320\t0.1520s/batch\ttrain_loss = 6.1219\n",
      "Epoch 00\tBatch 6330\t0.1503s/batch\ttrain_loss = 5.5291\n",
      "Epoch 00\tBatch 6330\ttest_loss = 5.4892\n",
      "Epoch 00\tBatch 6340\t0.1493s/batch\ttrain_loss = 5.3626\n",
      "Epoch 00\tBatch 6350\t0.1496s/batch\ttrain_loss = 5.3714\n",
      "Epoch 00\tBatch 6360\t0.1502s/batch\ttrain_loss = 5.8083\n",
      "Epoch 00\tBatch 6360\ttest_loss = 5.4843\n",
      "Epoch 00\tBatch 6370\t0.1502s/batch\ttrain_loss = 4.8521\n",
      "Epoch 00\tBatch 6380\t0.1487s/batch\ttrain_loss = 4.3100\n",
      "Epoch 00\tBatch 6390\t0.1520s/batch\ttrain_loss = 9.0259\n",
      "Epoch 00\tBatch 6390\ttest_loss = 5.4892\n",
      "Epoch 00\tBatch 6400\t0.1510s/batch\ttrain_loss = 6.8528\n",
      "Epoch 00\tBatch 6410\t0.1505s/batch\ttrain_loss = 5.7774\n",
      "Epoch 00\tBatch 6420\t0.1502s/batch\ttrain_loss = 5.7930\n",
      "Epoch 00\tBatch 6420\ttest_loss = 5.4754\n",
      "Epoch 00\tBatch 6430\t0.1502s/batch\ttrain_loss = 5.8547\n",
      "Epoch 00\tBatch 6440\t0.1494s/batch\ttrain_loss = 5.2051\n",
      "Epoch 00\tBatch 6450\t0.1514s/batch\ttrain_loss = 5.2493\n",
      "Epoch 00\tBatch 6450\ttest_loss = 5.4812\n",
      "Epoch 00\tBatch 6460\t0.1501s/batch\ttrain_loss = 5.2845\n",
      "Epoch 00\tBatch 6470\t0.1504s/batch\ttrain_loss = 5.3068\n",
      "Epoch 00\tBatch 6480\t0.1506s/batch\ttrain_loss = 6.3089\n",
      "Epoch 00\tBatch 6480\ttest_loss = 5.4835\n",
      "Epoch 00\tBatch 6490\t0.1488s/batch\ttrain_loss = 5.9098\n",
      "Epoch 00\tBatch 6500\t0.1610s/batch\ttrain_loss = 4.9929\n",
      "Epoch 00\tBatch 6510\t0.1508s/batch\ttrain_loss = 5.0975\n",
      "Epoch 00\tBatch 6510\ttest_loss = 5.4684\n",
      "new best loss (5.482391 -> 5.468418)\n",
      "Epoch 00\tBatch 6520\t0.1483s/batch\ttrain_loss = 4.8194\n",
      "Epoch 00\tBatch 6530\t0.1480s/batch\ttrain_loss = 5.1917\n",
      "Epoch 00\tBatch 6540\t0.1482s/batch\ttrain_loss = 4.5611\n",
      "Epoch 00\tBatch 6540\ttest_loss = 5.4801\n",
      "Epoch 00\tBatch 6550\t0.1474s/batch\ttrain_loss = 5.0359\n",
      "Epoch 00\tBatch 6560\t0.1485s/batch\ttrain_loss = 5.2022\n",
      "Epoch 00\tBatch 6570\t0.1447s/batch\ttrain_loss = 4.9778\n",
      "Epoch 00\tBatch 6570\ttest_loss = 5.4705\n",
      "Epoch 00\tBatch 6580\t0.1388s/batch\ttrain_loss = 5.7878\n",
      "Epoch 00\tBatch 6590\t0.1379s/batch\ttrain_loss = 5.1790\n",
      "Epoch 00\tBatch 6600\t0.1379s/batch\ttrain_loss = 5.1576\n",
      "Epoch 00\tBatch 6600\ttest_loss = 5.4826\n",
      "Epoch 00\tBatch 6610\t0.1385s/batch\ttrain_loss = 5.0169\n",
      "Epoch 00\tBatch 6620\t0.1384s/batch\ttrain_loss = 5.4687\n",
      "Epoch 00\tBatch 6630\t0.1415s/batch\ttrain_loss = 5.7523\n",
      "Epoch 00\tBatch 6630\ttest_loss = 5.4798\n",
      "Epoch 00\tBatch 6640\t0.1400s/batch\ttrain_loss = 6.2632\n",
      "Epoch 00\tBatch 6650\t0.1401s/batch\ttrain_loss = 6.9923\n",
      "Epoch 00\tBatch 6660\t0.1401s/batch\ttrain_loss = 6.1471\n",
      "Epoch 00\tBatch 6660\ttest_loss = 5.4907\n",
      "Epoch 00\tBatch 6670\t0.1402s/batch\ttrain_loss = 6.0333\n",
      "Epoch 00\tBatch 6680\t0.1391s/batch\ttrain_loss = 5.3879\n",
      "Epoch 00\tBatch 6690\t0.1398s/batch\ttrain_loss = 5.7839\n",
      "Epoch 00\tBatch 6690\ttest_loss = 5.4815\n",
      "Epoch 00\tBatch 6700\t0.1412s/batch\ttrain_loss = 6.7946\n",
      "Epoch 00\tBatch 6710\t0.1463s/batch\ttrain_loss = 6.0853\n",
      "Epoch 00\tBatch 6720\t0.1458s/batch\ttrain_loss = 7.5221\n",
      "Epoch 00\tBatch 6720\ttest_loss = 5.4682\n",
      "Epoch 00\tBatch 6730\t0.1399s/batch\ttrain_loss = 5.8135\n",
      "Epoch 00\tBatch 6740\t0.1390s/batch\ttrain_loss = 5.6868\n",
      "Epoch 00\tBatch 6750\t0.1452s/batch\ttrain_loss = 5.5320\n",
      "Epoch 00\tBatch 6750\ttest_loss = 5.4654\n",
      "Epoch 00\tBatch 6760\t0.1381s/batch\ttrain_loss = 4.9070\n",
      "Epoch 00\tBatch 6770\t0.1386s/batch\ttrain_loss = 5.0170\n",
      "Epoch 00\tBatch 6780\t0.1533s/batch\ttrain_loss = 5.8031\n",
      "Epoch 00\tBatch 6780\ttest_loss = 5.4793\n",
      "Epoch 00\tBatch 6790\t0.1391s/batch\ttrain_loss = 6.6569\n",
      "Epoch 00\tBatch 6800\t0.1397s/batch\ttrain_loss = 7.2332\n",
      "Epoch 00\tBatch 6810\t0.1398s/batch\ttrain_loss = 6.0332\n",
      "Epoch 00\tBatch 6810\ttest_loss = 5.4748\n",
      "Epoch 00\tBatch 6820\t0.1404s/batch\ttrain_loss = 6.1736\n",
      "Epoch 00\tBatch 6830\t0.1403s/batch\ttrain_loss = 5.7535\n",
      "Epoch 00\tBatch 6840\t0.1466s/batch\ttrain_loss = 7.2619\n",
      "Epoch 00\tBatch 6840\ttest_loss = 5.4760\n",
      "Epoch 00\tBatch 6850\t0.1414s/batch\ttrain_loss = 5.9487\n",
      "Epoch 00\tBatch 6860\t0.1414s/batch\ttrain_loss = 5.0914\n",
      "Epoch 00\tBatch 6870\t0.1379s/batch\ttrain_loss = 5.2409\n",
      "Epoch 00\tBatch 6870\ttest_loss = 5.4784\n",
      "Epoch 00\tBatch 6880\t0.1386s/batch\ttrain_loss = 4.6282\n",
      "Epoch 00\tBatch 6890\t0.1401s/batch\ttrain_loss = 4.2316\n",
      "Epoch 00\tBatch 6900\t0.1397s/batch\ttrain_loss = 6.2038\n",
      "Epoch 00\tBatch 6900\ttest_loss = 5.4981\n",
      "Epoch 00\tBatch 6910\t0.1391s/batch\ttrain_loss = 5.2964\n",
      "Epoch 00\tBatch 6920\t0.1383s/batch\ttrain_loss = 4.1416\n",
      "Epoch 00\tBatch 6930\t0.1397s/batch\ttrain_loss = 6.1974\n",
      "Epoch 00\tBatch 6930\ttest_loss = 5.4694\n",
      "Epoch 00\tBatch 6940\t0.1388s/batch\ttrain_loss = 5.8646\n",
      "Epoch 00\tBatch 6950\t0.1401s/batch\ttrain_loss = 6.2626\n",
      "Epoch 00\tBatch 6960\t0.1382s/batch\ttrain_loss = 5.4386\n",
      "Epoch 00\tBatch 6960\ttest_loss = 5.5125\n",
      "Epoch 00\tBatch 6970\t0.1422s/batch\ttrain_loss = 6.0403\n",
      "Epoch 00\tBatch 6980\t0.1389s/batch\ttrain_loss = 7.2112\n",
      "Epoch 00\tBatch 6990\t0.1393s/batch\ttrain_loss = 6.4980\n",
      "Epoch 00\tBatch 6990\ttest_loss = 5.4779\n",
      "Epoch 00\tBatch 7000\t0.1454s/batch\ttrain_loss = 6.6785\n",
      "Epoch 00\tBatch 7010\t0.1509s/batch\ttrain_loss = 6.3012\n",
      "Epoch 00\tBatch 7020\t0.1407s/batch\ttrain_loss = 7.4751\n",
      "Epoch 00\tBatch 7020\ttest_loss = 5.4765\n",
      "Epoch 00\tBatch 7030\t0.1406s/batch\ttrain_loss = 5.9304\n",
      "Epoch 00\tBatch 7040\t0.1401s/batch\ttrain_loss = 6.4479\n",
      "Epoch 00\tBatch 7050\t0.1390s/batch\ttrain_loss = 6.0855\n",
      "Epoch 00\tBatch 7050\ttest_loss = 5.4919\n",
      "Epoch 00\tBatch 7060\t0.1417s/batch\ttrain_loss = 5.6630\n",
      "Epoch 00\tBatch 7070\t0.1417s/batch\ttrain_loss = 6.0719\n",
      "Epoch 00\tBatch 7080\t0.1394s/batch\ttrain_loss = 5.2475\n",
      "Epoch 00\tBatch 7080\ttest_loss = 5.5299\n",
      "Epoch 00\tBatch 7090\t0.1392s/batch\ttrain_loss = 5.7369\n",
      "Epoch 00\tBatch 7100\t0.1398s/batch\ttrain_loss = 5.5078\n",
      "Epoch 00\tBatch 7110\t0.1390s/batch\ttrain_loss = 5.6540\n",
      "Epoch 00\tBatch 7110\ttest_loss = 5.4835\n",
      "Epoch 00\tBatch 7120\t0.1389s/batch\ttrain_loss = 5.0966\n",
      "Epoch 00\tBatch 7130\t0.1384s/batch\ttrain_loss = 5.6387\n",
      "Epoch 00\tBatch 7140\t0.1381s/batch\ttrain_loss = 5.0277\n",
      "Epoch 00\tBatch 7140\ttest_loss = 5.4726\n",
      "Epoch 00\tBatch 7150\t0.1393s/batch\ttrain_loss = 5.9990\n",
      "Epoch 00\tBatch 7160\t0.1404s/batch\ttrain_loss = 6.5638\n",
      "Epoch 00\tBatch 7170\t0.1395s/batch\ttrain_loss = 6.0556\n",
      "Epoch 00\tBatch 7170\ttest_loss = 5.4820\n",
      "Epoch 00\tBatch 7180\t0.1414s/batch\ttrain_loss = 6.0503\n",
      "Epoch 00\tBatch 7190\t0.1426s/batch\ttrain_loss = 5.8722\n",
      "Epoch 00\tBatch 7200\t0.1409s/batch\ttrain_loss = 6.2494\n",
      "Epoch 00\tBatch 7200\ttest_loss = 5.4786\n",
      "Epoch 00\tBatch 7210\t0.1546s/batch\ttrain_loss = 5.8294\n",
      "Epoch 00\tBatch 7220\t0.1409s/batch\ttrain_loss = 6.1598\n",
      "Epoch 00\tBatch 7230\t0.1394s/batch\ttrain_loss = 6.2593\n",
      "Epoch 00\tBatch 7230\ttest_loss = 5.4782\n",
      "Epoch 00\tBatch 7240\t0.1418s/batch\ttrain_loss = 6.7075\n",
      "Epoch 00\tBatch 7250\t0.1454s/batch\ttrain_loss = 5.3275\n",
      "Epoch 00\tBatch 7260\t0.1409s/batch\ttrain_loss = 5.8249\n",
      "Epoch 00\tBatch 7260\ttest_loss = 5.4786\n",
      "Epoch 00\tBatch 7270\t0.1405s/batch\ttrain_loss = 5.9631\n",
      "Epoch 00\tBatch 7280\t0.1388s/batch\ttrain_loss = 5.6876\n",
      "Epoch 00\tBatch 7290\t0.1391s/batch\ttrain_loss = 5.1883\n",
      "Epoch 00\tBatch 7290\ttest_loss = 5.4882\n",
      "Epoch 00\tBatch 7300\t0.1436s/batch\ttrain_loss = 4.9185\n",
      "Epoch 00\tBatch 7310\t0.1511s/batch\ttrain_loss = 5.0184\n",
      "Epoch 00\tBatch 7320\t0.1427s/batch\ttrain_loss = 4.9438\n",
      "Epoch 00\tBatch 7320\ttest_loss = 5.4977\n",
      "Epoch 00\tBatch 7330\t0.1393s/batch\ttrain_loss = 4.7499\n",
      "Epoch 00\tBatch 7340\t0.1454s/batch\ttrain_loss = 5.0522\n",
      "Epoch 00\tBatch 7350\t0.1379s/batch\ttrain_loss = 4.4792\n",
      "Epoch 00\tBatch 7350\ttest_loss = 5.5011\n",
      "Epoch 00\tBatch 7360\t0.1504s/batch\ttrain_loss = 4.6041\n",
      "Epoch 00\tBatch 7370\t0.1482s/batch\ttrain_loss = 5.7429\n",
      "Epoch 00\tBatch 7380\t0.1478s/batch\ttrain_loss = 4.9515\n",
      "Epoch 00\tBatch 7380\ttest_loss = 5.4719\n",
      "Epoch 00\tBatch 7390\t0.1400s/batch\ttrain_loss = 6.2454\n",
      "Epoch 00\tBatch 7400\t0.1387s/batch\ttrain_loss = 5.2171\n",
      "Epoch 00\tBatch 7410\t0.1419s/batch\ttrain_loss = 5.8156\n",
      "Epoch 00\tBatch 7410\ttest_loss = 5.4621\n",
      "Epoch 00\tBatch 7420\t0.1383s/batch\ttrain_loss = 4.8670\n",
      "Epoch 00\tBatch 7430\t0.1403s/batch\ttrain_loss = 6.0229\n",
      "Epoch 00\tBatch 7440\t0.1419s/batch\ttrain_loss = 6.6203\n",
      "Epoch 00\tBatch 7440\ttest_loss = 5.4626\n",
      "Epoch 00\tBatch 7450\t0.1402s/batch\ttrain_loss = 5.7941\n",
      "Epoch 00\tBatch 7460\t0.1395s/batch\ttrain_loss = 5.6501\n",
      "Epoch 00\tBatch 7470\t0.1375s/batch\ttrain_loss = 3.9616\n",
      "Epoch 00\tBatch 7470\ttest_loss = 5.4803\n",
      "Epoch 00\tBatch 7480\t0.1398s/batch\ttrain_loss = 4.9926\n",
      "Epoch 00\tBatch 7490\t0.1388s/batch\ttrain_loss = 5.2987\n",
      "Epoch 00\tBatch 7500\t0.1429s/batch\ttrain_loss = 5.4369\n",
      "Epoch 00\tBatch 7500\ttest_loss = 5.4721\n",
      "Epoch 00\tBatch 7510\t0.1394s/batch\ttrain_loss = 4.7886\n",
      "Epoch 00\tBatch 7520\t0.1389s/batch\ttrain_loss = 4.8153\n",
      "Epoch 00\tBatch 7530\t0.1390s/batch\ttrain_loss = 4.8331\n",
      "Epoch 00\tBatch 7530\ttest_loss = 5.4502\n",
      "new best loss (5.468418 -> 5.450162)\n",
      "Epoch 00\tBatch 7540\t0.1379s/batch\ttrain_loss = 4.1791\n",
      "Epoch 00\tBatch 7550\t0.1471s/batch\ttrain_loss = 4.7946\n",
      "Epoch 00\tBatch 7560\t0.1463s/batch\ttrain_loss = 5.4546\n",
      "Epoch 00\tBatch 7560\ttest_loss = 5.4585\n",
      "Epoch 00\tBatch 7570\t0.1444s/batch\ttrain_loss = 5.4719\n",
      "Epoch 00\tBatch 7580\t0.1492s/batch\ttrain_loss = 4.1308\n",
      "Epoch 00\tBatch 7590\t0.1411s/batch\ttrain_loss = 4.5516\n",
      "Epoch 00\tBatch 7590\ttest_loss = 5.4602\n",
      "Epoch 00\tBatch 7600\t0.1400s/batch\ttrain_loss = 5.0808\n",
      "Epoch 00\tBatch 7610\t0.1386s/batch\ttrain_loss = 4.5782\n",
      "Epoch 00\tBatch 7620\t0.1403s/batch\ttrain_loss = 4.2775\n",
      "Epoch 00\tBatch 7620\ttest_loss = 5.4546\n",
      "Epoch 00\tBatch 7630\t0.1388s/batch\ttrain_loss = 4.6294\n",
      "Epoch 00\tBatch 7640\t0.1403s/batch\ttrain_loss = 4.2849\n",
      "Epoch 00\tBatch 7650\t0.1401s/batch\ttrain_loss = 5.1460\n",
      "Epoch 00\tBatch 7650\ttest_loss = 5.4604\n",
      "Epoch 00\tBatch 7660\t0.1399s/batch\ttrain_loss = 5.6664\n",
      "Epoch 00\tBatch 7670\t0.1412s/batch\ttrain_loss = 5.7522\n",
      "Epoch 00\tBatch 7680\t0.1434s/batch\ttrain_loss = 6.1941\n",
      "Epoch 00\tBatch 7680\ttest_loss = 5.4743\n",
      "Epoch 00\tBatch 7690\t0.1406s/batch\ttrain_loss = 5.9060\n",
      "Epoch 00\tBatch 7700\t0.1409s/batch\ttrain_loss = 4.2757\n",
      "Epoch 00\tBatch 7710\t0.1513s/batch\ttrain_loss = 4.3753\n",
      "Epoch 00\tBatch 7710\ttest_loss = 5.4583\n",
      "Epoch 00\tBatch 7720\t0.1443s/batch\ttrain_loss = 5.0729\n",
      "Epoch 00\tBatch 7730\t0.1408s/batch\ttrain_loss = 6.6491\n",
      "Epoch 00\tBatch 7740\t0.1421s/batch\ttrain_loss = 7.0728\n",
      "Epoch 00\tBatch 7740\ttest_loss = 5.4546\n",
      "Epoch 00\tBatch 7750\t0.1450s/batch\ttrain_loss = 5.6618\n",
      "Epoch 00\tBatch 7760\t0.1401s/batch\ttrain_loss = 5.5099\n",
      "Epoch 00\tBatch 7770\t0.1411s/batch\ttrain_loss = 7.1356\n",
      "Epoch 00\tBatch 7770\ttest_loss = 5.4623\n",
      "Epoch 00\tBatch 7780\t0.1502s/batch\ttrain_loss = 6.1338\n",
      "Epoch 00\tBatch 7790\t0.1415s/batch\ttrain_loss = 7.9861\n",
      "Epoch 00\tBatch 7800\t0.1407s/batch\ttrain_loss = 5.2870\n",
      "Epoch 00\tBatch 7800\ttest_loss = 5.4493\n",
      "Epoch 00\tBatch 7810\t0.1467s/batch\ttrain_loss = 6.1493\n",
      "Epoch 00\tBatch 7820\t0.1413s/batch\ttrain_loss = 5.9648\n",
      "Epoch 00\tBatch 7830\t0.1445s/batch\ttrain_loss = 5.4497\n",
      "Epoch 00\tBatch 7830\ttest_loss = 5.4645\n",
      "Epoch 00\tBatch 7840\t0.1384s/batch\ttrain_loss = 4.4026\n",
      "Epoch 00\tBatch 7850\t0.1384s/batch\ttrain_loss = 3.9751\n",
      "Epoch 00\tBatch 7860\t0.1427s/batch\ttrain_loss = 4.9797\n",
      "Epoch 00\tBatch 7860\ttest_loss = 5.4672\n",
      "Epoch 00\tBatch 7870\t0.1393s/batch\ttrain_loss = 5.1298\n",
      "Epoch 00\tBatch 7880\t0.1398s/batch\ttrain_loss = 5.1642\n",
      "Epoch 00\tBatch 7890\t0.1386s/batch\ttrain_loss = 5.2323\n",
      "Epoch 00\tBatch 7890\ttest_loss = 5.4644\n",
      "Epoch 00\tBatch 7900\t0.1385s/batch\ttrain_loss = 5.0243\n",
      "Epoch 00\tBatch 7910\t0.1378s/batch\ttrain_loss = 4.9739\n",
      "Epoch 00\tBatch 7920\t0.1399s/batch\ttrain_loss = 5.7985\n",
      "Epoch 00\tBatch 7920\ttest_loss = 5.4697\n",
      "Epoch 00\tBatch 7930\t0.1406s/batch\ttrain_loss = 6.0789\n",
      "Epoch 00\tBatch 7940\t0.1388s/batch\ttrain_loss = 5.9361\n",
      "Epoch 00\tBatch 7950\t0.1391s/batch\ttrain_loss = 5.1376\n",
      "Epoch 00\tBatch 7950\ttest_loss = 5.4858\n",
      "Epoch 00\tBatch 7960\t0.1401s/batch\ttrain_loss = 6.1478\n",
      "Epoch 00\tBatch 7970\t0.1404s/batch\ttrain_loss = 7.1881\n",
      "Epoch 00\tBatch 7980\t0.1406s/batch\ttrain_loss = 7.1780\n",
      "Epoch 00\tBatch 7980\ttest_loss = 5.4844\n",
      "Epoch 00\tBatch 7990\t0.1399s/batch\ttrain_loss = 5.4007\n",
      "Epoch 00\tBatch 8000\t0.1399s/batch\ttrain_loss = 5.3525\n",
      "Epoch 00\tBatch 8010\t0.1396s/batch\ttrain_loss = 5.0244\n",
      "Epoch 00\tBatch 8010\ttest_loss = 5.4824\n",
      "Epoch 00\tBatch 8020\t0.1404s/batch\ttrain_loss = 5.1982\n",
      "Epoch 00\tBatch 8030\t0.1401s/batch\ttrain_loss = 5.6381\n",
      "Epoch 00\tBatch 8040\t0.1398s/batch\ttrain_loss = 5.5041\n",
      "Epoch 00\tBatch 8040\ttest_loss = 5.4747\n",
      "Epoch 00\tBatch 8050\t0.1389s/batch\ttrain_loss = 4.8160\n",
      "Epoch 00\tBatch 8060\t0.1392s/batch\ttrain_loss = 5.1330\n",
      "Epoch 00\tBatch 8070\t0.1385s/batch\ttrain_loss = 4.9036\n",
      "Epoch 00\tBatch 8070\ttest_loss = 5.4755\n",
      "Epoch 00\tBatch 8080\t0.1390s/batch\ttrain_loss = 4.6975\n",
      "Epoch 00\tBatch 8090\t0.1387s/batch\ttrain_loss = 4.7644\n",
      "Epoch 00\tBatch 8100\t0.1392s/batch\ttrain_loss = 4.2731\n",
      "Epoch 00\tBatch 8100\ttest_loss = 5.4747\n",
      "Epoch 00\tBatch 8110\t0.1383s/batch\ttrain_loss = 4.4685\n",
      "Epoch 00\tBatch 8120\t0.1461s/batch\ttrain_loss = 4.0779\n",
      "Epoch 00\tBatch 8130\t0.1480s/batch\ttrain_loss = 6.0486\n",
      "Epoch 00\tBatch 8130\ttest_loss = 5.4679\n",
      "Epoch 00\tBatch 8140\t0.1394s/batch\ttrain_loss = 6.0156\n",
      "Epoch 00\tBatch 8150\t0.1397s/batch\ttrain_loss = 6.4020\n",
      "Epoch 00\tBatch 8160\t0.1387s/batch\ttrain_loss = 5.5241\n",
      "Epoch 00\tBatch 8160\ttest_loss = 5.4459\n",
      "Epoch 00\tBatch 8170\t0.1366s/batch\ttrain_loss = 5.0499\n",
      "Epoch 00\tBatch 8180\t0.1370s/batch\ttrain_loss = 5.1280\n",
      "Epoch 00\tBatch 8190\t0.1379s/batch\ttrain_loss = 4.8691\n",
      "Epoch 00\tBatch 8190\ttest_loss = 5.4577\n",
      "Epoch 00\tBatch 8200\t0.1379s/batch\ttrain_loss = 5.6525\n",
      "Epoch 00\tBatch 8210\t0.1371s/batch\ttrain_loss = 4.5853\n",
      "Epoch 00\tBatch 8220\t0.1382s/batch\ttrain_loss = 5.3804\n",
      "Epoch 00\tBatch 8220\ttest_loss = 5.4508\n",
      "Epoch 00\tBatch 8230\t0.1392s/batch\ttrain_loss = 5.4002\n",
      "Epoch 00\tBatch 8240\t0.1389s/batch\ttrain_loss = 4.8049\n",
      "Epoch 00\tBatch 8250\t0.1382s/batch\ttrain_loss = 4.6875\n",
      "Epoch 00\tBatch 8250\ttest_loss = 5.4534\n",
      "Epoch 00\tBatch 8260\t0.1411s/batch\ttrain_loss = 7.7678\n",
      "Epoch 00\tBatch 8270\t0.1406s/batch\ttrain_loss = 7.9064\n",
      "Epoch 00\tBatch 8280\t0.1374s/batch\ttrain_loss = 5.2211\n",
      "Epoch 00\tBatch 8280\ttest_loss = 5.4536\n",
      "Epoch 00\tBatch 8290\t0.1385s/batch\ttrain_loss = 4.6901\n",
      "Epoch 00\tBatch 8300\t0.1407s/batch\ttrain_loss = 7.1288\n",
      "Epoch 00\tBatch 8310\t0.1408s/batch\ttrain_loss = 7.7001\n",
      "Epoch 00\tBatch 8310\ttest_loss = 5.4442\n",
      "Epoch 00\tBatch 8320\t0.1392s/batch\ttrain_loss = 6.4239\n",
      "Epoch 00\tBatch 8330\t0.1393s/batch\ttrain_loss = 6.3899\n",
      "Epoch 00\tBatch 8340\t0.1372s/batch\ttrain_loss = 4.5472\n",
      "Epoch 00\tBatch 8340\ttest_loss = 5.4496\n",
      "Epoch 00\tBatch 8350\t0.1394s/batch\ttrain_loss = 5.5885\n",
      "Epoch 00\tBatch 8360\t0.1380s/batch\ttrain_loss = 4.6283\n",
      "Epoch 00\tBatch 8370\t0.1397s/batch\ttrain_loss = 7.6255\n",
      "Epoch 00\tBatch 8370\ttest_loss = 5.4588\n",
      "Epoch 00\tBatch 8380\t0.1400s/batch\ttrain_loss = 6.2399\n",
      "Epoch 00\tBatch 8390\t0.1386s/batch\ttrain_loss = 5.3600\n",
      "Epoch 00\tBatch 8400\t0.1391s/batch\ttrain_loss = 5.7146\n",
      "Epoch 00\tBatch 8400\ttest_loss = 5.4614\n",
      "Epoch 00\tBatch 8410\t0.1383s/batch\ttrain_loss = 5.8395\n",
      "Epoch 00\tBatch 8420\t0.1385s/batch\ttrain_loss = 5.8299\n",
      "Epoch 00\tBatch 8430\t0.1396s/batch\ttrain_loss = 6.4889\n",
      "Epoch 00\tBatch 8430\ttest_loss = 5.4707\n",
      "Epoch 00\tBatch 8440\t0.1435s/batch\ttrain_loss = 6.6750\n",
      "Epoch 00\tBatch 8450\t0.1391s/batch\ttrain_loss = 4.8819\n",
      "Epoch 00\tBatch 8460\t0.1390s/batch\ttrain_loss = 4.9713\n",
      "Epoch 00\tBatch 8460\ttest_loss = 5.4472\n",
      "Epoch 00\tBatch 8470\t0.1369s/batch\ttrain_loss = 4.6846\n",
      "Epoch 00\tBatch 8480\t0.1385s/batch\ttrain_loss = 5.4060\n",
      "Epoch 00\tBatch 8490\t0.1387s/batch\ttrain_loss = 5.2791\n",
      "Epoch 00\tBatch 8490\ttest_loss = 5.4374\n",
      "new best loss (5.450162 -> 5.437356)\n",
      "Epoch 00\tBatch 8500\t0.1393s/batch\ttrain_loss = 6.6996\n",
      "Epoch 00\tBatch 8510\t0.1387s/batch\ttrain_loss = 5.7508\n",
      "Epoch 00\tBatch 8520\t0.1400s/batch\ttrain_loss = 5.6481\n",
      "Epoch 00\tBatch 8520\ttest_loss = 5.4466\n",
      "Epoch 00\tBatch 8530\t0.1395s/batch\ttrain_loss = 6.5470\n",
      "Epoch 00\tBatch 8540\t0.1430s/batch\ttrain_loss = 4.9659\n",
      "Epoch 00\tBatch 8550\t0.1378s/batch\ttrain_loss = 4.5660\n",
      "Epoch 00\tBatch 8550\ttest_loss = 5.4334\n",
      "Epoch 00\tBatch 8560\t0.1382s/batch\ttrain_loss = 4.8061\n",
      "Epoch 00\tBatch 8570\t0.1396s/batch\ttrain_loss = 5.5340\n",
      "Epoch 00\tBatch 8580\t0.1398s/batch\ttrain_loss = 5.3144\n",
      "Epoch 00\tBatch 8580\ttest_loss = 5.4496\n",
      "Epoch 00\tBatch 8590\t0.1436s/batch\ttrain_loss = 6.9356\n",
      "Epoch 00\tBatch 8600\t0.1406s/batch\ttrain_loss = 7.2414\n",
      "Epoch 00\tBatch 8610\t0.1418s/batch\ttrain_loss = 7.1691\n",
      "Epoch 00\tBatch 8610\ttest_loss = 5.4457\n",
      "Epoch 00\tBatch 8620\t0.1405s/batch\ttrain_loss = 7.2143\n",
      "Epoch 00\tBatch 8630\t0.1394s/batch\ttrain_loss = 6.2012\n",
      "Epoch 00\tBatch 8640\t0.1397s/batch\ttrain_loss = 5.7544\n",
      "Epoch 00\tBatch 8640\ttest_loss = 5.4474\n",
      "Epoch 00\tBatch 8650\t0.1426s/batch\ttrain_loss = 6.7612\n",
      "Epoch 00\tBatch 8660\t0.1543s/batch\ttrain_loss = 6.0066\n",
      "Epoch 00\tBatch 8670\t0.1516s/batch\ttrain_loss = 3.7331\n",
      "Epoch 00\tBatch 8670\ttest_loss = 5.4431\n",
      "Epoch 00\tBatch 8680\t0.1393s/batch\ttrain_loss = 4.8011\n",
      "Epoch 00\tBatch 8690\t0.1408s/batch\ttrain_loss = 6.3788\n",
      "Epoch 00\tBatch 8700\t0.1532s/batch\ttrain_loss = 6.5467\n",
      "Epoch 00\tBatch 8700\ttest_loss = 5.4483\n",
      "Epoch 00\tBatch 8710\t0.1403s/batch\ttrain_loss = 6.7463\n",
      "Epoch 00\tBatch 8720\t0.1489s/batch\ttrain_loss = 4.7326\n",
      "Epoch 00\tBatch 8730\t0.1444s/batch\ttrain_loss = 4.5794\n",
      "Epoch 00\tBatch 8730\ttest_loss = 5.4757\n",
      "Epoch 00\tBatch 8740\t0.1407s/batch\ttrain_loss = 5.3386\n",
      "Epoch 00\tBatch 8750\t0.1385s/batch\ttrain_loss = 6.4040\n",
      "Epoch 00\tBatch 8760\t0.1396s/batch\ttrain_loss = 4.8067\n",
      "Epoch 00\tBatch 8760\ttest_loss = 5.4525\n",
      "Epoch 00\tBatch 8770\t0.1375s/batch\ttrain_loss = 4.7143\n",
      "Epoch 00\tBatch 8780\t0.1381s/batch\ttrain_loss = 4.6825\n",
      "Epoch 00\tBatch 8790\t0.1476s/batch\ttrain_loss = 6.2755\n",
      "Epoch 00\tBatch 8790\ttest_loss = 5.4484\n",
      "Epoch 00\tBatch 8800\t0.1399s/batch\ttrain_loss = 5.9308\n",
      "Epoch 00\tBatch 8810\t0.1406s/batch\ttrain_loss = 5.8382\n",
      "Epoch 00\tBatch 8820\t0.1396s/batch\ttrain_loss = 5.4587\n",
      "Epoch 00\tBatch 8820\ttest_loss = 5.4417\n",
      "Epoch 00\tBatch 8830\t0.1398s/batch\ttrain_loss = 7.2553\n",
      "Epoch 00\tBatch 8840\t0.1386s/batch\ttrain_loss = 6.1550\n",
      "Epoch 00\tBatch 8850\t0.1381s/batch\ttrain_loss = 5.0439\n",
      "Epoch 00\tBatch 8850\ttest_loss = 5.4581\n",
      "Epoch 00\tBatch 8860\t0.1384s/batch\ttrain_loss = 4.4637\n",
      "Epoch 00\tBatch 8870\t0.1388s/batch\ttrain_loss = 4.6404\n",
      "Epoch 00\tBatch 8880\t0.1397s/batch\ttrain_loss = 6.0740\n",
      "Epoch 00\tBatch 8880\ttest_loss = 5.4667\n",
      "Epoch 00\tBatch 8890\t0.1395s/batch\ttrain_loss = 6.4017\n",
      "Epoch 00\tBatch 8900\t0.1370s/batch\ttrain_loss = 4.4941\n",
      "Epoch 00\tBatch 8910\t0.1377s/batch\ttrain_loss = 5.1397\n",
      "Epoch 00\tBatch 8910\ttest_loss = 5.4499\n",
      "Epoch 00\tBatch 8920\t0.1396s/batch\ttrain_loss = 6.7920\n",
      "Epoch 00\tBatch 8930\t0.1386s/batch\ttrain_loss = 5.4096\n",
      "Epoch 00\tBatch 8940\t0.1401s/batch\ttrain_loss = 5.8181\n",
      "Epoch 00\tBatch 8940\ttest_loss = 5.4382\n",
      "Epoch 00\tBatch 8950\t0.1389s/batch\ttrain_loss = 5.7520\n",
      "Epoch 00\tBatch 8960\t0.1400s/batch\ttrain_loss = 6.7860\n",
      "Epoch 00\tBatch 8970\t0.1405s/batch\ttrain_loss = 5.4781\n",
      "Epoch 00\tBatch 8970\ttest_loss = 5.4463\n",
      "Epoch 00\tBatch 8980\t0.1402s/batch\ttrain_loss = 5.6256\n",
      "Epoch 00\tBatch 8990\t0.1388s/batch\ttrain_loss = 6.2091\n",
      "Epoch 00\tBatch 9000\t0.1404s/batch\ttrain_loss = 5.9621\n",
      "Epoch 00\tBatch 9000\ttest_loss = 5.4453\n",
      "Epoch 00\tBatch 9010\t0.1416s/batch\ttrain_loss = 7.0351\n",
      "Epoch 00\tBatch 9020\t0.1395s/batch\ttrain_loss = 5.4199\n",
      "Epoch 00\tBatch 9030\t0.1384s/batch\ttrain_loss = 5.4530\n",
      "Epoch 00\tBatch 9030\ttest_loss = 5.4490\n",
      "Epoch 00\tBatch 9040\t0.1414s/batch\ttrain_loss = 6.2234\n",
      "Epoch 00\tBatch 9050\t0.1418s/batch\ttrain_loss = 6.6184\n",
      "Epoch 00\tBatch 9060\t0.1459s/batch\ttrain_loss = 6.1881\n",
      "Epoch 00\tBatch 9060\ttest_loss = 5.4459\n",
      "Epoch 00\tBatch 9070\t0.1403s/batch\ttrain_loss = 6.4475\n",
      "Epoch 00\tBatch 9080\t0.1398s/batch\ttrain_loss = 6.5142\n",
      "Epoch 00\tBatch 9090\t0.1384s/batch\ttrain_loss = 5.4691\n",
      "Epoch 00\tBatch 9090\ttest_loss = 5.4496\n",
      "Epoch 00\tBatch 9100\t0.1377s/batch\ttrain_loss = 5.3154\n",
      "Epoch 00\tBatch 9110\t0.1382s/batch\ttrain_loss = 5.4475\n",
      "Epoch 00\tBatch 9120\t0.1411s/batch\ttrain_loss = 5.3348\n",
      "Epoch 00\tBatch 9120\ttest_loss = 5.4453\n",
      "Epoch 00\tBatch 9130\t0.1417s/batch\ttrain_loss = 6.1424\n",
      "Epoch 00\tBatch 9140\t0.1432s/batch\ttrain_loss = 6.6768\n",
      "Epoch 00\tBatch 9150\t0.1452s/batch\ttrain_loss = 6.2694\n",
      "Epoch 00\tBatch 9150\ttest_loss = 5.4483\n",
      "Epoch 00\tBatch 9160\t0.1406s/batch\ttrain_loss = 6.2939\n",
      "Epoch 00\tBatch 9170\t0.1407s/batch\ttrain_loss = 5.8690\n",
      "Epoch 00\tBatch 9180\t0.1409s/batch\ttrain_loss = 5.3056\n",
      "Epoch 00\tBatch 9180\ttest_loss = 5.4509\n",
      "Epoch 00\tBatch 9190\t0.1392s/batch\ttrain_loss = 4.7176\n",
      "Epoch 00\tBatch 9200\t0.1398s/batch\ttrain_loss = 5.0833\n",
      "Epoch 00\tBatch 9210\t0.1410s/batch\ttrain_loss = 5.9058\n",
      "Epoch 00\tBatch 9210\ttest_loss = 5.4299\n",
      "Epoch 00\tBatch 9220\t0.1457s/batch\ttrain_loss = 5.4500\n",
      "Epoch 00\tBatch 9230\t0.1661s/batch\ttrain_loss = 5.2961\n",
      "Epoch 00\tBatch 9240\t0.1439s/batch\ttrain_loss = 5.6205\n",
      "Epoch 00\tBatch 9240\ttest_loss = 5.4319\n",
      "Epoch 00\tBatch 9250\t0.1461s/batch\ttrain_loss = 5.1071\n",
      "Epoch 00\tBatch 9260\t0.1476s/batch\ttrain_loss = 4.9684\n",
      "Epoch 00\tBatch 9270\t0.1450s/batch\ttrain_loss = 5.3987\n",
      "Epoch 00\tBatch 9270\ttest_loss = 5.4322\n",
      "Epoch 00\tBatch 9280\t0.1387s/batch\ttrain_loss = 5.0311\n",
      "Epoch 00\tBatch 9290\t0.1391s/batch\ttrain_loss = 5.4500\n",
      "Epoch 00\tBatch 9300\t0.1379s/batch\ttrain_loss = 5.2289\n",
      "Epoch 00\tBatch 9300\ttest_loss = 5.4607\n",
      "Epoch 00\tBatch 9310\t0.1385s/batch\ttrain_loss = 6.0486\n",
      "Epoch 00\tBatch 9320\t0.1391s/batch\ttrain_loss = 5.1019\n",
      "Epoch 00\tBatch 9330\t0.1390s/batch\ttrain_loss = 4.9688\n",
      "Epoch 00\tBatch 9330\ttest_loss = 5.4355\n",
      "Epoch 00\tBatch 9340\t0.1376s/batch\ttrain_loss = 4.3856\n",
      "Epoch 00\tBatch 9350\t0.1382s/batch\ttrain_loss = 4.9378\n",
      "Epoch 00\tBatch 9360\t0.1393s/batch\ttrain_loss = 5.3883\n",
      "Epoch 00\tBatch 9360\ttest_loss = 5.4356\n",
      "Epoch 00\tBatch 9370\t0.1390s/batch\ttrain_loss = 5.0741\n",
      "Epoch 00\tBatch 9380\t0.1383s/batch\ttrain_loss = 5.5117\n",
      "Epoch 00\tBatch 9390\t0.1392s/batch\ttrain_loss = 5.8571\n",
      "Epoch 00\tBatch 9390\ttest_loss = 5.4220\n",
      "new best loss (5.437356 -> 5.422029)\n",
      "Epoch 00\tBatch 9400\t0.1403s/batch\ttrain_loss = 5.1475\n",
      "Epoch 00\tBatch 9410\t0.1391s/batch\ttrain_loss = 5.1681\n",
      "Epoch 00\tBatch 9420\t0.1395s/batch\ttrain_loss = 6.1349\n",
      "Epoch 00\tBatch 9420\ttest_loss = 5.4253\n",
      "Epoch 00\tBatch 9430\t0.1400s/batch\ttrain_loss = 6.5393\n",
      "Epoch 00\tBatch 9440\t0.1391s/batch\ttrain_loss = 5.7826\n",
      "Epoch 00\tBatch 9450\t0.1392s/batch\ttrain_loss = 6.6062\n",
      "Epoch 00\tBatch 9450\ttest_loss = 5.4317\n",
      "Epoch 00\tBatch 9460\t0.1402s/batch\ttrain_loss = 6.0058\n",
      "Epoch 00\tBatch 9470\t0.1374s/batch\ttrain_loss = 4.2733\n",
      "Epoch 00\tBatch 9480\t0.1389s/batch\ttrain_loss = 5.3101\n",
      "Epoch 00\tBatch 9480\ttest_loss = 5.4406\n",
      "Epoch 00\tBatch 9490\t0.1399s/batch\ttrain_loss = 5.7617\n",
      "Epoch 00\tBatch 9500\t0.1387s/batch\ttrain_loss = 4.9185\n",
      "Epoch 00\tBatch 9510\t0.1382s/batch\ttrain_loss = 5.0431\n",
      "Epoch 00\tBatch 9510\ttest_loss = 5.4354\n",
      "Epoch 00\tBatch 9520\t0.1390s/batch\ttrain_loss = 5.4429\n",
      "Epoch 00\tBatch 9530\t0.1384s/batch\ttrain_loss = 5.3425\n",
      "Epoch 00\tBatch 9540\t0.1381s/batch\ttrain_loss = 5.5118\n",
      "Epoch 00\tBatch 9540\ttest_loss = 5.4279\n",
      "Epoch 00\tBatch 9550\t0.1393s/batch\ttrain_loss = 5.7982\n",
      "Epoch 00\tBatch 9560\t0.1388s/batch\ttrain_loss = 5.6541\n",
      "Epoch 00\tBatch 9570\t0.1376s/batch\ttrain_loss = 4.3895\n",
      "Epoch 00\tBatch 9570\ttest_loss = 5.4394\n",
      "Epoch 00\tBatch 9580\t0.1389s/batch\ttrain_loss = 4.6553\n",
      "Epoch 00\tBatch 9590\t0.1396s/batch\ttrain_loss = 5.7827\n",
      "Epoch 00\tBatch 9600\t0.1401s/batch\ttrain_loss = 5.9262\n",
      "Epoch 00\tBatch 9600\ttest_loss = 5.4424\n",
      "Epoch 00\tBatch 9610\t0.1378s/batch\ttrain_loss = 4.8304\n",
      "Epoch 00\tBatch 9620\t0.1393s/batch\ttrain_loss = 5.2770\n",
      "Epoch 00\tBatch 9630\t0.1389s/batch\ttrain_loss = 4.8739\n",
      "Epoch 00\tBatch 9630\ttest_loss = 5.4362\n",
      "Epoch 00\tBatch 9640\t0.1388s/batch\ttrain_loss = 6.1494\n",
      "Epoch 00\tBatch 9650\t0.1381s/batch\ttrain_loss = 4.8796\n",
      "Epoch 00\tBatch 9660\t0.1388s/batch\ttrain_loss = 5.7941\n",
      "Epoch 00\tBatch 9660\ttest_loss = 5.4772\n",
      "Epoch 00\tBatch 9670\t0.1383s/batch\ttrain_loss = 5.4931\n",
      "Epoch 00\tBatch 9680\t0.1390s/batch\ttrain_loss = 5.4699\n",
      "Epoch 00\tBatch 9690\t0.1391s/batch\ttrain_loss = 5.2285\n",
      "Epoch 00\tBatch 9690\ttest_loss = 5.4643\n",
      "Epoch 00\tBatch 9700\t0.1391s/batch\ttrain_loss = 4.9967\n",
      "Epoch 00\tBatch 9710\t0.1394s/batch\ttrain_loss = 4.9810\n",
      "Epoch 00\tBatch 9720\t0.1393s/batch\ttrain_loss = 5.1276\n",
      "Epoch 00\tBatch 9720\ttest_loss = 5.4647\n",
      "Epoch 00\tBatch 9730\t0.1382s/batch\ttrain_loss = 4.5198\n",
      "Epoch 00\tBatch 9740\t0.1397s/batch\ttrain_loss = 6.1358\n",
      "Epoch 00\tBatch 9750\t0.1392s/batch\ttrain_loss = 4.9214\n",
      "Epoch 00\tBatch 9750\ttest_loss = 5.4710\n",
      "Epoch 00\tBatch 9760\t0.1397s/batch\ttrain_loss = 5.9354\n",
      "Epoch 00\tBatch 9770\t0.1408s/batch\ttrain_loss = 6.4336\n",
      "Epoch 00\tBatch 9780\t0.1407s/batch\ttrain_loss = 6.3692\n",
      "Epoch 00\tBatch 9780\ttest_loss = 5.4326\n",
      "Epoch 00\tBatch 9790\t0.1398s/batch\ttrain_loss = 5.3684\n",
      "Epoch 00\tBatch 9800\t0.1389s/batch\ttrain_loss = 5.3777\n",
      "Epoch 00\tBatch 9810\t0.1374s/batch\ttrain_loss = 4.6273\n",
      "Epoch 00\tBatch 9810\ttest_loss = 5.4423\n",
      "Epoch 00\tBatch 9820\t0.1390s/batch\ttrain_loss = 5.6949\n",
      "Epoch 00\tBatch 9830\t0.1416s/batch\ttrain_loss = 6.4094\n",
      "Epoch 00\tBatch 9840\t0.1418s/batch\ttrain_loss = 5.8904\n",
      "Epoch 00\tBatch 9840\ttest_loss = 5.4285\n",
      "Epoch 00\tBatch 9850\t0.1421s/batch\ttrain_loss = 6.9648\n",
      "Epoch 00\tBatch 9860\t0.1444s/batch\ttrain_loss = 6.8357\n",
      "Epoch 00\tBatch 9870\t0.1427s/batch\ttrain_loss = 7.6511\n",
      "Epoch 00\tBatch 9870\ttest_loss = 5.4517\n",
      "Epoch 00\tBatch 9880\t0.1390s/batch\ttrain_loss = 5.7642\n",
      "Epoch 00\tBatch 9890\t0.1381s/batch\ttrain_loss = 4.3918\n",
      "Epoch 00\tBatch 9900\t0.1377s/batch\ttrain_loss = 4.3969\n",
      "Epoch 00\tBatch 9900\ttest_loss = 5.4479\n",
      "Epoch 00\tBatch 9910\t0.1398s/batch\ttrain_loss = 5.4514\n",
      "Epoch 00\tBatch 9920\t0.1382s/batch\ttrain_loss = 4.9576\n",
      "Epoch 00\tBatch 9930\t0.1381s/batch\ttrain_loss = 4.8992\n",
      "Epoch 00\tBatch 9930\ttest_loss = 5.4347\n",
      "Epoch 00\tBatch 9940\t0.1388s/batch\ttrain_loss = 4.9435\n",
      "Epoch 00\tBatch 9950\t0.1380s/batch\ttrain_loss = 4.9514\n",
      "Epoch 00\tBatch 9960\t0.1382s/batch\ttrain_loss = 5.4843\n",
      "Epoch 00\tBatch 9960\ttest_loss = 5.4430\n",
      "Epoch 00\tBatch 9970\t0.1392s/batch\ttrain_loss = 6.5038\n",
      "Epoch 00\tBatch 9980\t0.1381s/batch\ttrain_loss = 5.0539\n",
      "Epoch 00\tBatch 9990\t0.1389s/batch\ttrain_loss = 4.9990\n",
      "Epoch 00\tBatch 9990\ttest_loss = 5.4451\n",
      "Epoch 00\tBatch 10000\t0.1399s/batch\ttrain_loss = 5.3377\n",
      "Epoch 00\tBatch 10010\t0.1391s/batch\ttrain_loss = 5.3056\n",
      "Epoch 00\tBatch 10020\t0.1391s/batch\ttrain_loss = 5.7874\n",
      "Epoch 00\tBatch 10020\ttest_loss = 5.4246\n",
      "Epoch 00\tBatch 10030\t0.1400s/batch\ttrain_loss = 7.1207\n",
      "Epoch 00\tBatch 10040\t0.1387s/batch\ttrain_loss = 6.0999\n",
      "Epoch 00\tBatch 10050\t0.1403s/batch\ttrain_loss = 7.0210\n",
      "Epoch 00\tBatch 10050\ttest_loss = 5.4311\n",
      "Epoch 00\tBatch 10060\t0.1403s/batch\ttrain_loss = 6.5888\n",
      "Epoch 00\tBatch 10070\t0.1392s/batch\ttrain_loss = 5.9631\n",
      "Epoch 00\tBatch 10080\t0.1406s/batch\ttrain_loss = 6.0179\n",
      "Epoch 00\tBatch 10080\ttest_loss = 5.4341\n",
      "Epoch 00\tBatch 10090\t0.1388s/batch\ttrain_loss = 5.3583\n",
      "Epoch 00\tBatch 10100\t0.1375s/batch\ttrain_loss = 4.7736\n",
      "Epoch 00\tBatch 10110\t0.1381s/batch\ttrain_loss = 5.7282\n",
      "Epoch 00\tBatch 10110\ttest_loss = 5.4395\n",
      "Epoch 00\tBatch 10120\t0.1504s/batch\ttrain_loss = 4.4609\n",
      "Epoch 00\tBatch 10130\t0.1402s/batch\ttrain_loss = 6.0115\n",
      "Epoch 00\tBatch 10140\t0.1395s/batch\ttrain_loss = 6.1162\n",
      "Epoch 00\tBatch 10140\ttest_loss = 5.4342\n",
      "Epoch 00\tBatch 10150\t0.1400s/batch\ttrain_loss = 5.8997\n",
      "Epoch 00\tBatch 10160\t0.1399s/batch\ttrain_loss = 7.0177\n",
      "Epoch 00\tBatch 10170\t0.1401s/batch\ttrain_loss = 6.8575\n",
      "Epoch 00\tBatch 10170\ttest_loss = 5.4449\n",
      "Epoch 00\tBatch 10180\t0.1446s/batch\ttrain_loss = 6.7038\n",
      "Epoch 00\tBatch 10190\t0.1406s/batch\ttrain_loss = 6.1146\n",
      "Epoch 00\tBatch 10200\t0.1406s/batch\ttrain_loss = 6.5993\n",
      "Epoch 00\tBatch 10200\ttest_loss = 5.4339\n",
      "Epoch 00\tBatch 10210\t0.1389s/batch\ttrain_loss = 5.6069\n",
      "Epoch 00\tBatch 10220\t0.1397s/batch\ttrain_loss = 5.3830\n",
      "Epoch 00\tBatch 10230\t0.1404s/batch\ttrain_loss = 6.4603\n",
      "Epoch 00\tBatch 10230\ttest_loss = 5.4347\n",
      "Epoch 00\tBatch 10240\t0.1433s/batch\ttrain_loss = 5.3403\n",
      "Epoch 00\tBatch 10250\t0.1415s/batch\ttrain_loss = 5.5377\n",
      "Epoch 00\tBatch 10260\t0.1390s/batch\ttrain_loss = 5.6058\n",
      "Epoch 00\tBatch 10260\ttest_loss = 5.4177\n",
      "Epoch 00\tBatch 10270\t0.1386s/batch\ttrain_loss = 6.1658\n",
      "Epoch 00\tBatch 10280\t0.1383s/batch\ttrain_loss = 5.8362\n",
      "Epoch 00\tBatch 10290\t0.1385s/batch\ttrain_loss = 5.4535\n",
      "Epoch 00\tBatch 10290\ttest_loss = 5.4270\n",
      "Epoch 00\tBatch 10300\t0.1395s/batch\ttrain_loss = 6.2890\n",
      "Epoch 00\tBatch 10310\t0.1393s/batch\ttrain_loss = 5.7008\n",
      "Epoch 00\tBatch 10320\t0.1393s/batch\ttrain_loss = 5.7006\n",
      "Epoch 00\tBatch 10320\ttest_loss = 5.4371\n",
      "Epoch 00\tBatch 10330\t0.1382s/batch\ttrain_loss = 5.3956\n",
      "Epoch 00\tBatch 10340\t0.1393s/batch\ttrain_loss = 6.3670\n",
      "Epoch 00\tBatch 10350\t0.1373s/batch\ttrain_loss = 4.2116\n",
      "Epoch 00\tBatch 10350\ttest_loss = 5.4195\n",
      "Epoch 00\tBatch 10360\t0.1401s/batch\ttrain_loss = 5.1394\n",
      "Epoch 00\tBatch 10370\t0.1417s/batch\ttrain_loss = 5.9193\n",
      "Epoch 00\tBatch 10380\t0.1427s/batch\ttrain_loss = 5.8168\n",
      "Epoch 00\tBatch 10380\ttest_loss = 5.4281\n",
      "Epoch 00\tBatch 10390\t0.1397s/batch\ttrain_loss = 5.3205\n",
      "Epoch 00\tBatch 10400\t0.1405s/batch\ttrain_loss = 5.4621\n",
      "Epoch 00\tBatch 10410\t0.1407s/batch\ttrain_loss = 5.4320\n",
      "Epoch 00\tBatch 10410\ttest_loss = 5.4336\n",
      "Epoch 00\tBatch 10420\t0.1376s/batch\ttrain_loss = 4.5555\n",
      "Epoch 00\tBatch 10430\t0.1392s/batch\ttrain_loss = 5.4929\n",
      "Epoch 00\tBatch 10440\t0.1405s/batch\ttrain_loss = 6.1320\n",
      "Epoch 00\tBatch 10440\ttest_loss = 5.4427\n",
      "Epoch 00\tBatch 10450\t0.1407s/batch\ttrain_loss = 6.6656\n",
      "Epoch 00\tBatch 10460\t0.1411s/batch\ttrain_loss = 6.9986\n",
      "Epoch 00\tBatch 10470\t0.1389s/batch\ttrain_loss = 5.3343\n",
      "Epoch 00\tBatch 10470\ttest_loss = 5.4317\n",
      "Epoch 00\tBatch 10480\t0.1372s/batch\ttrain_loss = 4.9726\n",
      "Epoch 00\tBatch 10490\t0.1381s/batch\ttrain_loss = 5.6809\n",
      "Epoch 00\tBatch 10500\t0.1396s/batch\ttrain_loss = 4.7495\n",
      "Epoch 00\tBatch 10500\ttest_loss = 5.4286\n",
      "Epoch 00\tBatch 10510\t0.1404s/batch\ttrain_loss = 4.5443\n",
      "Epoch 00\tBatch 10520\t0.1440s/batch\ttrain_loss = 5.1248\n",
      "Epoch 00\tBatch 10530\t0.1461s/batch\ttrain_loss = 5.9849\n",
      "Epoch 00\tBatch 10530\ttest_loss = 5.4325\n",
      "Epoch 00\tBatch 10540\t0.1398s/batch\ttrain_loss = 6.0656\n",
      "Epoch 00\tBatch 10550\t0.1405s/batch\ttrain_loss = 6.1535\n",
      "Epoch 00\tBatch 10560\t0.1382s/batch\ttrain_loss = 5.1471\n",
      "Epoch 00\tBatch 10560\ttest_loss = 5.4360\n",
      "Epoch 00\tBatch 10570\t0.1382s/batch\ttrain_loss = 5.2486\n",
      "Epoch 00\tBatch 10580\t0.1381s/batch\ttrain_loss = 6.1099\n",
      "Epoch 00\tBatch 10590\t0.1378s/batch\ttrain_loss = 5.4197\n",
      "Epoch 00\tBatch 10590\ttest_loss = 5.4293\n",
      "Epoch 00\tBatch 10600\t0.1380s/batch\ttrain_loss = 5.0690\n",
      "Epoch 00\tBatch 10610\t0.1373s/batch\ttrain_loss = 5.2324\n",
      "Epoch 00\tBatch 10620\t0.1376s/batch\ttrain_loss = 5.5807\n",
      "Epoch 00\tBatch 10620\ttest_loss = 5.4160\n",
      "Epoch 00\tBatch 10630\t0.1379s/batch\ttrain_loss = 4.9219\n",
      "Epoch 00\tBatch 10640\t0.1381s/batch\ttrain_loss = 5.8829\n",
      "Epoch 00\tBatch 10650\t0.1398s/batch\ttrain_loss = 4.6908\n",
      "Epoch 00\tBatch 10650\ttest_loss = 5.4345\n",
      "Epoch 00\tBatch 10660\t0.1396s/batch\ttrain_loss = 6.4586\n",
      "Epoch 00\tBatch 10670\t0.1400s/batch\ttrain_loss = 5.7774\n",
      "Epoch 00\tBatch 10680\t0.1396s/batch\ttrain_loss = 6.4433\n",
      "Epoch 00\tBatch 10680\ttest_loss = 5.4264\n",
      "Epoch 00\tBatch 10690\t0.1410s/batch\ttrain_loss = 6.9586\n",
      "Epoch 00\tBatch 10700\t0.1392s/batch\ttrain_loss = 6.5588\n",
      "Epoch 00\tBatch 10710\t0.1460s/batch\ttrain_loss = 3.8192\n",
      "Epoch 00\tBatch 10710\ttest_loss = 5.4371\n",
      "Epoch 00\tBatch 10720\t0.1389s/batch\ttrain_loss = 4.5993\n",
      "Epoch 00\tBatch 10730\t0.1388s/batch\ttrain_loss = 5.0023\n",
      "Epoch 00\tBatch 10740\t0.1391s/batch\ttrain_loss = 5.5393\n",
      "Epoch 00\tBatch 10740\ttest_loss = 5.4444\n",
      "Epoch 00\tBatch 10750\t0.1422s/batch\ttrain_loss = 5.6427\n",
      "Epoch 00\tBatch 10760\t0.1386s/batch\ttrain_loss = 5.3885\n",
      "Epoch 00\tBatch 10770\t0.1374s/batch\ttrain_loss = 5.0215\n",
      "Epoch 00\tBatch 10770\ttest_loss = 5.4566\n",
      "Epoch 00\tBatch 10780\t0.1379s/batch\ttrain_loss = 4.7175\n",
      "Epoch 00\tBatch 10790\t0.1385s/batch\ttrain_loss = 5.5061\n",
      "Epoch 00\tBatch 10800\t0.1394s/batch\ttrain_loss = 5.8122\n",
      "Epoch 00\tBatch 10800\ttest_loss = 5.4365\n",
      "Epoch 00\tBatch 10810\t0.1382s/batch\ttrain_loss = 5.0971\n",
      "Epoch 00\tBatch 10820\t0.1369s/batch\ttrain_loss = 4.1002\n",
      "Epoch 00\tBatch 10830\t0.1397s/batch\ttrain_loss = 6.3789\n",
      "Epoch 00\tBatch 10830\ttest_loss = 5.4207\n",
      "Epoch 00\tBatch 10840\t0.1399s/batch\ttrain_loss = 6.8740\n",
      "Epoch 00\tBatch 10850\t0.1383s/batch\ttrain_loss = 5.0428\n",
      "Epoch 00\tBatch 10860\t0.1378s/batch\ttrain_loss = 4.4177\n",
      "Epoch 00\tBatch 10860\ttest_loss = 5.4297\n",
      "Epoch 00\tBatch 10870\t0.1448s/batch\ttrain_loss = 4.6227\n",
      "Epoch 00\tBatch 10880\t0.1401s/batch\ttrain_loss = 5.4406\n",
      "Epoch 00\tBatch 10890\t0.1558s/batch\ttrain_loss = 6.2403\n",
      "Epoch 00\tBatch 10890\ttest_loss = 5.4229\n",
      "converged after 0 epochs (lowest achieved loss: 5.4220)\n",
      "loading best checkpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([12.480655002593995,\n",
       "  10.337840366363526,\n",
       "  9.27308497428894,\n",
       "  7.700550651550293,\n",
       "  7.923036003112793,\n",
       "  8.229965114593506,\n",
       "  8.077570152282714,\n",
       "  6.80824384689331,\n",
       "  7.3201847791671755,\n",
       "  8.444734334945679,\n",
       "  7.351213073730468,\n",
       "  7.3514951229095455,\n",
       "  5.748010849952697,\n",
       "  6.407313013076783,\n",
       "  7.217664480209351,\n",
       "  8.230033826828002,\n",
       "  6.200826740264892,\n",
       "  7.017363166809082,\n",
       "  5.73690938949585,\n",
       "  5.752015089988708,\n",
       "  5.38360013961792,\n",
       "  7.702769136428833,\n",
       "  6.270129585266114,\n",
       "  5.957297468185425,\n",
       "  5.78797287940979,\n",
       "  6.573450517654419,\n",
       "  6.6606817722320555,\n",
       "  5.652288770675659,\n",
       "  6.038947057723999,\n",
       "  6.832077836990356,\n",
       "  6.597845268249512,\n",
       "  6.2784895420074465,\n",
       "  7.243807315826416,\n",
       "  6.0229291915893555,\n",
       "  6.245186185836792,\n",
       "  5.169253802299499,\n",
       "  6.317964649200439,\n",
       "  4.909299206733704,\n",
       "  5.594918990135193,\n",
       "  6.524342489242554,\n",
       "  7.971635246276856,\n",
       "  8.263182353973388,\n",
       "  5.837405157089234,\n",
       "  6.134846305847168,\n",
       "  6.8851581573486325,\n",
       "  5.681737208366394,\n",
       "  6.365463542938232,\n",
       "  4.946884083747864,\n",
       "  5.85056881904602,\n",
       "  6.221946597099304,\n",
       "  5.324380373954773,\n",
       "  5.717210483551026,\n",
       "  5.487320423126221,\n",
       "  6.629015302658081,\n",
       "  5.750008010864258,\n",
       "  7.359167671203613,\n",
       "  5.529591464996338,\n",
       "  6.406217384338379,\n",
       "  6.4560737133026125,\n",
       "  7.492988061904907,\n",
       "  6.701817989349365,\n",
       "  6.147345018386841,\n",
       "  6.353316926956177,\n",
       "  6.1019586563110355,\n",
       "  6.036409473419189,\n",
       "  7.295755243301391,\n",
       "  6.58570237159729,\n",
       "  6.2780297756195065,\n",
       "  5.469473886489868,\n",
       "  5.3639153957366945,\n",
       "  5.4357232570648195,\n",
       "  6.0930253028869625,\n",
       "  6.181731772422791,\n",
       "  5.283114576339722,\n",
       "  5.332410144805908,\n",
       "  7.5846644878387455,\n",
       "  8.040576457977295,\n",
       "  7.8395781993865965,\n",
       "  7.262721395492553,\n",
       "  6.006673979759216,\n",
       "  6.000699186325074,\n",
       "  5.5904515266418455,\n",
       "  5.973167610168457,\n",
       "  4.99111840724945,\n",
       "  5.509352684020996,\n",
       "  6.041359710693359,\n",
       "  7.811305999755859,\n",
       "  5.475213527679443,\n",
       "  6.193825531005859,\n",
       "  5.6777496337890625,\n",
       "  6.160075259208679,\n",
       "  6.7282617568969725,\n",
       "  7.589051914215088,\n",
       "  5.132896256446839,\n",
       "  6.789687633514404,\n",
       "  5.279513812065124,\n",
       "  5.275793170928955,\n",
       "  5.409569549560547,\n",
       "  4.784296703338623,\n",
       "  4.33900933265686,\n",
       "  4.7164746761322025,\n",
       "  6.113026475906372,\n",
       "  5.37598876953125,\n",
       "  6.397194027900696,\n",
       "  6.2050331115722654,\n",
       "  6.463024210929871,\n",
       "  6.120140504837036,\n",
       "  6.132868528366089,\n",
       "  5.65953803062439,\n",
       "  5.290152144432068,\n",
       "  5.106161260604859,\n",
       "  5.7915387630462645,\n",
       "  7.023927307128906,\n",
       "  6.938411617279053,\n",
       "  6.820195889472961,\n",
       "  6.998554944992065,\n",
       "  5.180202913284302,\n",
       "  6.311740136146545,\n",
       "  6.98979082107544,\n",
       "  6.631764698028564,\n",
       "  6.7246928215026855,\n",
       "  6.121648931503296,\n",
       "  6.614410400390625,\n",
       "  4.851698589324951,\n",
       "  5.829772162437439,\n",
       "  5.178844356536866,\n",
       "  5.066039204597473,\n",
       "  5.780625557899475,\n",
       "  6.182561779022217,\n",
       "  6.0213285207748415,\n",
       "  6.491726350784302,\n",
       "  7.991080856323242,\n",
       "  5.856959104537964,\n",
       "  6.427522993087768,\n",
       "  7.249266195297241,\n",
       "  5.707018756866455,\n",
       "  5.532806873321533,\n",
       "  5.827021741867066,\n",
       "  5.324790811538696,\n",
       "  7.149824428558349,\n",
       "  6.324166798591614,\n",
       "  7.4513732433319095,\n",
       "  8.010559844970704,\n",
       "  8.364555311203002,\n",
       "  5.177084279060364,\n",
       "  6.900130414962769,\n",
       "  6.634296894073486,\n",
       "  7.167564821243286,\n",
       "  6.241582441329956,\n",
       "  5.700920104980469,\n",
       "  6.518917274475098,\n",
       "  5.911790466308593,\n",
       "  6.760253047943115,\n",
       "  6.915885257720947,\n",
       "  6.8738932609558105,\n",
       "  5.90774974822998,\n",
       "  6.5467041015625,\n",
       "  7.426207065582275,\n",
       "  5.990667772293091,\n",
       "  6.683961725234985,\n",
       "  6.1792027950286865,\n",
       "  6.07065749168396,\n",
       "  5.668355274200439,\n",
       "  5.636959409713745,\n",
       "  5.748363971710205,\n",
       "  5.167990899085998,\n",
       "  5.855813884735108,\n",
       "  6.786028480529785,\n",
       "  5.9070343494415285,\n",
       "  6.042416906356811,\n",
       "  6.527919292449951,\n",
       "  5.752035164833069,\n",
       "  6.437387084960937,\n",
       "  5.731173658370972,\n",
       "  5.979291725158691,\n",
       "  5.2751895666122435,\n",
       "  4.796236968040466,\n",
       "  5.999603176116944,\n",
       "  5.578016853332519,\n",
       "  7.044779682159424,\n",
       "  6.0262038230896,\n",
       "  5.928596472740173,\n",
       "  6.447607707977295,\n",
       "  5.781071949005127,\n",
       "  5.905691242218017,\n",
       "  5.801038551330566,\n",
       "  5.8100098133087155,\n",
       "  6.571745109558106,\n",
       "  7.14462513923645,\n",
       "  5.669997310638427,\n",
       "  5.927457475662232,\n",
       "  6.079646921157837,\n",
       "  5.420289659500122,\n",
       "  6.10375452041626,\n",
       "  6.564245271682739,\n",
       "  6.584604930877686,\n",
       "  7.152981805801391,\n",
       "  6.805717897415161,\n",
       "  5.851904773712159,\n",
       "  7.150875520706177,\n",
       "  6.945963335037232,\n",
       "  5.117238831520081,\n",
       "  4.5884110689163204,\n",
       "  5.392428779602051,\n",
       "  4.873056840896607,\n",
       "  5.595894432067871,\n",
       "  5.282904434204101,\n",
       "  5.267531132698059,\n",
       "  5.796795415878296,\n",
       "  5.458595204353332,\n",
       "  4.897149050235749,\n",
       "  5.679983997344971,\n",
       "  4.2113251209259035,\n",
       "  5.035502219200135,\n",
       "  4.759793090820312,\n",
       "  5.03373806476593,\n",
       "  6.384503173828125,\n",
       "  5.979223394393921,\n",
       "  5.810764503479004,\n",
       "  5.058396315574646,\n",
       "  5.5946417331695555,\n",
       "  4.508094930648804,\n",
       "  5.975812244415283,\n",
       "  6.684767246246338,\n",
       "  7.740841197967529,\n",
       "  6.2743998050689695,\n",
       "  6.084995198249817,\n",
       "  6.348787641525268,\n",
       "  6.22690110206604,\n",
       "  4.504698657989502,\n",
       "  6.021101689338684,\n",
       "  5.645445156097412,\n",
       "  6.443636131286621,\n",
       "  5.518414545059204,\n",
       "  5.955030393600464,\n",
       "  6.0385175228118895,\n",
       "  5.105849623680115,\n",
       "  5.35667040348053,\n",
       "  5.316830635070801,\n",
       "  5.69115309715271,\n",
       "  7.156922912597656,\n",
       "  6.295903873443604,\n",
       "  7.375877380371094,\n",
       "  8.041431570053101,\n",
       "  5.968450832366943,\n",
       "  5.431176280975341,\n",
       "  5.694734025001526,\n",
       "  5.821719741821289,\n",
       "  4.94291090965271,\n",
       "  5.420771503448487,\n",
       "  5.6782917976379395,\n",
       "  8.581635570526123,\n",
       "  6.187002372741699,\n",
       "  6.700239610671997,\n",
       "  6.86672830581665,\n",
       "  7.36919093132019,\n",
       "  5.155921506881714,\n",
       "  4.971368288993835,\n",
       "  5.296621775627136,\n",
       "  5.770696115493775,\n",
       "  6.310008215904236,\n",
       "  5.435165429115296,\n",
       "  5.4545094013214115,\n",
       "  5.953637361526489,\n",
       "  5.664659833908081,\n",
       "  4.896577215194702,\n",
       "  4.147246837615967,\n",
       "  5.786209964752198,\n",
       "  5.8255348920822145,\n",
       "  5.576234531402588,\n",
       "  5.538947439193725,\n",
       "  7.512312936782837,\n",
       "  6.1124287128448485,\n",
       "  5.199051141738892,\n",
       "  7.476094579696655,\n",
       "  7.302139949798584,\n",
       "  4.412468314170837,\n",
       "  4.63132746219635,\n",
       "  5.684611225128174,\n",
       "  5.438956642150879,\n",
       "  6.996505928039551,\n",
       "  5.965803909301758,\n",
       "  5.890804386138916,\n",
       "  5.355122137069702,\n",
       "  5.454470133781433,\n",
       "  6.107055044174194,\n",
       "  5.4084922790527346,\n",
       "  6.203166484832764,\n",
       "  6.202074813842773,\n",
       "  5.633708190917969,\n",
       "  6.194917011260986,\n",
       "  4.958647751808167,\n",
       "  4.799344086647034,\n",
       "  4.774810194969177,\n",
       "  5.305983829498291,\n",
       "  6.768968677520752,\n",
       "  6.241441679000855,\n",
       "  6.865430593490601,\n",
       "  5.644435882568359,\n",
       "  6.076251077651977,\n",
       "  5.858047986030579,\n",
       "  7.050051689147949,\n",
       "  6.122279810905456,\n",
       "  6.704516839981079,\n",
       "  5.876513528823852,\n",
       "  4.672719740867615,\n",
       "  4.618731927871704,\n",
       "  3.8847391843795775,\n",
       "  6.039422988891602,\n",
       "  5.920994520187378,\n",
       "  5.728524971008301,\n",
       "  6.133343362808228,\n",
       "  6.658247590065002,\n",
       "  5.31776602268219,\n",
       "  5.054439759254455,\n",
       "  5.5824622631073,\n",
       "  5.372791337966919,\n",
       "  5.153882026672363,\n",
       "  6.426637601852417,\n",
       "  7.517702960968018,\n",
       "  6.216437768936157,\n",
       "  6.381211662292481,\n",
       "  6.566656684875488,\n",
       "  6.117858076095581,\n",
       "  6.8784243106842045,\n",
       "  7.712832927703857,\n",
       "  6.4119260787963865,\n",
       "  5.343777990341186,\n",
       "  5.727417516708374,\n",
       "  5.408056688308716,\n",
       "  5.069146728515625,\n",
       "  4.949238848686218,\n",
       "  6.063714385032654,\n",
       "  4.861617207527161,\n",
       "  8.307057428359986,\n",
       "  6.432677626609802,\n",
       "  5.5120453357696535,\n",
       "  4.097025060653687,\n",
       "  5.054420161247253,\n",
       "  5.24340786933899,\n",
       "  4.960559344291687,\n",
       "  5.62771201133728,\n",
       "  5.550897574424743,\n",
       "  4.34520947933197,\n",
       "  5.430930089950562,\n",
       "  5.549647045135498,\n",
       "  5.245153307914734,\n",
       "  5.105150604248047,\n",
       "  5.218227910995483,\n",
       "  4.570273852348327,\n",
       "  5.9777907371521,\n",
       "  6.091089153289795,\n",
       "  6.093116760253906,\n",
       "  6.303151035308838,\n",
       "  4.542548394203186,\n",
       "  5.705144023895263,\n",
       "  4.871149277687072,\n",
       "  5.6265849590301515,\n",
       "  5.78045425415039,\n",
       "  4.789750123023987,\n",
       "  5.0743186473846436,\n",
       "  5.637128782272339,\n",
       "  5.202907228469849,\n",
       "  4.800840926170349,\n",
       "  5.258401012420654,\n",
       "  5.396814918518066,\n",
       "  6.39773645401001,\n",
       "  5.3729006290435795,\n",
       "  6.039051294326782,\n",
       "  5.295343828201294,\n",
       "  4.692821097373963,\n",
       "  6.319629549980164,\n",
       "  7.01379165649414,\n",
       "  7.067410087585449,\n",
       "  5.525006747245788,\n",
       "  5.037862658500671,\n",
       "  4.656158018112182,\n",
       "  5.335639762878418,\n",
       "  5.263654446601867,\n",
       "  4.95098648071289,\n",
       "  5.199108123779297,\n",
       "  5.898663139343261,\n",
       "  5.44190616607666,\n",
       "  4.511771225929261,\n",
       "  5.274831008911133,\n",
       "  6.1439875841140745,\n",
       "  6.361652469635009,\n",
       "  5.8203336000442505,\n",
       "  6.685647630691529,\n",
       "  6.150611352920532,\n",
       "  5.672845387458802,\n",
       "  5.943700361251831,\n",
       "  5.44062762260437,\n",
       "  5.8807751655578615,\n",
       "  7.926223278045654,\n",
       "  6.7835756778717045,\n",
       "  6.670352792739868,\n",
       "  6.411674642562867,\n",
       "  5.481561613082886,\n",
       "  4.40427873134613,\n",
       "  5.109799385070801,\n",
       "  4.7372865915298465,\n",
       "  5.524926066398621,\n",
       "  6.194099092483521,\n",
       "  6.752781343460083,\n",
       "  6.83120174407959,\n",
       "  6.8788610935211185,\n",
       "  5.115577960014344,\n",
       "  5.76586127281189,\n",
       "  5.321274638175964,\n",
       "  6.307314729690551,\n",
       "  6.006308507919312,\n",
       "  6.39711012840271,\n",
       "  5.9131608486175535,\n",
       "  5.755809259414673,\n",
       "  6.673940229415893,\n",
       "  6.216671895980835,\n",
       "  6.197769451141357,\n",
       "  5.899753713607788,\n",
       "  5.833269357681274,\n",
       "  6.738984298706055,\n",
       "  5.662049007415772,\n",
       "  6.222061061859131,\n",
       "  5.621481847763062,\n",
       "  5.597103619575501,\n",
       "  5.263212633132935,\n",
       "  7.092463397979737,\n",
       "  5.431957745552063,\n",
       "  4.667938828468323,\n",
       "  5.715945696830749,\n",
       "  6.4708997249603275,\n",
       "  5.778001976013184,\n",
       "  5.63147554397583,\n",
       "  6.18506875038147,\n",
       "  5.133477115631104,\n",
       "  5.668390464782715,\n",
       "  6.864115858078003,\n",
       "  5.742999649047851,\n",
       "  5.446447253227234,\n",
       "  7.11875467300415,\n",
       "  6.022322368621826,\n",
       "  7.29409351348877,\n",
       "  6.647526121139526,\n",
       "  5.174590444564819,\n",
       "  5.1157451391220095,\n",
       "  5.6396490097045895,\n",
       "  5.605787634849548,\n",
       "  4.898842835426331,\n",
       "  5.110262632369995,\n",
       "  5.157764863967896,\n",
       "  4.8600311279296875,\n",
       "  4.710505223274231,\n",
       "  4.833663296699524,\n",
       "  4.773805928230286,\n",
       "  4.325368857383728,\n",
       "  5.693647956848144,\n",
       "  4.891530990600586,\n",
       "  4.195546102523804,\n",
       "  7.0282543182373045,\n",
       "  5.917175841331482,\n",
       "  7.083496904373169,\n",
       "  5.346882176399231,\n",
       "  6.0756038427352905,\n",
       "  6.153909230232239,\n",
       "  6.070160698890686,\n",
       "  7.103706169128418,\n",
       "  5.0503199100494385,\n",
       "  5.493311524391174,\n",
       "  5.53057336807251,\n",
       "  4.491285181045532,\n",
       "  6.187214994430542,\n",
       "  5.431065011024475,\n",
       "  4.910419201850891,\n",
       "  5.924432706832886,\n",
       "  5.610901832580566,\n",
       "  4.966796660423279,\n",
       "  4.386314463615418,\n",
       "  5.54684362411499,\n",
       "  5.021157622337341,\n",
       "  6.350281667709351,\n",
       "  5.415586280822754,\n",
       "  5.381845617294312,\n",
       "  5.65651125907898,\n",
       "  5.9796315908432005,\n",
       "  5.444618201255798,\n",
       "  4.699107027053833,\n",
       "  5.3128526449203495,\n",
       "  5.701871013641357,\n",
       "  5.530520176887512,\n",
       "  5.897450876235962,\n",
       "  5.935463190078735,\n",
       "  5.82215051651001,\n",
       "  7.377212190628052,\n",
       "  5.868909335136413,\n",
       "  6.077367305755615,\n",
       "  6.214988756179809,\n",
       "  5.788730192184448,\n",
       "  6.512480545043945,\n",
       "  6.212866497039795,\n",
       "  4.70838623046875,\n",
       "  4.501693892478943,\n",
       "  5.391015529632568,\n",
       "  4.638649892807007,\n",
       "  5.517160129547119,\n",
       "  4.629975271224976,\n",
       "  5.063982391357422,\n",
       "  3.8937089920043944,\n",
       "  4.7356091976165775,\n",
       "  4.147401309013366,\n",
       "  3.7194297313690186,\n",
       "  5.1200470447540285,\n",
       "  4.221398735046387,\n",
       "  6.633234453201294,\n",
       "  5.679150724411011,\n",
       "  6.384418058395386,\n",
       "  5.902649450302124,\n",
       "  6.144601178169251,\n",
       "  5.19597373008728,\n",
       "  4.886659908294678,\n",
       "  5.1576759099960325,\n",
       "  6.798641514778137,\n",
       "  6.234956932067871,\n",
       "  6.87843656539917,\n",
       "  6.258711481094361,\n",
       "  5.182198524475098,\n",
       "  5.1230137825012205,\n",
       "  4.495439434051514,\n",
       "  4.8503193855285645,\n",
       "  4.312030780315399,\n",
       "  4.591696453094483,\n",
       "  4.280297255516052,\n",
       "  4.7706369161605835,\n",
       "  4.604163479804993,\n",
       "  5.807371258735657,\n",
       "  5.190665650367737,\n",
       "  4.000123357772827,\n",
       "  4.8304630517959595,\n",
       "  4.5550055503845215,\n",
       "  4.661022353172302,\n",
       "  5.105457282066345,\n",
       "  6.862795162200928,\n",
       "  7.753801584243774,\n",
       "  5.809750604629516,\n",
       "  4.704683995246887,\n",
       "  4.857412385940552,\n",
       "  4.916347098350525,\n",
       "  7.501245641708374,\n",
       "  6.491361522674561,\n",
       "  6.321763229370117,\n",
       "  6.320673894882202,\n",
       "  5.456451559066773,\n",
       "  6.0607337474823,\n",
       "  5.440144944190979,\n",
       "  5.780956506729126,\n",
       "  6.370069265365601,\n",
       "  6.7280608177185055,\n",
       "  7.166239929199219,\n",
       "  5.2288600444793705,\n",
       "  5.215531206130981,\n",
       "  6.137933015823364,\n",
       "  6.320790338516235,\n",
       "  6.668266773223877,\n",
       "  5.262218260765076,\n",
       "  5.189231657981873,\n",
       "  5.948573470115662,\n",
       "  5.971660947799682,\n",
       "  6.817254877090454,\n",
       "  4.822221970558166,\n",
       "  5.154851031303406,\n",
       "  4.5110018491745,\n",
       "  5.180987930297851,\n",
       "  5.093782186508179,\n",
       "  6.224689149856568,\n",
       "  5.445679235458374,\n",
       "  5.478082776069641,\n",
       "  5.02535388469696,\n",
       "  4.189932322502136,\n",
       "  3.6494874000549316,\n",
       "  3.6520771026611327,\n",
       "  4.157517838478088,\n",
       "  6.931878232955933,\n",
       "  5.0318989038467405,\n",
       "  6.132874226570129,\n",
       "  6.311168551445007,\n",
       "  5.196893906593322,\n",
       "  6.339163875579834,\n",
       "  6.0157759428024296,\n",
       "  5.003476881980896,\n",
       "  5.441109657287598,\n",
       "  5.853714513778686,\n",
       "  5.146851491928101,\n",
       "  6.092636060714722,\n",
       "  6.1767576932907104,\n",
       "  5.984222555160523,\n",
       "  5.5494483470916744,\n",
       "  5.037049627304077,\n",
       "  5.020272302627563,\n",
       "  4.676840209960938,\n",
       "  4.457380485534668,\n",
       "  4.302833724021911,\n",
       "  4.555559229850769,\n",
       "  5.781989192962646,\n",
       "  5.170355439186096,\n",
       "  5.931420397758484,\n",
       "  5.795066165924072,\n",
       "  6.679623651504516,\n",
       "  4.401809620857239,\n",
       "  5.35236976146698,\n",
       "  5.347705602645874,\n",
       "  4.997746133804322,\n",
       "  4.534723949432373,\n",
       "  5.24079442024231,\n",
       "  5.890882563591004,\n",
       "  4.111625909805298,\n",
       "  4.2115740299224855,\n",
       "  3.656753730773926,\n",
       "  4.258234024047852,\n",
       "  5.356351137161255,\n",
       "  5.183221435546875,\n",
       "  5.307609629631043,\n",
       "  4.4418275356292725,\n",
       "  3.9771908283233643,\n",
       "  6.0206217765808105,\n",
       "  6.756477975845337,\n",
       "  5.204224872589111,\n",
       "  5.612057447433472,\n",
       "  5.864437055587769,\n",
       "  6.51332459449768,\n",
       "  5.912784433364868,\n",
       "  6.380127620697022,\n",
       "  5.57764778137207,\n",
       "  6.121942806243896,\n",
       "  5.529134726524353,\n",
       "  5.362610936164856,\n",
       "  5.371363735198974,\n",
       "  5.808304691314698,\n",
       "  4.85207417011261,\n",
       "  4.310027241706848,\n",
       "  9.025925636291504,\n",
       "  6.852840280532837,\n",
       "  5.777433395385742,\n",
       "  5.792984390258789,\n",
       "  5.8546998500823975,\n",
       "  5.205106210708618,\n",
       "  5.249301648139953,\n",
       "  5.284477353096008,\n",
       "  5.3067538976669315,\n",
       "  6.308853387832642,\n",
       "  5.909772443771362,\n",
       "  4.992924332618713,\n",
       "  5.097486901283264,\n",
       "  4.81937882900238,\n",
       "  5.191699814796448,\n",
       "  4.561056971549988,\n",
       "  5.035891056060791,\n",
       "  5.202150106430054,\n",
       "  4.977761960029602,\n",
       "  5.787829828262329,\n",
       "  5.17901725769043,\n",
       "  5.157639503479004,\n",
       "  5.016850137710572,\n",
       "  5.468666839599609,\n",
       "  5.752291059494018,\n",
       "  6.263216543197632,\n",
       "  6.992251777648926,\n",
       "  6.147128391265869,\n",
       "  6.0332849502563475,\n",
       "  5.387934160232544,\n",
       "  5.783933043479919,\n",
       "  6.794558668136597,\n",
       "  6.085278844833374,\n",
       "  7.522140312194824,\n",
       "  5.813524389266968,\n",
       "  5.686823415756225,\n",
       "  5.531986713409424,\n",
       "  4.907023644447326,\n",
       "  5.016951394081116,\n",
       "  5.803138685226441,\n",
       "  6.656884884834289,\n",
       "  7.233192682266235,\n",
       "  6.033177232742309,\n",
       "  6.173614835739135,\n",
       "  5.753485655784607,\n",
       "  7.261880731582641,\n",
       "  5.94868278503418,\n",
       "  5.091372990608216,\n",
       "  5.240850615501404,\n",
       "  4.6281514167785645,\n",
       "  4.231604361534119,\n",
       "  6.203790497779846,\n",
       "  5.296445274353028,\n",
       "  4.14162175655365,\n",
       "  6.197388339042663,\n",
       "  5.864622259140015,\n",
       "  6.262616205215454,\n",
       "  5.438571548461914,\n",
       "  6.040299558639527,\n",
       "  7.211155414581299,\n",
       "  6.497986221313477,\n",
       "  6.678540515899658,\n",
       "  6.301174950599671,\n",
       "  7.475122880935669,\n",
       "  5.93039038181305,\n",
       "  6.447881507873535,\n",
       "  6.085549592971802,\n",
       "  5.6630433082580565,\n",
       "  6.071880769729614,\n",
       "  5.247538375854492,\n",
       "  5.736907434463501,\n",
       "  5.507794499397278,\n",
       "  5.654006147384644,\n",
       "  5.096574115753174,\n",
       "  5.638700270652771,\n",
       "  5.027742481231689,\n",
       "  5.9990376949310305,\n",
       "  6.563781213760376,\n",
       "  6.055589866638184,\n",
       "  6.050315809249878,\n",
       "  5.872243785858155,\n",
       "  6.2494265079498295,\n",
       "  5.829374647140503,\n",
       "  6.159810781478882,\n",
       "  6.259286546707154,\n",
       "  6.707480335235596,\n",
       "  5.3275449752807615,\n",
       "  5.824859189987182,\n",
       "  5.963145232200622,\n",
       "  5.687550640106201,\n",
       "  5.188319802284241,\n",
       "  4.918482518196106,\n",
       "  5.018352365493774,\n",
       "  4.9438331604003904,\n",
       "  4.74994204044342,\n",
       "  5.052185201644898,\n",
       "  4.4792273998260494,\n",
       "  4.604110646247864,\n",
       "  5.742903375625611,\n",
       "  4.951517486572266,\n",
       "  6.245442485809326,\n",
       "  5.217076396942138,\n",
       "  5.815571594238281,\n",
       "  4.867041730880738,\n",
       "  6.022863245010376,\n",
       "  6.620250296592713,\n",
       "  5.794053411483764,\n",
       "  5.650115346908569,\n",
       "  3.961584734916687,\n",
       "  4.992629361152649,\n",
       "  5.298745846748352,\n",
       "  5.436878824234009,\n",
       "  4.788566613197327,\n",
       "  4.815312337875366,\n",
       "  4.833074355125428,\n",
       "  4.179073095321655,\n",
       "  4.794574761390686,\n",
       "  5.4546245574951175,\n",
       "  5.471871066093445,\n",
       "  4.130838108062744,\n",
       "  4.551604604721069,\n",
       "  5.080830240249634,\n",
       "  4.578185272216797,\n",
       "  4.277477478981018,\n",
       "  4.629392123222351,\n",
       "  4.284872388839721,\n",
       "  5.146045565605164,\n",
       "  5.666404032707215,\n",
       "  5.752182340621948,\n",
       "  6.194050216674805,\n",
       "  5.906030321121216,\n",
       "  4.275713038444519,\n",
       "  4.375303840637207,\n",
       "  5.072883176803589,\n",
       "  6.649117708206177,\n",
       "  7.07277204990387,\n",
       "  5.661755084991455,\n",
       "  5.509908509254456,\n",
       "  7.135578298568726,\n",
       "  6.1338249206542965,\n",
       "  7.986136817932129,\n",
       "  5.286951303482056,\n",
       "  6.149295520782471,\n",
       "  5.964831399917602,\n",
       "  5.449718499183655,\n",
       "  4.4026206731796265,\n",
       "  3.975094199180603,\n",
       "  4.979675889015198,\n",
       "  5.129753589630127,\n",
       "  5.164227437973023,\n",
       "  5.2323373556137085,\n",
       "  5.024279761314392,\n",
       "  4.9738994359970095,\n",
       "  5.798486542701721,\n",
       "  6.078943157196045,\n",
       "  5.936107635498047,\n",
       "  5.137637615203857,\n",
       "  6.147782039642334,\n",
       "  7.188142919540406,\n",
       "  7.177981758117676,\n",
       "  5.400656151771545,\n",
       "  5.352507162094116,\n",
       "  5.024399018287658,\n",
       "  5.198153567314148,\n",
       "  5.638119697570801,\n",
       "  5.504107666015625,\n",
       "  4.8159850358963014,\n",
       "  5.132957506179809,\n",
       "  4.903553795814514,\n",
       "  4.697519683837891,\n",
       "  4.764419436454773,\n",
       "  4.2730810165405275,\n",
       "  4.468519616127014,\n",
       "  4.077879476547241,\n",
       "  6.0486010074615475,\n",
       "  6.015563917160034,\n",
       "  6.402031350135803,\n",
       "  5.5240607261657715,\n",
       "  5.049888443946839,\n",
       "  5.127952718734742,\n",
       "  4.869064688682556,\n",
       "  5.652468967437744,\n",
       "  4.585344123840332,\n",
       "  5.3803691387176515,\n",
       "  5.40016987323761,\n",
       "  4.804907608032226,\n",
       "  4.687501525878906,\n",
       "  7.767830181121826,\n",
       "  7.906366968154908,\n",
       "  5.221100282669068,\n",
       "  4.690117597579956,\n",
       "  7.128769826889038,\n",
       "  7.700072336196899,\n",
       "  6.423870611190796,\n",
       "  6.389927196502685,\n",
       "  4.547151756286621,\n",
       "  5.588548994064331,\n",
       "  4.628345727920532,\n",
       "  7.625544834136963,\n",
       "  6.239912223815918,\n",
       "  5.3600152969360355,\n",
       "  5.714565706253052,\n",
       "  5.839542818069458,\n",
       "  5.829920816421509,\n",
       "  6.488930988311767,\n",
       "  6.67498664855957,\n",
       "  4.881949806213379,\n",
       "  4.971314167976379,\n",
       "  4.684597444534302,\n",
       "  5.406043267250061,\n",
       "  5.279061508178711,\n",
       "  6.69963059425354,\n",
       "  5.7507568359375,\n",
       "  5.648095226287841,\n",
       "  6.546974754333496,\n",
       "  4.965898180007935,\n",
       "  4.5659528255462645,\n",
       "  4.806111264228821,\n",
       "  5.533972072601318,\n",
       "  5.314390420913696,\n",
       "  6.9355779647827145,\n",
       "  7.2414491176605225,\n",
       "  7.169086027145386,\n",
       "  7.214268445968628,\n",
       "  6.201241159439087,\n",
       "  5.754441595077514,\n",
       "  6.761150789260864,\n",
       "  6.006556510925293,\n",
       "  3.7330830335617065,\n",
       "  4.801060724258423,\n",
       "  6.378791427612304,\n",
       "  6.546735525131226,\n",
       "  6.746309518814087,\n",
       "  4.732637357711792,\n",
       "  4.579387760162353,\n",
       "  5.338568449020386,\n",
       "  6.40400505065918,\n",
       "  4.806664347648621,\n",
       "  4.714347243309021,\n",
       "  4.682484269142151,\n",
       "  6.275468969345093,\n",
       "  5.930786204338074,\n",
       "  5.838211488723755,\n",
       "  5.458735537528992,\n",
       "  7.255267810821533,\n",
       "  6.154998445510865,\n",
       "  5.0438968420028685,\n",
       "  4.463676190376281,\n",
       "  4.640419292449951,\n",
       "  6.074040842056275,\n",
       "  6.4017448902130125,\n",
       "  4.4941002368927006,\n",
       "  5.139716482162475,\n",
       "  6.791977071762085,\n",
       "  5.409636211395264,\n",
       "  5.818130874633789,\n",
       "  5.752049279212952,\n",
       "  6.785967588424683,\n",
       "  5.4781334400177,\n",
       "  5.625556755065918,\n",
       "  6.2090888023376465,\n",
       "  5.96210618019104,\n",
       "  7.035142183303833,\n",
       "  5.41989643573761,\n",
       "  5.453027462959289,\n",
       "  6.223368120193482,\n",
       "  6.618385601043701,\n",
       "  6.18808798789978,\n",
       "  6.4475151062011715,\n",
       "  6.514212369918823,\n",
       "  5.469065237045288,\n",
       "  5.315413904190064,\n",
       "  5.447492265701294,\n",
       "  5.334842157363892,\n",
       "  6.1423626899719235,\n",
       "  6.676806211471558,\n",
       "  6.269396209716797,\n",
       "  6.293911790847778,\n",
       "  5.868973731994629,\n",
       "  5.305555391311645,\n",
       "  4.717556738853455,\n",
       "  5.083277797698974,\n",
       "  5.905812454223633,\n",
       "  5.449969339370727,\n",
       "  5.296087741851807,\n",
       "  5.620490169525146,\n",
       "  5.107052111625672,\n",
       "  4.9684243440628055,\n",
       "  5.398723077774048,\n",
       "  5.031096410751343,\n",
       "  5.449978685379028,\n",
       "  5.228934383392334,\n",
       "  6.048585534095764,\n",
       "  5.101905250549317,\n",
       "  4.968778991699219,\n",
       "  4.38564510345459,\n",
       "  4.937823057174683,\n",
       "  5.388344311714173,\n",
       "  5.074069404602051,\n",
       "  5.51174578666687,\n",
       "  5.857082271575928,\n",
       "  5.1475417137146,\n",
       "  5.168081378936767,\n",
       "  6.134855580329895,\n",
       "  6.539331245422363,\n",
       "  5.78259379863739,\n",
       "  6.606181907653808,\n",
       "  6.00576171875,\n",
       "  4.273330307006836,\n",
       "  5.31013057231903,\n",
       "  5.761724877357483,\n",
       "  4.918474626541138,\n",
       "  5.0431359767913815,\n",
       "  5.442936086654663,\n",
       "  5.342524242401123,\n",
       "  5.511832427978516,\n",
       "  5.798232054710388,\n",
       "  5.654110550880432,\n",
       "  4.389549350738525,\n",
       "  4.655288362503052,\n",
       "  5.782688760757447,\n",
       "  5.926234102249145,\n",
       "  4.830412578582764,\n",
       "  5.277047061920166,\n",
       "  4.873946404457092,\n",
       "  6.149394702911377,\n",
       "  4.879601645469665,\n",
       "  5.794140911102295,\n",
       "  5.493089294433593,\n",
       "  5.469853067398072,\n",
       "  5.228500604629517,\n",
       "  4.996673488616944,\n",
       "  4.980967783927918,\n",
       "  5.127568984031678,\n",
       "  4.519753885269165,\n",
       "  6.13575234413147,\n",
       "  4.921429443359375,\n",
       "  5.935416293144226,\n",
       "  6.433559942245483,\n",
       "  6.369150686264038,\n",
       "  5.368411517143249,\n",
       "  5.37772045135498,\n",
       "  4.627305817604065,\n",
       "  5.694918894767762,\n",
       "  6.409399795532226,\n",
       "  5.890434050559998,\n",
       "  6.964824485778808,\n",
       "  6.83567624092102,\n",
       "  7.651111364364624,\n",
       "  5.764188194274903,\n",
       "  4.391774106025696,\n",
       "  4.396871614456177,\n",
       "  5.451360034942627,\n",
       "  4.957612442970276,\n",
       "  4.899231553077698,\n",
       "  4.943484234809875,\n",
       "  4.951373624801636,\n",
       "  5.484321331977844,\n",
       "  6.503752708435059,\n",
       "  5.0539216041564945,\n",
       "  4.999001145362854,\n",
       "  5.33765926361084,\n",
       "  ...],\n",
       " [7.245416369438171,\n",
       "  6.680334994792938,\n",
       "  6.470734353065491,\n",
       "  6.318651058673859,\n",
       "  6.225259990692138,\n",
       "  6.196219606399536,\n",
       "  6.175847387313842,\n",
       "  6.199817292690277,\n",
       "  6.13998907327652,\n",
       "  6.129893281459808,\n",
       "  6.122682845592498,\n",
       "  6.1011220288276675,\n",
       "  6.107194497585296,\n",
       "  6.080706226825714,\n",
       "  6.067921416759491,\n",
       "  6.058471245765686,\n",
       "  6.0528228259086605,\n",
       "  6.025758872032165,\n",
       "  5.994681508541107,\n",
       "  6.0046279239654545,\n",
       "  5.981147539615631,\n",
       "  5.934836421012879,\n",
       "  5.930049126148224,\n",
       "  5.921682798862458,\n",
       "  5.905445303916931,\n",
       "  5.8998711919784546,\n",
       "  5.91099454164505,\n",
       "  5.8813250088691715,\n",
       "  5.8808762335777285,\n",
       "  5.840916790962219,\n",
       "  5.846558084487915,\n",
       "  5.833019502162934,\n",
       "  5.831278729438782,\n",
       "  5.821563606262207,\n",
       "  5.829563727378845,\n",
       "  5.828631963729858,\n",
       "  5.787161104679107,\n",
       "  5.803503897190094,\n",
       "  5.762726800441742,\n",
       "  5.781457602977753,\n",
       "  5.760579288005829,\n",
       "  5.763808817863464,\n",
       "  5.73967425107956,\n",
       "  5.737314684391022,\n",
       "  5.7411223101615905,\n",
       "  5.72505033493042,\n",
       "  5.744922423362732,\n",
       "  5.71479163646698,\n",
       "  5.713029968738556,\n",
       "  5.715119490623474,\n",
       "  5.732736835479736,\n",
       "  5.699819061756134,\n",
       "  5.690701608657837,\n",
       "  5.687506732940673,\n",
       "  5.6998904824256895,\n",
       "  5.676526877880097,\n",
       "  5.683117532730103,\n",
       "  5.6903985357284546,\n",
       "  5.678159835338593,\n",
       "  5.660548505783081,\n",
       "  5.670336539745331,\n",
       "  5.677994711399078,\n",
       "  5.673890790939331,\n",
       "  5.6617777943611145,\n",
       "  5.650343616008758,\n",
       "  5.653016486167908,\n",
       "  5.662503061294555,\n",
       "  5.646020603179932,\n",
       "  5.629682836532592,\n",
       "  5.659725067615509,\n",
       "  5.628231549263001,\n",
       "  5.626786787509918,\n",
       "  5.608688573837281,\n",
       "  5.621481273174286,\n",
       "  5.6312362241745,\n",
       "  5.608325531482697,\n",
       "  5.6181265735626225,\n",
       "  5.615655066967011,\n",
       "  5.672789597511292,\n",
       "  5.595486977100372,\n",
       "  5.5970738530159,\n",
       "  5.631119167804718,\n",
       "  5.618606009483337,\n",
       "  5.607171313762665,\n",
       "  5.593022050857544,\n",
       "  5.591059126853943,\n",
       "  5.57521591424942,\n",
       "  5.591624948978424,\n",
       "  5.583046486377716,\n",
       "  5.593489897251129,\n",
       "  5.593230130672455,\n",
       "  5.600811488628388,\n",
       "  5.589800581932068,\n",
       "  5.576145215034485,\n",
       "  5.583232200145721,\n",
       "  5.597989501953125,\n",
       "  5.602239985466003,\n",
       "  5.605421149730683,\n",
       "  5.599269189834595,\n",
       "  5.575699138641357,\n",
       "  5.569144971370697,\n",
       "  5.588620753288269,\n",
       "  5.602650237083435,\n",
       "  5.572305841445923,\n",
       "  5.562455463409424,\n",
       "  5.573284029960632,\n",
       "  5.558000974655151,\n",
       "  5.546913888454437,\n",
       "  5.587684104442596,\n",
       "  5.561171379089355,\n",
       "  5.568756761550904,\n",
       "  5.585530037879944,\n",
       "  5.588756148815155,\n",
       "  5.567458093166351,\n",
       "  5.577282378673553,\n",
       "  5.544870579242707,\n",
       "  5.542443220615387,\n",
       "  5.542432079315185,\n",
       "  5.735492739677429,\n",
       "  5.54092312335968,\n",
       "  5.549967031478882,\n",
       "  5.544525465965271,\n",
       "  5.552044973373413,\n",
       "  5.540502598285675,\n",
       "  5.5416537857055665,\n",
       "  5.550566854476929,\n",
       "  5.553891224861145,\n",
       "  5.548611760139465,\n",
       "  5.553943028450012,\n",
       "  5.5627184987068174,\n",
       "  5.5586282968521115,\n",
       "  5.556449666023254,\n",
       "  5.558663156032562,\n",
       "  5.557795634269715,\n",
       "  5.558636140823364,\n",
       "  5.548997454643249,\n",
       "  5.541335878372192,\n",
       "  5.547105932235718,\n",
       "  5.539716968536377,\n",
       "  5.528266532421112,\n",
       "  5.536077728271485,\n",
       "  5.550821118354797,\n",
       "  5.524007515907288,\n",
       "  5.532891247272492,\n",
       "  5.576526520252227,\n",
       "  5.5258332133293155,\n",
       "  5.511763825416565,\n",
       "  5.513187546730041,\n",
       "  5.51458357334137,\n",
       "  5.537550065517426,\n",
       "  5.53143848657608,\n",
       "  5.528161709308624,\n",
       "  5.512624049186707,\n",
       "  5.519149622917175,\n",
       "  5.509731774330139,\n",
       "  5.514077348709106,\n",
       "  5.507410185337067,\n",
       "  5.508165748119354,\n",
       "  5.529531106948853,\n",
       "  5.543745079040527,\n",
       "  5.539859416484833,\n",
       "  5.502835955619812,\n",
       "  5.4969240975379945,\n",
       "  5.524986555576325,\n",
       "  5.509506371021271,\n",
       "  5.544954075813293,\n",
       "  5.515217959880829,\n",
       "  5.55194893360138,\n",
       "  5.540357015132904,\n",
       "  5.569525730609894,\n",
       "  5.5017521190643315,\n",
       "  5.504265260696411,\n",
       "  5.506436772346497,\n",
       "  5.501188266277313,\n",
       "  5.50097757101059,\n",
       "  5.513143775463104,\n",
       "  5.538262820243835,\n",
       "  5.519390833377838,\n",
       "  5.51695104598999,\n",
       "  5.5092833662033085,\n",
       "  5.508618688583374,\n",
       "  5.54057514667511,\n",
       "  5.543294970989227,\n",
       "  5.523396706581115,\n",
       "  5.51969137430191,\n",
       "  5.50514476776123,\n",
       "  5.49916916847229,\n",
       "  5.5045303845405575,\n",
       "  5.51705623626709,\n",
       "  5.506055700778961,\n",
       "  5.514873085021972,\n",
       "  5.489295289516449,\n",
       "  5.530239131450653,\n",
       "  5.509611086845398,\n",
       "  5.537827205657959,\n",
       "  5.519608664512634,\n",
       "  5.514402997493744,\n",
       "  5.494947094917297,\n",
       "  5.507410652637482,\n",
       "  5.497708232402801,\n",
       "  5.490549113750458,\n",
       "  5.496476194858551,\n",
       "  5.48823961019516,\n",
       "  5.495980708599091,\n",
       "  5.4880232548713686,\n",
       "  5.4823912596702575,\n",
       "  5.508169069290161,\n",
       "  5.474332962036133,\n",
       "  5.479213454723358,\n",
       "  5.482025990486145,\n",
       "  5.489158034324646,\n",
       "  5.484335751533508,\n",
       "  5.489168365001678,\n",
       "  5.475418951511383,\n",
       "  5.481247971057892,\n",
       "  5.483462524414063,\n",
       "  5.46841783285141,\n",
       "  5.480075650215149,\n",
       "  5.470469655990601,\n",
       "  5.48255348443985,\n",
       "  5.479779031276703,\n",
       "  5.490689828395843,\n",
       "  5.4815147018432615,\n",
       "  5.468220224380493,\n",
       "  5.465376470088959,\n",
       "  5.479302787780762,\n",
       "  5.474786665439606,\n",
       "  5.476042535305023,\n",
       "  5.478420295715332,\n",
       "  5.498100717067718,\n",
       "  5.46939597606659,\n",
       "  5.512475361824036,\n",
       "  5.4778752970695495,\n",
       "  5.476477384567261,\n",
       "  5.491910684108734,\n",
       "  5.5298936605453495,\n",
       "  5.483538141250611,\n",
       "  5.472585775852203,\n",
       "  5.481951563358307,\n",
       "  5.4786402177810665,\n",
       "  5.478190271854401,\n",
       "  5.478592004776001,\n",
       "  5.488216378688812,\n",
       "  5.49766028881073,\n",
       "  5.501064393520355,\n",
       "  5.4718691658973695,\n",
       "  5.462146737575531,\n",
       "  5.462630355358124,\n",
       "  5.480279817581176,\n",
       "  5.472134244441986,\n",
       "  5.450162186622619,\n",
       "  5.458500444889069,\n",
       "  5.460223062038422,\n",
       "  5.454596164226532,\n",
       "  5.46038635969162,\n",
       "  5.474314708709716,\n",
       "  5.458296198844909,\n",
       "  5.454608938694,\n",
       "  5.462303535938263,\n",
       "  5.449263753890992,\n",
       "  5.464538021087646,\n",
       "  5.467208125591278,\n",
       "  5.464437656402588,\n",
       "  5.469743602275848,\n",
       "  5.4857557582855225,\n",
       "  5.4844198894500735,\n",
       "  5.482403795719147,\n",
       "  5.474662797451019,\n",
       "  5.475493819713592,\n",
       "  5.474709079265595,\n",
       "  5.467932076454162,\n",
       "  5.4459184813499455,\n",
       "  5.4577000617980955,\n",
       "  5.450823919773102,\n",
       "  5.453414528369904,\n",
       "  5.453604998588562,\n",
       "  5.444167902469635,\n",
       "  5.44962541103363,\n",
       "  5.458849303722381,\n",
       "  5.461373891830444,\n",
       "  5.470712058544159,\n",
       "  5.447220935821533,\n",
       "  5.437355773448944,\n",
       "  5.446605062484741,\n",
       "  5.433364334106446,\n",
       "  5.4495701050758365,\n",
       "  5.445662431716919,\n",
       "  5.447372868061065,\n",
       "  5.44311817407608,\n",
       "  5.44827810049057,\n",
       "  5.475683438777923,\n",
       "  5.452490282058716,\n",
       "  5.4484272480010985,\n",
       "  5.441723177433014,\n",
       "  5.458139972686768,\n",
       "  5.466663110256195,\n",
       "  5.4498926091194155,\n",
       "  5.438243145942688,\n",
       "  5.446303758621216,\n",
       "  5.445319271087646,\n",
       "  5.448975517749786,\n",
       "  5.445893681049347,\n",
       "  5.449591650962829,\n",
       "  5.445338978767395,\n",
       "  5.448348777294159,\n",
       "  5.450868380069733,\n",
       "  5.429889802932739,\n",
       "  5.4319499754905705,\n",
       "  5.432223207950592,\n",
       "  5.460746488571167,\n",
       "  5.435541191101074,\n",
       "  5.435592999458313,\n",
       "  5.42202873468399,\n",
       "  5.425327334403992,\n",
       "  5.431659543514252,\n",
       "  5.440602822303772,\n",
       "  5.43539473772049,\n",
       "  5.427912127971649,\n",
       "  5.439371044635773,\n",
       "  5.442399680614471,\n",
       "  5.436232497692108,\n",
       "  5.477224781513214,\n",
       "  5.464334902763366,\n",
       "  5.46466153383255,\n",
       "  5.4709604191780095,\n",
       "  5.432570052146912,\n",
       "  5.442290585041047,\n",
       "  5.428505675792694,\n",
       "  5.4516660237312315,\n",
       "  5.447890548706055,\n",
       "  5.434723353385925,\n",
       "  5.443003296852112,\n",
       "  5.445059397220612,\n",
       "  5.424639298915863,\n",
       "  5.431094338893891,\n",
       "  5.4340926456451415,\n",
       "  5.43952632188797,\n",
       "  5.4341528344154355,\n",
       "  5.444908037185669,\n",
       "  5.433905503749847,\n",
       "  5.434704945087433,\n",
       "  5.4177128911018375,\n",
       "  5.427024772167206,\n",
       "  5.437050037384033,\n",
       "  5.419531173706055,\n",
       "  5.4280536532402035,\n",
       "  5.4335951280593875,\n",
       "  5.442739534378052,\n",
       "  5.431729633808136,\n",
       "  5.428626756668091,\n",
       "  5.432531702518463,\n",
       "  5.436032912731171,\n",
       "  5.429295337200164,\n",
       "  5.416037316322327,\n",
       "  5.434532914161682,\n",
       "  5.426377701759338,\n",
       "  5.437062878608703,\n",
       "  5.444365110397339,\n",
       "  5.456553409099579,\n",
       "  5.436508507728576,\n",
       "  5.420670921802521,\n",
       "  5.429717419147491,\n",
       "  5.422873101234436],\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_batches(\n",
    "    batches_train=batches_train,\n",
    "    batches_test=batches_dev,\n",
    "    epochs=-1, \n",
    "    lr=3e-3, \n",
    "    print_every=10,\n",
    "    test_every=30,\n",
    "    patience=20,\n",
    "    min_improvement=.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([23702, 8]), 23702)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_test = it.RestartableBatchIterator(list(idx_test), batch_size*4)\n",
    "batches_test = it.RestartableMapIterator(batches_test, lambda batch: T(batch).long().transpose(0, 1))\n",
    "\n",
    "embs_test=[]\n",
    "for batch in batches_test: \n",
    "    batch = emb_layer(batch)\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        batch = model.encoder(batch)\n",
    "    embs_test.append(batch)\n",
    "embs_test = torch.vstack(embs_test)\n",
    "embs_test.shape, len(simple_targets_test)\n",
    "\n",
    "batches_dev = it.RestartableBatchIterator(list(idx_dev), batch_size*4)\n",
    "batches_dev = it.RestartableMapIterator(batches_dev, lambda batch: T(batch).long().transpose(0, 1))\n",
    "\n",
    "embs_dev=[]\n",
    "for batch in batches_dev: \n",
    "    batch = emb_layer(batch)\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        batch = model.encoder(batch)\n",
    "    embs_dev.append(batch)\n",
    "embs_dev = torch.vstack(embs_dev)\n",
    "embs_dev.shape, len(simple_targets_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lrcv10_acc': 0.6535037758933468, 'tree10_acc': 0.674851284647513}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result={}\n",
    "for clf_name, clf_init in classifiers.items():\n",
    "    clf = clf_init()\n",
    "    clf.fit(embs_dev[:n_samples], simple_targets_dev[:n_samples])\n",
    "    preds = clf.predict(embs_test)\n",
    "    acc = accuracy_score(simple_targets_test, preds)\n",
    "    f1_micro = f1_score(simple_targets_test, preds, average='micro')\n",
    "    f1_macro = f1_score(simple_targets_test, preds, average='macro')\n",
    "    #print(f'{clf_name}\\tacc={acc}\\tf1_micro={f1_micro}\\tf1_macro={f1_macro}')\n",
    "    result[f'{clf_name}_acc'] = acc\n",
    "    #result[f'{clf_name}_f1ma'] = f1_macro\n",
    "    #result[f'{clf_name}_f1mi'] = f1_micro\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now autoencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "duplicate = lambda x: (x, x)\n",
    "batches_train = it.RestartableMapIterator(idx_train, duplicate)\n",
    "batches_train = it.RestartableBatchIterator(batches_train, batch_size=batch_size)\n",
    "\n",
    "batches_dev = it.RestartableMapIterator(idx_dev[:2000], duplicate)\n",
    "batches_dev = it.RestartableBatchIterator(batches_dev, batch_size=batch_size*4)\n",
    "#print(*zip(*next(iter(batches_dev))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_latent = 8\n",
    "dim_token = 8\n",
    "emb_layer = module.init_embedding(len(ctest), dim_token)\n",
    "hyperparams = {\n",
    "    'bidirectional': True, \n",
    "    'hidden_size': 64,\n",
    "    'num_layers': 2,\n",
    "    'dropout': .1,\n",
    "}\n",
    "model_ae = SequenceSG(dim_token, dim_latent, hyperparams, emb_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using a test set of 16 batches (BS=128)\n",
      "Epoch 00\tBatch 10\t0.1582s/batch\ttrain_loss = 10.9084\n",
      "Epoch 00\tBatch 20\t0.1540s/batch\ttrain_loss = 7.9610\n",
      "Epoch 00\tBatch 30\t0.1561s/batch\ttrain_loss = 7.4731\n",
      "Epoch 00\tBatch 30\ttest_loss = 6.2067\n",
      "new best loss (inf -> 6.206695)\n",
      "Epoch 00\tBatch 40\t0.1537s/batch\ttrain_loss = 6.4377\n",
      "Epoch 00\tBatch 50\t0.1543s/batch\ttrain_loss = 5.8636\n",
      "Epoch 00\tBatch 60\t0.1522s/batch\ttrain_loss = 5.6789\n",
      "Epoch 00\tBatch 60\ttest_loss = 5.8271\n",
      "new best loss (6.206695 -> 5.827077)\n",
      "Epoch 00\tBatch 70\t0.1526s/batch\ttrain_loss = 5.7004\n",
      "Epoch 00\tBatch 80\t0.1533s/batch\ttrain_loss = 5.6049\n",
      "Epoch 00\tBatch 90\t0.1550s/batch\ttrain_loss = 5.6420\n",
      "Epoch 00\tBatch 90\ttest_loss = 5.7064\n",
      "new best loss (5.827077 -> 5.706404)\n",
      "Epoch 00\tBatch 100\t0.1528s/batch\ttrain_loss = 5.0152\n",
      "Epoch 00\tBatch 110\t0.1555s/batch\ttrain_loss = 6.4473\n",
      "Epoch 00\tBatch 120\t0.1534s/batch\ttrain_loss = 5.4879\n",
      "Epoch 00\tBatch 120\ttest_loss = 5.6296\n",
      "new best loss (5.706404 -> 5.629607)\n",
      "Epoch 00\tBatch 130\t0.1538s/batch\ttrain_loss = 4.9949\n",
      "Epoch 00\tBatch 140\t0.1547s/batch\ttrain_loss = 5.2879\n",
      "Epoch 00\tBatch 150\t0.1550s/batch\ttrain_loss = 5.5775\n",
      "Epoch 00\tBatch 150\ttest_loss = 5.4905\n",
      "new best loss (5.629607 -> 5.490467)\n",
      "Epoch 00\tBatch 160\t0.1550s/batch\ttrain_loss = 5.9781\n",
      "Epoch 00\tBatch 170\t0.1553s/batch\ttrain_loss = 5.5605\n",
      "Epoch 00\tBatch 180\t0.1550s/batch\ttrain_loss = 4.9192\n",
      "Epoch 00\tBatch 180\ttest_loss = 5.3681\n",
      "new best loss (5.490467 -> 5.368062)\n",
      "Epoch 00\tBatch 190\t0.1552s/batch\ttrain_loss = 5.1081\n",
      "Epoch 00\tBatch 200\t0.1572s/batch\ttrain_loss = 6.5891\n",
      "Epoch 00\tBatch 210\t0.1561s/batch\ttrain_loss = 5.5739\n",
      "Epoch 00\tBatch 210\ttest_loss = 5.3645\n",
      "new best loss (5.368062 -> 5.364451)\n",
      "Epoch 00\tBatch 220\t0.1635s/batch\ttrain_loss = 4.8096\n",
      "Epoch 00\tBatch 230\t0.1551s/batch\ttrain_loss = 5.5496\n",
      "Epoch 00\tBatch 240\t0.1545s/batch\ttrain_loss = 5.7164\n",
      "Epoch 00\tBatch 240\ttest_loss = 5.2363\n",
      "new best loss (5.364451 -> 5.236259)\n",
      "Epoch 00\tBatch 250\t0.1542s/batch\ttrain_loss = 4.9295\n",
      "Epoch 00\tBatch 260\t0.1540s/batch\ttrain_loss = 4.1169\n",
      "Epoch 00\tBatch 270\t0.1548s/batch\ttrain_loss = 5.1246\n",
      "Epoch 00\tBatch 270\ttest_loss = 5.1109\n",
      "new best loss (5.236259 -> 5.110878)\n",
      "Epoch 00\tBatch 280\t0.1554s/batch\ttrain_loss = 5.2279\n",
      "Epoch 00\tBatch 290\t0.1553s/batch\ttrain_loss = 4.8878\n",
      "Epoch 00\tBatch 300\t0.1550s/batch\ttrain_loss = 5.3891\n",
      "Epoch 00\tBatch 300\ttest_loss = 5.1019\n",
      "new best loss (5.110878 -> 5.101859)\n",
      "Epoch 00\tBatch 310\t0.1574s/batch\ttrain_loss = 5.8286\n",
      "Epoch 00\tBatch 320\t0.1552s/batch\ttrain_loss = 4.8893\n",
      "Epoch 00\tBatch 330\t0.1557s/batch\ttrain_loss = 4.5120\n",
      "Epoch 00\tBatch 330\ttest_loss = 5.0289\n",
      "new best loss (5.101859 -> 5.028891)\n",
      "Epoch 00\tBatch 340\t0.1560s/batch\ttrain_loss = 5.6102\n",
      "Epoch 00\tBatch 350\t0.1591s/batch\ttrain_loss = 5.3629\n",
      "Epoch 00\tBatch 360\t0.1588s/batch\ttrain_loss = 4.9580\n",
      "Epoch 00\tBatch 360\ttest_loss = 5.0588\n",
      "Epoch 00\tBatch 370\t0.1590s/batch\ttrain_loss = 6.6472\n",
      "Epoch 00\tBatch 380\t0.1572s/batch\ttrain_loss = 5.7813\n",
      "Epoch 00\tBatch 390\t0.1549s/batch\ttrain_loss = 4.8425\n",
      "Epoch 00\tBatch 390\ttest_loss = 4.9262\n",
      "new best loss (5.028891 -> 4.926201)\n",
      "Epoch 00\tBatch 400\t0.1562s/batch\ttrain_loss = 5.5322\n",
      "Epoch 00\tBatch 410\t0.1566s/batch\ttrain_loss = 5.6503\n",
      "Epoch 00\tBatch 420\t0.1454s/batch\ttrain_loss = 5.1131\n",
      "Epoch 00\tBatch 420\ttest_loss = 4.9194\n",
      "new best loss (4.926201 -> 4.919364)\n",
      "Epoch 00\tBatch 430\t0.1471s/batch\ttrain_loss = 4.6392\n",
      "Epoch 00\tBatch 440\t0.1473s/batch\ttrain_loss = 5.0738\n",
      "Epoch 00\tBatch 450\t0.1462s/batch\ttrain_loss = 4.9050\n",
      "Epoch 00\tBatch 450\ttest_loss = 4.8020\n",
      "new best loss (4.919364 -> 4.801973)\n",
      "Epoch 00\tBatch 460\t0.1470s/batch\ttrain_loss = 4.4162\n",
      "Epoch 00\tBatch 470\t0.1481s/batch\ttrain_loss = 5.2515\n",
      "Epoch 00\tBatch 480\t0.1468s/batch\ttrain_loss = 4.9937\n",
      "Epoch 00\tBatch 480\ttest_loss = 4.7936\n",
      "new best loss (4.801973 -> 4.793634)\n",
      "Epoch 00\tBatch 490\t0.1488s/batch\ttrain_loss = 5.4344\n",
      "Epoch 00\tBatch 500\t0.1474s/batch\ttrain_loss = 4.9829\n",
      "Epoch 00\tBatch 510\t0.1489s/batch\ttrain_loss = 5.5601\n",
      "Epoch 00\tBatch 510\ttest_loss = 4.7478\n",
      "new best loss (4.793634 -> 4.747814)\n",
      "Epoch 00\tBatch 520\t0.1464s/batch\ttrain_loss = 4.6330\n",
      "Epoch 00\tBatch 530\t0.1484s/batch\ttrain_loss = 4.2649\n",
      "Epoch 00\tBatch 540\t0.1456s/batch\ttrain_loss = 4.1176\n",
      "Epoch 00\tBatch 540\ttest_loss = 4.6937\n",
      "new best loss (4.747814 -> 4.693735)\n",
      "Epoch 00\tBatch 550\t0.1480s/batch\ttrain_loss = 4.1077\n",
      "Epoch 00\tBatch 560\t0.1465s/batch\ttrain_loss = 4.5294\n",
      "Epoch 00\tBatch 570\t0.1462s/batch\ttrain_loss = 4.0302\n",
      "Epoch 00\tBatch 570\ttest_loss = 4.6356\n",
      "new best loss (4.693735 -> 4.635618)\n",
      "Epoch 00\tBatch 580\t0.1492s/batch\ttrain_loss = 5.7113\n",
      "Epoch 00\tBatch 590\t0.1460s/batch\ttrain_loss = 4.5741\n",
      "Epoch 00\tBatch 600\t0.1473s/batch\ttrain_loss = 4.6584\n",
      "Epoch 00\tBatch 600\ttest_loss = 4.6347\n",
      "new best loss (4.635618 -> 4.634698)\n",
      "Epoch 00\tBatch 610\t0.1475s/batch\ttrain_loss = 4.3082\n",
      "Epoch 00\tBatch 620\t0.1473s/batch\ttrain_loss = 4.7591\n",
      "Epoch 00\tBatch 630\t0.1494s/batch\ttrain_loss = 5.6655\n",
      "Epoch 00\tBatch 630\ttest_loss = 4.6250\n",
      "new best loss (4.634698 -> 4.624985)\n",
      "Epoch 00\tBatch 640\t0.1479s/batch\ttrain_loss = 4.3640\n",
      "Epoch 00\tBatch 650\t0.1501s/batch\ttrain_loss = 5.5523\n",
      "Epoch 00\tBatch 660\t0.1486s/batch\ttrain_loss = 4.9764\n",
      "Epoch 00\tBatch 660\ttest_loss = 4.6222\n",
      "new best loss (4.624985 -> 4.622200)\n",
      "Epoch 00\tBatch 670\t0.1480s/batch\ttrain_loss = 4.5168\n",
      "Epoch 00\tBatch 680\t0.1472s/batch\ttrain_loss = 4.3226\n",
      "Epoch 00\tBatch 690\t0.1462s/batch\ttrain_loss = 4.0642\n",
      "Epoch 00\tBatch 690\ttest_loss = 4.5671\n",
      "new best loss (4.622200 -> 4.567075)\n",
      "Epoch 00\tBatch 700\t0.1501s/batch\ttrain_loss = 4.8799\n",
      "Epoch 00\tBatch 710\t0.1480s/batch\ttrain_loss = 4.8501\n",
      "Epoch 00\tBatch 720\t0.1472s/batch\ttrain_loss = 4.5843\n",
      "Epoch 00\tBatch 720\ttest_loss = 4.5532\n",
      "new best loss (4.567075 -> 4.553177)\n",
      "Epoch 00\tBatch 730\t0.1477s/batch\ttrain_loss = 4.4965\n",
      "Epoch 00\tBatch 740\t0.1474s/batch\ttrain_loss = 4.8869\n",
      "Epoch 00\tBatch 750\t0.1474s/batch\ttrain_loss = 4.2121\n",
      "Epoch 00\tBatch 750\ttest_loss = 4.4712\n",
      "new best loss (4.553177 -> 4.471228)\n",
      "Epoch 00\tBatch 760\t0.1472s/batch\ttrain_loss = 4.2995\n",
      "Epoch 00\tBatch 770\t0.1484s/batch\ttrain_loss = 4.7628\n",
      "Epoch 00\tBatch 780\t0.1485s/batch\ttrain_loss = 5.1563\n",
      "Epoch 00\tBatch 780\ttest_loss = 4.4402\n",
      "new best loss (4.471228 -> 4.440236)\n",
      "Epoch 00\tBatch 790\t0.1465s/batch\ttrain_loss = 3.4430\n",
      "Epoch 00\tBatch 800\t0.1483s/batch\ttrain_loss = 4.6750\n",
      "Epoch 00\tBatch 810\t0.1474s/batch\ttrain_loss = 4.3052\n",
      "Epoch 00\tBatch 810\ttest_loss = 4.4749\n",
      "Epoch 00\tBatch 820\t0.1480s/batch\ttrain_loss = 4.8798\n",
      "Epoch 00\tBatch 830\t0.1489s/batch\ttrain_loss = 4.9919\n",
      "Epoch 00\tBatch 840\t0.1495s/batch\ttrain_loss = 5.2670\n",
      "Epoch 00\tBatch 840\ttest_loss = 4.4329\n",
      "new best loss (4.440236 -> 4.432889)\n",
      "Epoch 00\tBatch 850\t0.1459s/batch\ttrain_loss = 3.9925\n",
      "Epoch 00\tBatch 860\t0.1481s/batch\ttrain_loss = 4.8963\n",
      "Epoch 00\tBatch 870\t0.1460s/batch\ttrain_loss = 3.7516\n",
      "Epoch 00\tBatch 870\ttest_loss = 4.3662\n",
      "new best loss (4.432889 -> 4.366242)\n",
      "Epoch 00\tBatch 880\t0.1466s/batch\ttrain_loss = 3.8170\n",
      "Epoch 00\tBatch 890\t0.1456s/batch\ttrain_loss = 4.0316\n",
      "Epoch 00\tBatch 900\t0.1469s/batch\ttrain_loss = 3.9714\n",
      "Epoch 00\tBatch 900\ttest_loss = 4.3122\n",
      "new best loss (4.366242 -> 4.312155)\n",
      "Epoch 00\tBatch 910\t0.1480s/batch\ttrain_loss = 4.4045\n",
      "Epoch 00\tBatch 920\t0.1480s/batch\ttrain_loss = 4.2910\n",
      "Epoch 00\tBatch 930\t0.1462s/batch\ttrain_loss = 3.8417\n",
      "Epoch 00\tBatch 930\ttest_loss = 4.3856\n",
      "Epoch 00\tBatch 940\t0.1462s/batch\ttrain_loss = 4.1355\n",
      "Epoch 00\tBatch 950\t0.1464s/batch\ttrain_loss = 3.8953\n",
      "Epoch 00\tBatch 960\t0.1489s/batch\ttrain_loss = 4.9593\n",
      "Epoch 00\tBatch 960\ttest_loss = 4.2893\n",
      "new best loss (4.312155 -> 4.289312)\n",
      "Epoch 00\tBatch 970\t0.1467s/batch\ttrain_loss = 3.6335\n",
      "Epoch 00\tBatch 980\t0.1479s/batch\ttrain_loss = 3.9946\n",
      "Epoch 00\tBatch 990\t0.1466s/batch\ttrain_loss = 4.0552\n",
      "Epoch 00\tBatch 990\ttest_loss = 4.2798\n",
      "new best loss (4.289312 -> 4.279831)\n",
      "Epoch 00\tBatch 1000\t0.1483s/batch\ttrain_loss = 4.6936\n",
      "Epoch 00\tBatch 1010\t0.1494s/batch\ttrain_loss = 4.6999\n",
      "Epoch 00\tBatch 1020\t0.1500s/batch\ttrain_loss = 5.1604\n",
      "Epoch 00\tBatch 1020\ttest_loss = 4.2653\n",
      "new best loss (4.279831 -> 4.265286)\n",
      "Epoch 00\tBatch 1030\t0.1478s/batch\ttrain_loss = 3.5937\n",
      "Epoch 00\tBatch 1040\t0.1493s/batch\ttrain_loss = 5.0793\n",
      "Epoch 00\tBatch 1050\t0.1475s/batch\ttrain_loss = 4.1552\n",
      "Epoch 00\tBatch 1050\ttest_loss = 4.2096\n",
      "new best loss (4.265286 -> 4.209590)\n",
      "Epoch 00\tBatch 1060\t0.1495s/batch\ttrain_loss = 4.5334\n",
      "Epoch 00\tBatch 1070\t0.1498s/batch\ttrain_loss = 4.7715\n",
      "Epoch 00\tBatch 1080\t0.1471s/batch\ttrain_loss = 4.4787\n",
      "Epoch 00\tBatch 1080\ttest_loss = 4.1755\n",
      "new best loss (4.209590 -> 4.175495)\n",
      "Epoch 00\tBatch 1090\t0.1482s/batch\ttrain_loss = 4.1274\n",
      "Epoch 00\tBatch 1100\t0.1478s/batch\ttrain_loss = 4.4781\n",
      "Epoch 00\tBatch 1110\t0.1486s/batch\ttrain_loss = 4.2668\n",
      "Epoch 00\tBatch 1110\ttest_loss = 4.1402\n",
      "new best loss (4.175495 -> 4.140173)\n",
      "Epoch 00\tBatch 1120\t0.1491s/batch\ttrain_loss = 4.2088\n",
      "Epoch 00\tBatch 1130\t0.1503s/batch\ttrain_loss = 4.8998\n",
      "Epoch 00\tBatch 1140\t0.1482s/batch\ttrain_loss = 4.2744\n",
      "Epoch 00\tBatch 1140\ttest_loss = 4.1258\n",
      "new best loss (4.140173 -> 4.125818)\n",
      "Epoch 00\tBatch 1150\t0.1474s/batch\ttrain_loss = 3.7920\n",
      "Epoch 00\tBatch 1160\t0.1462s/batch\ttrain_loss = 3.3386\n",
      "Epoch 00\tBatch 1170\t0.1453s/batch\ttrain_loss = 3.4074\n",
      "Epoch 00\tBatch 1170\ttest_loss = 4.1373\n",
      "Epoch 00\tBatch 1180\t0.1489s/batch\ttrain_loss = 4.7469\n",
      "Epoch 00\tBatch 1190\t0.1484s/batch\ttrain_loss = 4.3573\n",
      "Epoch 00\tBatch 1200\t0.1478s/batch\ttrain_loss = 4.2623\n",
      "Epoch 00\tBatch 1200\ttest_loss = 4.1029\n",
      "new best loss (4.125818 -> 4.102878)\n",
      "Epoch 00\tBatch 1210\t0.1463s/batch\ttrain_loss = 3.7234\n",
      "Epoch 00\tBatch 1220\t0.1469s/batch\ttrain_loss = 3.6742\n",
      "Epoch 00\tBatch 1230\t0.1489s/batch\ttrain_loss = 4.0931\n",
      "Epoch 00\tBatch 1230\ttest_loss = 4.1527\n",
      "Epoch 00\tBatch 1240\t0.1485s/batch\ttrain_loss = 4.0308\n",
      "Epoch 00\tBatch 1250\t0.1470s/batch\ttrain_loss = 3.8295\n",
      "Epoch 00\tBatch 1260\t0.1487s/batch\ttrain_loss = 4.5329\n",
      "Epoch 00\tBatch 1260\ttest_loss = 4.1619\n",
      "Epoch 00\tBatch 1270\t0.1500s/batch\ttrain_loss = 4.7591\n",
      "Epoch 00\tBatch 1280\t0.1470s/batch\ttrain_loss = 4.0096\n",
      "Epoch 00\tBatch 1290\t0.1472s/batch\ttrain_loss = 3.6168\n",
      "Epoch 00\tBatch 1290\ttest_loss = 4.1237\n",
      "Epoch 00\tBatch 1300\t0.1458s/batch\ttrain_loss = 3.2075\n",
      "Epoch 00\tBatch 1310\t0.1478s/batch\ttrain_loss = 3.2511\n",
      "Epoch 00\tBatch 1320\t0.1483s/batch\ttrain_loss = 4.2462\n",
      "Epoch 00\tBatch 1320\ttest_loss = 4.0862\n",
      "new best loss (4.102878 -> 4.086200)\n",
      "Epoch 00\tBatch 1330\t0.1483s/batch\ttrain_loss = 3.9509\n",
      "Epoch 00\tBatch 1340\t0.1502s/batch\ttrain_loss = 5.1477\n",
      "Epoch 00\tBatch 1350\t0.1473s/batch\ttrain_loss = 3.3764\n",
      "Epoch 00\tBatch 1350\ttest_loss = 4.0359\n",
      "new best loss (4.086200 -> 4.035863)\n",
      "Epoch 00\tBatch 1360\t0.1472s/batch\ttrain_loss = 2.9847\n",
      "Epoch 00\tBatch 1370\t0.1466s/batch\ttrain_loss = 3.4798\n",
      "Epoch 00\tBatch 1380\t0.1464s/batch\ttrain_loss = 3.4901\n",
      "Epoch 00\tBatch 1380\ttest_loss = 4.0550\n",
      "Epoch 00\tBatch 1390\t0.1495s/batch\ttrain_loss = 4.4654\n",
      "Epoch 00\tBatch 1400\t0.1480s/batch\ttrain_loss = 4.2865\n",
      "Epoch 00\tBatch 1410\t0.1495s/batch\ttrain_loss = 4.4023\n",
      "Epoch 00\tBatch 1410\ttest_loss = 4.0675\n",
      "Epoch 00\tBatch 1420\t0.1488s/batch\ttrain_loss = 4.5818\n",
      "Epoch 00\tBatch 1430\t0.1493s/batch\ttrain_loss = 4.2641\n",
      "Epoch 00\tBatch 1440\t0.1480s/batch\ttrain_loss = 4.1787\n",
      "Epoch 00\tBatch 1440\ttest_loss = 4.0121\n",
      "new best loss (4.035863 -> 4.012133)\n",
      "Epoch 00\tBatch 1450\t0.1503s/batch\ttrain_loss = 4.4655\n",
      "Epoch 00\tBatch 1460\t0.1464s/batch\ttrain_loss = 3.0772\n",
      "Epoch 00\tBatch 1470\t0.1479s/batch\ttrain_loss = 3.7993\n",
      "Epoch 00\tBatch 1470\ttest_loss = 4.0628\n",
      "Epoch 00\tBatch 1480\t0.1453s/batch\ttrain_loss = 2.5533\n",
      "Epoch 00\tBatch 1490\t0.1478s/batch\ttrain_loss = 4.0413\n",
      "Epoch 00\tBatch 1500\t0.1486s/batch\ttrain_loss = 4.1645\n",
      "Epoch 00\tBatch 1500\ttest_loss = 4.0276\n",
      "Epoch 00\tBatch 1510\t0.1781s/batch\ttrain_loss = 3.6439\n",
      "Epoch 00\tBatch 1520\t0.1535s/batch\ttrain_loss = 4.3103\n",
      "Epoch 00\tBatch 1530\t0.1477s/batch\ttrain_loss = 3.1918\n",
      "Epoch 00\tBatch 1530\ttest_loss = 3.9422\n",
      "new best loss (4.012133 -> 3.942204)\n",
      "Epoch 00\tBatch 1540\t0.1473s/batch\ttrain_loss = 3.2424\n",
      "Epoch 00\tBatch 1550\t0.1488s/batch\ttrain_loss = 4.0721\n",
      "Epoch 00\tBatch 1560\t0.1466s/batch\ttrain_loss = 3.3294\n",
      "Epoch 00\tBatch 1560\ttest_loss = 3.8877\n",
      "new best loss (3.942204 -> 3.887722)\n",
      "Epoch 00\tBatch 1570\t0.1465s/batch\ttrain_loss = 3.3005\n",
      "Epoch 00\tBatch 1580\t0.1459s/batch\ttrain_loss = 2.8162\n",
      "Epoch 00\tBatch 1590\t0.1465s/batch\ttrain_loss = 3.1974\n",
      "Epoch 00\tBatch 1590\ttest_loss = 3.8798\n",
      "new best loss (3.887722 -> 3.879753)\n",
      "Epoch 00\tBatch 1600\t0.1490s/batch\ttrain_loss = 3.8972\n",
      "Epoch 00\tBatch 1610\t0.1495s/batch\ttrain_loss = 4.1900\n",
      "Epoch 00\tBatch 1620\t0.1480s/batch\ttrain_loss = 3.8341\n",
      "Epoch 00\tBatch 1620\ttest_loss = 3.9144\n",
      "Epoch 00\tBatch 1630\t0.1459s/batch\ttrain_loss = 3.6196\n",
      "Epoch 00\tBatch 1640\t0.1488s/batch\ttrain_loss = 5.2058\n",
      "Epoch 00\tBatch 1650\t0.1455s/batch\ttrain_loss = 3.6949\n",
      "Epoch 00\tBatch 1650\ttest_loss = 3.8736\n",
      "new best loss (3.879753 -> 3.873569)\n",
      "Epoch 00\tBatch 1660\t0.1502s/batch\ttrain_loss = 4.0128\n",
      "Epoch 00\tBatch 1670\t0.1458s/batch\ttrain_loss = 3.0423\n",
      "Epoch 00\tBatch 1680\t0.1461s/batch\ttrain_loss = 3.5787\n",
      "Epoch 00\tBatch 1680\ttest_loss = 3.8599\n",
      "new best loss (3.873569 -> 3.859885)\n",
      "Epoch 00\tBatch 1690\t0.1471s/batch\ttrain_loss = 3.4895\n",
      "Epoch 00\tBatch 1700\t0.1490s/batch\ttrain_loss = 4.5646\n",
      "Epoch 00\tBatch 1710\t0.1479s/batch\ttrain_loss = 4.1803\n",
      "Epoch 00\tBatch 1710\ttest_loss = 3.8350\n",
      "new best loss (3.859885 -> 3.835018)\n",
      "Epoch 00\tBatch 1720\t0.1508s/batch\ttrain_loss = 4.7293\n",
      "Epoch 00\tBatch 1730\t0.1476s/batch\ttrain_loss = 3.5102\n",
      "Epoch 00\tBatch 1740\t0.1492s/batch\ttrain_loss = 4.5820\n",
      "Epoch 00\tBatch 1740\ttest_loss = 3.8751\n",
      "Epoch 00\tBatch 1750\t0.1503s/batch\ttrain_loss = 4.3115\n",
      "Epoch 00\tBatch 1760\t0.1467s/batch\ttrain_loss = 3.1548\n",
      "Epoch 00\tBatch 1770\t0.1476s/batch\ttrain_loss = 3.7361\n",
      "Epoch 00\tBatch 1770\ttest_loss = 3.8800\n",
      "Epoch 00\tBatch 1780\t0.1511s/batch\ttrain_loss = 4.1819\n",
      "Epoch 00\tBatch 1790\t0.1566s/batch\ttrain_loss = 4.8876\n",
      "Epoch 00\tBatch 1800\t0.1531s/batch\ttrain_loss = 4.8075\n",
      "Epoch 00\tBatch 1800\ttest_loss = 3.8454\n",
      "Epoch 00\tBatch 1810\t0.1493s/batch\ttrain_loss = 4.0220\n",
      "Epoch 00\tBatch 1820\t0.1489s/batch\ttrain_loss = 3.7386\n",
      "Epoch 00\tBatch 1830\t0.1500s/batch\ttrain_loss = 4.0585\n",
      "Epoch 00\tBatch 1830\ttest_loss = 3.7803\n",
      "new best loss (3.835018 -> 3.780255)\n",
      "Epoch 00\tBatch 1840\t0.1499s/batch\ttrain_loss = 4.1632\n",
      "Epoch 00\tBatch 1850\t0.1565s/batch\ttrain_loss = 4.2414\n",
      "Epoch 00\tBatch 1860\t0.1631s/batch\ttrain_loss = 3.9141\n",
      "Epoch 00\tBatch 1860\ttest_loss = 3.8414\n",
      "Epoch 00\tBatch 1870\t0.1515s/batch\ttrain_loss = 3.2125\n",
      "Epoch 00\tBatch 1880\t0.1511s/batch\ttrain_loss = 2.7869\n",
      "Epoch 00\tBatch 1890\t0.1527s/batch\ttrain_loss = 3.7724\n",
      "Epoch 00\tBatch 1890\ttest_loss = 3.7784\n",
      "new best loss (3.780255 -> 3.778399)\n",
      "Epoch 00\tBatch 1900\t0.1497s/batch\ttrain_loss = 4.0168\n",
      "Epoch 00\tBatch 1910\t0.1494s/batch\ttrain_loss = 3.1602\n",
      "Epoch 00\tBatch 1920\t0.1489s/batch\ttrain_loss = 3.4805\n",
      "Epoch 00\tBatch 1920\ttest_loss = 3.7853\n",
      "Epoch 00\tBatch 1930\t0.1479s/batch\ttrain_loss = 3.2695\n",
      "Epoch 00\tBatch 1940\t0.1479s/batch\ttrain_loss = 3.1119\n",
      "Epoch 00\tBatch 1950\t0.1465s/batch\ttrain_loss = 2.7915\n",
      "Epoch 00\tBatch 1950\ttest_loss = 3.7643\n",
      "new best loss (3.778399 -> 3.764315)\n",
      "Epoch 00\tBatch 1960\t0.1487s/batch\ttrain_loss = 3.6836\n",
      "Epoch 00\tBatch 1970\t0.1482s/batch\ttrain_loss = 3.4808\n",
      "Epoch 00\tBatch 1980\t0.1490s/batch\ttrain_loss = 4.4512\n",
      "Epoch 00\tBatch 1980\ttest_loss = 3.7209\n",
      "new best loss (3.764315 -> 3.720891)\n",
      "Epoch 00\tBatch 1990\t0.1500s/batch\ttrain_loss = 4.6985\n",
      "Epoch 00\tBatch 2000\t0.1479s/batch\ttrain_loss = 3.5519\n",
      "Epoch 00\tBatch 2010\t0.1454s/batch\ttrain_loss = 2.9015\n",
      "Epoch 00\tBatch 2010\ttest_loss = 3.7552\n",
      "Epoch 00\tBatch 2020\t0.1489s/batch\ttrain_loss = 3.3196\n",
      "Epoch 00\tBatch 2030\t0.1481s/batch\ttrain_loss = 3.8967\n",
      "Epoch 00\tBatch 2040\t0.1509s/batch\ttrain_loss = 4.5328\n",
      "Epoch 00\tBatch 2040\ttest_loss = 3.7532\n",
      "Epoch 00\tBatch 2050\t0.1480s/batch\ttrain_loss = 3.5294\n",
      "Epoch 00\tBatch 2060\t0.1486s/batch\ttrain_loss = 3.2286\n",
      "Epoch 00\tBatch 2070\t0.1553s/batch\ttrain_loss = 2.6590\n",
      "Epoch 00\tBatch 2070\ttest_loss = 3.7647\n",
      "Epoch 00\tBatch 2080\t0.1464s/batch\ttrain_loss = 3.8216\n",
      "Epoch 00\tBatch 2090\t0.1454s/batch\ttrain_loss = 3.3789\n",
      "Epoch 00\tBatch 2100\t0.1458s/batch\ttrain_loss = 3.4634\n",
      "Epoch 00\tBatch 2100\ttest_loss = 3.7212\n",
      "Epoch 00\tBatch 2110\t0.1487s/batch\ttrain_loss = 4.1760\n",
      "Epoch 00\tBatch 2120\t0.1524s/batch\ttrain_loss = 4.1735\n",
      "Epoch 00\tBatch 2130\t0.1474s/batch\ttrain_loss = 4.0586\n",
      "Epoch 00\tBatch 2130\ttest_loss = 3.7041\n",
      "new best loss (3.720891 -> 3.704056)\n",
      "Epoch 00\tBatch 2140\t0.1485s/batch\ttrain_loss = 4.0321\n",
      "Epoch 00\tBatch 2150\t0.1478s/batch\ttrain_loss = 3.9733\n",
      "Epoch 00\tBatch 2160\t0.1483s/batch\ttrain_loss = 4.0111\n",
      "Epoch 00\tBatch 2160\ttest_loss = 3.9668\n",
      "Epoch 00\tBatch 2170\t0.1465s/batch\ttrain_loss = 3.9582\n",
      "Epoch 00\tBatch 2180\t0.1478s/batch\ttrain_loss = 4.0172\n",
      "Epoch 00\tBatch 2190\t0.1462s/batch\ttrain_loss = 3.3721\n",
      "Epoch 00\tBatch 2190\ttest_loss = 3.7791\n",
      "Epoch 00\tBatch 2200\t0.1500s/batch\ttrain_loss = 5.2389\n",
      "Epoch 00\tBatch 2210\t0.1487s/batch\ttrain_loss = 4.3313\n",
      "Epoch 00\tBatch 2220\t0.1466s/batch\ttrain_loss = 3.4847\n",
      "Epoch 00\tBatch 2220\ttest_loss = 3.7604\n",
      "Epoch 00\tBatch 2230\t0.1475s/batch\ttrain_loss = 3.7463\n",
      "Epoch 00\tBatch 2240\t0.1483s/batch\ttrain_loss = 3.6459\n",
      "Epoch 00\tBatch 2250\t0.1489s/batch\ttrain_loss = 3.9817\n",
      "Epoch 00\tBatch 2250\ttest_loss = 3.7458\n",
      "Epoch 00\tBatch 2260\t0.1496s/batch\ttrain_loss = 4.0625\n",
      "Epoch 00\tBatch 2270\t0.1483s/batch\ttrain_loss = 3.7325\n",
      "Epoch 00\tBatch 2280\t0.1469s/batch\ttrain_loss = 3.6748\n",
      "Epoch 00\tBatch 2280\ttest_loss = 3.7529\n",
      "Epoch 00\tBatch 2290\t0.1488s/batch\ttrain_loss = 3.9524\n",
      "Epoch 00\tBatch 2300\t0.1492s/batch\ttrain_loss = 4.4554\n",
      "Epoch 00\tBatch 2310\t0.1481s/batch\ttrain_loss = 4.0488\n",
      "Epoch 00\tBatch 2310\ttest_loss = 3.6572\n",
      "new best loss (3.704056 -> 3.657158)\n",
      "Epoch 00\tBatch 2320\t0.1481s/batch\ttrain_loss = 4.2222\n",
      "Epoch 00\tBatch 2330\t0.1470s/batch\ttrain_loss = 3.5231\n",
      "Epoch 00\tBatch 2340\t0.1490s/batch\ttrain_loss = 4.4314\n",
      "Epoch 00\tBatch 2340\ttest_loss = 3.6762\n",
      "Epoch 00\tBatch 2350\t0.1509s/batch\ttrain_loss = 3.3563\n",
      "Epoch 00\tBatch 2360\t0.1473s/batch\ttrain_loss = 3.6864\n",
      "Epoch 00\tBatch 2370\t0.1457s/batch\ttrain_loss = 3.1449\n",
      "Epoch 00\tBatch 2370\ttest_loss = 3.6670\n",
      "Epoch 00\tBatch 2380\t0.1574s/batch\ttrain_loss = 3.7764\n",
      "Epoch 00\tBatch 2390\t0.1503s/batch\ttrain_loss = 3.2562\n",
      "Epoch 00\tBatch 2400\t0.1485s/batch\ttrain_loss = 3.5502\n",
      "Epoch 00\tBatch 2400\ttest_loss = 3.7667\n",
      "Epoch 00\tBatch 2410\t0.1470s/batch\ttrain_loss = 4.1358\n",
      "Epoch 00\tBatch 2420\t0.1474s/batch\ttrain_loss = 3.6503\n",
      "Epoch 00\tBatch 2430\t0.1467s/batch\ttrain_loss = 3.5083\n",
      "Epoch 00\tBatch 2430\ttest_loss = 3.6981\n",
      "Epoch 00\tBatch 2440\t0.1478s/batch\ttrain_loss = 3.7106\n",
      "Epoch 00\tBatch 2450\t0.1465s/batch\ttrain_loss = 3.2727\n",
      "Epoch 00\tBatch 2460\t0.1466s/batch\ttrain_loss = 3.1548\n",
      "Epoch 00\tBatch 2460\ttest_loss = 3.6133\n",
      "new best loss (3.657158 -> 3.613344)\n",
      "Epoch 00\tBatch 2470\t0.1457s/batch\ttrain_loss = 3.4939\n",
      "Epoch 00\tBatch 2480\t0.1430s/batch\ttrain_loss = 3.1135\n",
      "Epoch 00\tBatch 2490\t0.1442s/batch\ttrain_loss = 3.3587\n",
      "Epoch 00\tBatch 2490\ttest_loss = 3.6532\n",
      "Epoch 00\tBatch 2500\t0.1452s/batch\ttrain_loss = 3.9069\n",
      "Epoch 00\tBatch 2510\t0.1448s/batch\ttrain_loss = 3.6659\n",
      "Epoch 00\tBatch 2520\t0.1479s/batch\ttrain_loss = 4.9318\n",
      "Epoch 00\tBatch 2520\ttest_loss = 3.6187\n",
      "Epoch 00\tBatch 2530\t0.1432s/batch\ttrain_loss = 2.8171\n",
      "Epoch 00\tBatch 2540\t0.1437s/batch\ttrain_loss = 3.2466\n",
      "Epoch 00\tBatch 2550\t0.1441s/batch\ttrain_loss = 3.6376\n",
      "Epoch 00\tBatch 2550\ttest_loss = 3.6325\n",
      "Epoch 00\tBatch 2560\t0.1451s/batch\ttrain_loss = 4.0180\n",
      "Epoch 00\tBatch 2570\t0.1466s/batch\ttrain_loss = 4.5621\n",
      "Epoch 00\tBatch 2580\t0.1436s/batch\ttrain_loss = 3.4615\n",
      "Epoch 00\tBatch 2580\ttest_loss = 3.5984\n",
      "new best loss (3.613344 -> 3.598433)\n",
      "Epoch 00\tBatch 2590\t0.1514s/batch\ttrain_loss = 3.5986\n",
      "Epoch 00\tBatch 2600\t0.1481s/batch\ttrain_loss = 4.7168\n",
      "Epoch 00\tBatch 2610\t0.1477s/batch\ttrain_loss = 3.7388\n",
      "Epoch 00\tBatch 2610\ttest_loss = 3.6213\n",
      "Epoch 00\tBatch 2620\t0.1456s/batch\ttrain_loss = 3.7025\n",
      "Epoch 00\tBatch 2630\t0.1456s/batch\ttrain_loss = 3.9769\n",
      "Epoch 00\tBatch 2640\t0.1464s/batch\ttrain_loss = 3.6605\n",
      "Epoch 00\tBatch 2640\ttest_loss = 3.6330\n",
      "Epoch 00\tBatch 2650\t0.1454s/batch\ttrain_loss = 3.5966\n",
      "Epoch 00\tBatch 2660\t0.1589s/batch\ttrain_loss = 3.3778\n",
      "Epoch 00\tBatch 2670\t0.1464s/batch\ttrain_loss = 4.4598\n",
      "Epoch 00\tBatch 2670\ttest_loss = 3.6379\n",
      "Epoch 00\tBatch 2680\t0.1442s/batch\ttrain_loss = 3.2184\n",
      "Epoch 00\tBatch 2690\t0.1455s/batch\ttrain_loss = 3.6948\n",
      "Epoch 00\tBatch 2700\t0.1449s/batch\ttrain_loss = 3.7751\n",
      "Epoch 00\tBatch 2700\ttest_loss = 3.6609\n",
      "Epoch 00\tBatch 2710\t0.1454s/batch\ttrain_loss = 3.3930\n",
      "Epoch 00\tBatch 2720\t0.1463s/batch\ttrain_loss = 3.6614\n",
      "Epoch 00\tBatch 2730\t0.1474s/batch\ttrain_loss = 4.3439\n",
      "Epoch 00\tBatch 2730\ttest_loss = 3.5830\n",
      "new best loss (3.598433 -> 3.582980)\n",
      "Epoch 00\tBatch 2740\t0.1484s/batch\ttrain_loss = 2.8410\n",
      "Epoch 00\tBatch 2750\t0.1449s/batch\ttrain_loss = 3.3524\n",
      "Epoch 00\tBatch 2760\t0.1437s/batch\ttrain_loss = 3.2263\n",
      "Epoch 00\tBatch 2760\ttest_loss = 3.5860\n",
      "Epoch 00\tBatch 2770\t0.1450s/batch\ttrain_loss = 3.7154\n",
      "Epoch 00\tBatch 2780\t0.1449s/batch\ttrain_loss = 3.5179\n",
      "Epoch 00\tBatch 2790\t0.1462s/batch\ttrain_loss = 4.5819\n",
      "Epoch 00\tBatch 2790\ttest_loss = 3.6062\n",
      "Epoch 00\tBatch 2800\t0.1478s/batch\ttrain_loss = 3.8058\n",
      "Epoch 00\tBatch 2810\t0.1469s/batch\ttrain_loss = 3.9371\n",
      "Epoch 00\tBatch 2820\t0.1455s/batch\ttrain_loss = 3.5962\n",
      "Epoch 00\tBatch 2820\ttest_loss = 3.5844\n",
      "Epoch 00\tBatch 2830\t0.1435s/batch\ttrain_loss = 2.8676\n",
      "Epoch 00\tBatch 2840\t0.1433s/batch\ttrain_loss = 3.3921\n",
      "Epoch 00\tBatch 2850\t0.1451s/batch\ttrain_loss = 3.5175\n",
      "Epoch 00\tBatch 2850\ttest_loss = 3.6126\n",
      "Epoch 00\tBatch 2860\t0.1460s/batch\ttrain_loss = 3.7489\n",
      "Epoch 00\tBatch 2870\t0.1447s/batch\ttrain_loss = 3.5086\n",
      "Epoch 00\tBatch 2880\t0.1456s/batch\ttrain_loss = 4.1422\n",
      "Epoch 00\tBatch 2880\ttest_loss = 3.6441\n",
      "Epoch 00\tBatch 2890\t0.1456s/batch\ttrain_loss = 3.6385\n",
      "Epoch 00\tBatch 2900\t0.1451s/batch\ttrain_loss = 3.8077\n",
      "Epoch 00\tBatch 2910\t0.1457s/batch\ttrain_loss = 3.7415\n",
      "Epoch 00\tBatch 2910\ttest_loss = 3.5578\n",
      "new best loss (3.582980 -> 3.557831)\n",
      "Epoch 00\tBatch 2920\t0.1444s/batch\ttrain_loss = 3.1877\n",
      "Epoch 00\tBatch 2930\t0.1430s/batch\ttrain_loss = 2.8073\n",
      "Epoch 00\tBatch 2940\t0.1450s/batch\ttrain_loss = 4.2235\n",
      "Epoch 00\tBatch 2940\ttest_loss = 3.5490\n",
      "new best loss (3.557831 -> 3.548955)\n",
      "Epoch 00\tBatch 2950\t0.1442s/batch\ttrain_loss = 3.6582\n",
      "Epoch 00\tBatch 2960\t0.1438s/batch\ttrain_loss = 3.2603\n",
      "Epoch 00\tBatch 2970\t0.1439s/batch\ttrain_loss = 3.0447\n",
      "Epoch 00\tBatch 2970\ttest_loss = 3.5557\n",
      "Epoch 00\tBatch 2980\t0.1450s/batch\ttrain_loss = 3.4904\n",
      "Epoch 00\tBatch 2990\t0.1426s/batch\ttrain_loss = 2.5102\n",
      "Epoch 00\tBatch 3000\t0.1442s/batch\ttrain_loss = 3.6116\n",
      "Epoch 00\tBatch 3000\ttest_loss = 3.5753\n",
      "Epoch 00\tBatch 3010\t0.1534s/batch\ttrain_loss = 3.8209\n",
      "Epoch 00\tBatch 3020\t0.1508s/batch\ttrain_loss = 3.2303\n",
      "Epoch 00\tBatch 3030\t0.1510s/batch\ttrain_loss = 3.1990\n",
      "Epoch 00\tBatch 3030\ttest_loss = 3.5548\n",
      "Epoch 00\tBatch 3040\t0.1479s/batch\ttrain_loss = 4.2475\n",
      "Epoch 00\tBatch 3050\t0.1493s/batch\ttrain_loss = 3.4474\n",
      "Epoch 00\tBatch 3060\t0.1456s/batch\ttrain_loss = 3.3842\n",
      "Epoch 00\tBatch 3060\ttest_loss = 3.5766\n",
      "Epoch 00\tBatch 3070\t0.1457s/batch\ttrain_loss = 3.7073\n",
      "Epoch 00\tBatch 3080\t0.1467s/batch\ttrain_loss = 4.2695\n",
      "Epoch 00\tBatch 3090\t0.1459s/batch\ttrain_loss = 4.0417\n",
      "Epoch 00\tBatch 3090\ttest_loss = 3.5474\n",
      "new best loss (3.548955 -> 3.547427)\n",
      "Epoch 00\tBatch 3100\t0.1456s/batch\ttrain_loss = 3.2571\n",
      "Epoch 00\tBatch 3110\t0.1448s/batch\ttrain_loss = 3.3090\n",
      "Epoch 00\tBatch 3120\t0.1439s/batch\ttrain_loss = 2.9297\n",
      "Epoch 00\tBatch 3120\ttest_loss = 3.5444\n",
      "new best loss (3.547427 -> 3.544396)\n",
      "Epoch 00\tBatch 3130\t0.1457s/batch\ttrain_loss = 3.4996\n",
      "Epoch 00\tBatch 3140\t0.1442s/batch\ttrain_loss = 3.1213\n",
      "Epoch 00\tBatch 3150\t0.1456s/batch\ttrain_loss = 3.9006\n",
      "Epoch 00\tBatch 3150\ttest_loss = 3.5730\n",
      "Epoch 00\tBatch 3160\t0.1445s/batch\ttrain_loss = 3.3724\n",
      "Epoch 00\tBatch 3170\t0.1432s/batch\ttrain_loss = 2.6357\n",
      "Epoch 00\tBatch 3180\t0.1454s/batch\ttrain_loss = 3.7605\n",
      "Epoch 00\tBatch 3180\ttest_loss = 3.5295\n",
      "new best loss (3.544396 -> 3.529542)\n",
      "Epoch 00\tBatch 3190\t0.1480s/batch\ttrain_loss = 2.9155\n",
      "Epoch 00\tBatch 3200\t0.1466s/batch\ttrain_loss = 3.8023\n",
      "Epoch 00\tBatch 3210\t0.1436s/batch\ttrain_loss = 2.7403\n",
      "Epoch 00\tBatch 3210\ttest_loss = 3.5034\n",
      "new best loss (3.529542 -> 3.503434)\n",
      "Epoch 00\tBatch 3220\t0.1451s/batch\ttrain_loss = 3.4252\n",
      "Epoch 00\tBatch 3230\t0.1452s/batch\ttrain_loss = 3.3896\n",
      "Epoch 00\tBatch 3240\t0.1443s/batch\ttrain_loss = 3.1578\n",
      "Epoch 00\tBatch 3240\ttest_loss = 3.4382\n",
      "new best loss (3.503434 -> 3.438168)\n",
      "Epoch 00\tBatch 3250\t0.1569s/batch\ttrain_loss = 3.7122\n",
      "Epoch 00\tBatch 3260\t0.1459s/batch\ttrain_loss = 3.5942\n",
      "Epoch 00\tBatch 3270\t0.1467s/batch\ttrain_loss = 3.9276\n",
      "Epoch 00\tBatch 3270\ttest_loss = 3.5345\n",
      "Epoch 00\tBatch 3280\t0.1455s/batch\ttrain_loss = 3.6698\n",
      "Epoch 00\tBatch 3290\t0.1448s/batch\ttrain_loss = 3.5532\n",
      "Epoch 00\tBatch 3300\t0.1448s/batch\ttrain_loss = 3.4527\n",
      "Epoch 00\tBatch 3300\ttest_loss = 3.5529\n",
      "Epoch 00\tBatch 3310\t0.1446s/batch\ttrain_loss = 3.4741\n",
      "Epoch 00\tBatch 3320\t0.1435s/batch\ttrain_loss = 2.8714\n",
      "Epoch 00\tBatch 3330\t0.1457s/batch\ttrain_loss = 3.5454\n",
      "Epoch 00\tBatch 3330\ttest_loss = 3.6097\n",
      "Epoch 00\tBatch 3340\t0.1453s/batch\ttrain_loss = 3.4174\n",
      "Epoch 00\tBatch 3350\t0.1458s/batch\ttrain_loss = 4.1468\n",
      "Epoch 00\tBatch 3360\t0.1435s/batch\ttrain_loss = 2.5408\n",
      "Epoch 00\tBatch 3360\ttest_loss = 3.5274\n",
      "Epoch 00\tBatch 3370\t0.1457s/batch\ttrain_loss = 3.6965\n",
      "Epoch 00\tBatch 3380\t0.1440s/batch\ttrain_loss = 3.3279\n",
      "Epoch 00\tBatch 3390\t0.1456s/batch\ttrain_loss = 3.1972\n",
      "Epoch 00\tBatch 3390\ttest_loss = 3.4695\n",
      "Epoch 00\tBatch 3400\t0.1435s/batch\ttrain_loss = 2.9986\n",
      "Epoch 00\tBatch 3410\t0.1440s/batch\ttrain_loss = 3.0703\n",
      "Epoch 00\tBatch 3420\t0.1455s/batch\ttrain_loss = 3.2709\n",
      "Epoch 00\tBatch 3420\ttest_loss = 3.5177\n",
      "Epoch 00\tBatch 3430\t0.1442s/batch\ttrain_loss = 3.0273\n",
      "Epoch 00\tBatch 3440\t0.1443s/batch\ttrain_loss = 3.2758\n",
      "Epoch 00\tBatch 3450\t0.1453s/batch\ttrain_loss = 3.5044\n",
      "Epoch 00\tBatch 3450\ttest_loss = 3.3997\n",
      "new best loss (3.438168 -> 3.399723)\n",
      "Epoch 00\tBatch 3460\t0.1459s/batch\ttrain_loss = 3.5798\n",
      "Epoch 00\tBatch 3470\t0.1455s/batch\ttrain_loss = 3.6288\n",
      "Epoch 00\tBatch 3480\t0.1455s/batch\ttrain_loss = 3.3677\n",
      "Epoch 00\tBatch 3480\ttest_loss = 3.4681\n",
      "Epoch 00\tBatch 3490\t0.1441s/batch\ttrain_loss = 2.9717\n",
      "Epoch 00\tBatch 3500\t0.1446s/batch\ttrain_loss = 3.2342\n",
      "Epoch 00\tBatch 3510\t0.1435s/batch\ttrain_loss = 2.9718\n",
      "Epoch 00\tBatch 3510\ttest_loss = 3.4514\n",
      "Epoch 00\tBatch 3520\t0.1458s/batch\ttrain_loss = 3.7850\n",
      "Epoch 00\tBatch 3530\t0.1460s/batch\ttrain_loss = 3.3789\n",
      "Epoch 00\tBatch 3540\t0.1452s/batch\ttrain_loss = 3.5374\n",
      "Epoch 00\tBatch 3540\ttest_loss = 3.5095\n",
      "Epoch 00\tBatch 3550\t0.1446s/batch\ttrain_loss = 3.4339\n",
      "Epoch 00\tBatch 3560\t0.1444s/batch\ttrain_loss = 3.2820\n",
      "Epoch 00\tBatch 3570\t0.1484s/batch\ttrain_loss = 4.2981\n",
      "Epoch 00\tBatch 3570\ttest_loss = 3.5085\n",
      "Epoch 00\tBatch 3580\t0.1467s/batch\ttrain_loss = 4.1316\n",
      "Epoch 00\tBatch 3590\t0.1449s/batch\ttrain_loss = 3.2945\n",
      "Epoch 00\tBatch 3600\t0.1462s/batch\ttrain_loss = 3.5609\n",
      "Epoch 00\tBatch 3600\ttest_loss = 3.4001\n",
      "Epoch 00\tBatch 3610\t0.1467s/batch\ttrain_loss = 3.6631\n",
      "Epoch 00\tBatch 3620\t0.1449s/batch\ttrain_loss = 3.1528\n",
      "Epoch 00\tBatch 3630\t0.1449s/batch\ttrain_loss = 3.3636\n",
      "Epoch 00\tBatch 3630\ttest_loss = 3.4140\n",
      "Epoch 00\tBatch 3640\t0.1465s/batch\ttrain_loss = 3.8843\n",
      "Epoch 00\tBatch 3650\t0.1436s/batch\ttrain_loss = 2.8157\n",
      "Epoch 00\tBatch 3660\t0.1455s/batch\ttrain_loss = 3.4582\n",
      "Epoch 00\tBatch 3660\ttest_loss = 3.4211\n",
      "Epoch 00\tBatch 3670\t0.1454s/batch\ttrain_loss = 3.7716\n",
      "Epoch 00\tBatch 3680\t0.1455s/batch\ttrain_loss = 3.2517\n",
      "Epoch 00\tBatch 3690\t0.1471s/batch\ttrain_loss = 4.2294\n",
      "Epoch 00\tBatch 3690\ttest_loss = 3.4570\n",
      "Epoch 00\tBatch 3700\t0.1453s/batch\ttrain_loss = 3.6056\n",
      "Epoch 00\tBatch 3710\t0.1457s/batch\ttrain_loss = 3.5999\n",
      "Epoch 00\tBatch 3720\t0.1445s/batch\ttrain_loss = 3.2622\n",
      "Epoch 00\tBatch 3720\ttest_loss = 3.3756\n",
      "new best loss (3.399723 -> 3.375592)\n",
      "Epoch 00\tBatch 3730\t0.1449s/batch\ttrain_loss = 3.2829\n",
      "Epoch 00\tBatch 3740\t0.1437s/batch\ttrain_loss = 2.8167\n",
      "Epoch 00\tBatch 3750\t0.1452s/batch\ttrain_loss = 3.2171\n",
      "Epoch 00\tBatch 3750\ttest_loss = 3.3988\n",
      "Epoch 00\tBatch 3760\t0.1450s/batch\ttrain_loss = 3.4706\n",
      "Epoch 00\tBatch 3770\t0.1464s/batch\ttrain_loss = 3.8929\n",
      "Epoch 00\tBatch 3780\t0.1449s/batch\ttrain_loss = 3.2302\n",
      "Epoch 00\tBatch 3780\ttest_loss = 3.3453\n",
      "new best loss (3.375592 -> 3.345306)\n",
      "Epoch 00\tBatch 3790\t0.1462s/batch\ttrain_loss = 3.8023\n",
      "Epoch 00\tBatch 3800\t0.1469s/batch\ttrain_loss = 3.9929\n",
      "Epoch 00\tBatch 3810\t0.1455s/batch\ttrain_loss = 3.8185\n",
      "Epoch 00\tBatch 3810\ttest_loss = 3.4381\n",
      "Epoch 00\tBatch 3820\t0.1443s/batch\ttrain_loss = 2.8510\n",
      "Epoch 00\tBatch 3830\t0.1470s/batch\ttrain_loss = 3.8736\n",
      "Epoch 00\tBatch 3840\t0.1462s/batch\ttrain_loss = 3.7164\n",
      "Epoch 00\tBatch 3840\ttest_loss = 3.3661\n",
      "Epoch 00\tBatch 3850\t0.1472s/batch\ttrain_loss = 3.8337\n",
      "Epoch 00\tBatch 3860\t0.1450s/batch\ttrain_loss = 3.4696\n",
      "Epoch 00\tBatch 3870\t0.1454s/batch\ttrain_loss = 3.4199\n",
      "Epoch 00\tBatch 3870\ttest_loss = 3.3718\n",
      "Epoch 00\tBatch 3880\t0.1472s/batch\ttrain_loss = 3.8048\n",
      "Epoch 00\tBatch 3890\t0.1459s/batch\ttrain_loss = 3.6588\n",
      "Epoch 00\tBatch 3900\t0.1452s/batch\ttrain_loss = 3.6714\n",
      "Epoch 00\tBatch 3900\ttest_loss = 3.3484\n",
      "Epoch 00\tBatch 3910\t0.1462s/batch\ttrain_loss = 3.7371\n",
      "Epoch 00\tBatch 3920\t0.1460s/batch\ttrain_loss = 3.8874\n",
      "Epoch 00\tBatch 3930\t0.1462s/batch\ttrain_loss = 3.5045\n",
      "Epoch 00\tBatch 3930\ttest_loss = 3.4117\n",
      "Epoch 00\tBatch 3940\t0.1463s/batch\ttrain_loss = 3.9813\n",
      "Epoch 00\tBatch 3950\t0.1481s/batch\ttrain_loss = 4.2376\n",
      "Epoch 00\tBatch 3960\t0.1464s/batch\ttrain_loss = 3.8575\n",
      "Epoch 00\tBatch 3960\ttest_loss = 3.4199\n",
      "Epoch 00\tBatch 3970\t0.1453s/batch\ttrain_loss = 3.2262\n",
      "Epoch 00\tBatch 3980\t0.1453s/batch\ttrain_loss = 3.4443\n",
      "Epoch 00\tBatch 3990\t0.1435s/batch\ttrain_loss = 2.6126\n",
      "Epoch 00\tBatch 3990\ttest_loss = 3.3399\n",
      "new best loss (3.345306 -> 3.339852)\n",
      "Epoch 00\tBatch 4000\t0.1453s/batch\ttrain_loss = 3.2566\n",
      "Epoch 00\tBatch 4010\t0.1439s/batch\ttrain_loss = 3.0246\n",
      "Epoch 00\tBatch 4020\t0.1446s/batch\ttrain_loss = 3.2567\n",
      "Epoch 00\tBatch 4020\ttest_loss = 3.3816\n",
      "Epoch 00\tBatch 4030\t0.1453s/batch\ttrain_loss = 3.1513\n",
      "Epoch 00\tBatch 4040\t0.1464s/batch\ttrain_loss = 3.6140\n",
      "Epoch 00\tBatch 4050\t0.1455s/batch\ttrain_loss = 3.4421\n",
      "Epoch 00\tBatch 4050\ttest_loss = 3.3557\n",
      "Epoch 00\tBatch 4060\t0.1433s/batch\ttrain_loss = 2.6270\n",
      "Epoch 00\tBatch 4070\t0.1455s/batch\ttrain_loss = 3.1548\n",
      "Epoch 00\tBatch 4080\t0.1468s/batch\ttrain_loss = 3.7389\n",
      "Epoch 00\tBatch 4080\ttest_loss = 3.3653\n",
      "Epoch 00\tBatch 4090\t0.1454s/batch\ttrain_loss = 3.4236\n",
      "Epoch 00\tBatch 4100\t0.1468s/batch\ttrain_loss = 3.7654\n",
      "Epoch 00\tBatch 4110\t0.1458s/batch\ttrain_loss = 3.2530\n",
      "Epoch 00\tBatch 4110\ttest_loss = 3.3759\n",
      "Epoch 00\tBatch 4120\t0.1532s/batch\ttrain_loss = 3.5204\n",
      "Epoch 00\tBatch 4130\t0.1464s/batch\ttrain_loss = 3.7068\n",
      "Epoch 00\tBatch 4140\t0.1466s/batch\ttrain_loss = 3.6273\n",
      "Epoch 00\tBatch 4140\ttest_loss = 3.3520\n",
      "Epoch 00\tBatch 4150\t0.1459s/batch\ttrain_loss = 3.2873\n",
      "Epoch 00\tBatch 4160\t0.1447s/batch\ttrain_loss = 3.2223\n",
      "Epoch 00\tBatch 4170\t0.1451s/batch\ttrain_loss = 3.1279\n",
      "Epoch 00\tBatch 4170\ttest_loss = 3.3002\n",
      "new best loss (3.339852 -> 3.300167)\n",
      "Epoch 00\tBatch 4180\t0.1583s/batch\ttrain_loss = 4.2611\n",
      "Epoch 00\tBatch 4190\t0.1463s/batch\ttrain_loss = 3.7198\n",
      "Epoch 00\tBatch 4200\t0.1497s/batch\ttrain_loss = 4.0193\n",
      "Epoch 00\tBatch 4200\ttest_loss = 3.3375\n",
      "Epoch 00\tBatch 4210\t0.1464s/batch\ttrain_loss = 4.2952\n",
      "Epoch 00\tBatch 4220\t0.1446s/batch\ttrain_loss = 3.1794\n",
      "Epoch 00\tBatch 4230\t0.1454s/batch\ttrain_loss = 3.2179\n",
      "Epoch 00\tBatch 4230\ttest_loss = 3.2984\n",
      "new best loss (3.300167 -> 3.298422)\n",
      "Epoch 00\tBatch 4240\t0.1479s/batch\ttrain_loss = 3.2713\n",
      "Epoch 00\tBatch 4250\t0.1522s/batch\ttrain_loss = 3.4879\n",
      "Epoch 00\tBatch 4260\t0.1575s/batch\ttrain_loss = 4.0520\n",
      "Epoch 00\tBatch 4260\ttest_loss = 3.3410\n",
      "Epoch 00\tBatch 4270\t0.1482s/batch\ttrain_loss = 3.7322\n",
      "Epoch 00\tBatch 4280\t0.1482s/batch\ttrain_loss = 3.5797\n",
      "Epoch 00\tBatch 4290\t0.1481s/batch\ttrain_loss = 3.3191\n",
      "Epoch 00\tBatch 4290\ttest_loss = 3.2986\n",
      "Epoch 00\tBatch 4300\t0.1465s/batch\ttrain_loss = 3.3437\n",
      "Epoch 00\tBatch 4310\t0.1446s/batch\ttrain_loss = 2.8487\n",
      "Epoch 00\tBatch 4320\t0.1447s/batch\ttrain_loss = 2.5710\n",
      "Epoch 00\tBatch 4320\ttest_loss = 3.2868\n",
      "new best loss (3.298422 -> 3.286819)\n",
      "Epoch 00\tBatch 4330\t0.1444s/batch\ttrain_loss = 2.5998\n",
      "Epoch 00\tBatch 4340\t0.1455s/batch\ttrain_loss = 3.2703\n",
      "Epoch 00\tBatch 4350\t0.1448s/batch\ttrain_loss = 3.5110\n",
      "Epoch 00\tBatch 4350\ttest_loss = 3.3053\n",
      "Epoch 00\tBatch 4360\t0.1451s/batch\ttrain_loss = 3.4696\n",
      "Epoch 00\tBatch 4370\t0.1454s/batch\ttrain_loss = 3.0883\n",
      "Epoch 00\tBatch 4380\t0.1451s/batch\ttrain_loss = 3.8410\n",
      "Epoch 00\tBatch 4380\ttest_loss = 3.3966\n",
      "Epoch 00\tBatch 4390\t0.1456s/batch\ttrain_loss = 3.4334\n",
      "Epoch 00\tBatch 4400\t0.1454s/batch\ttrain_loss = 3.1143\n",
      "Epoch 00\tBatch 4410\t0.1479s/batch\ttrain_loss = 3.7862\n",
      "Epoch 00\tBatch 4410\ttest_loss = 3.3634\n",
      "Epoch 00\tBatch 4420\t0.1451s/batch\ttrain_loss = 2.9860\n",
      "Epoch 00\tBatch 4430\t0.1431s/batch\ttrain_loss = 2.5111\n",
      "Epoch 00\tBatch 4440\t0.1464s/batch\ttrain_loss = 3.6853\n",
      "Epoch 00\tBatch 4440\ttest_loss = 3.2843\n",
      "new best loss (3.286819 -> 3.284297)\n",
      "Epoch 00\tBatch 4450\t0.1459s/batch\ttrain_loss = 2.8543\n",
      "Epoch 00\tBatch 4460\t0.1469s/batch\ttrain_loss = 3.2016\n",
      "Epoch 00\tBatch 4470\t0.1455s/batch\ttrain_loss = 3.6748\n",
      "Epoch 00\tBatch 4470\ttest_loss = 3.2746\n",
      "new best loss (3.284297 -> 3.274612)\n",
      "Epoch 00\tBatch 4480\t0.1443s/batch\ttrain_loss = 3.1924\n",
      "Epoch 00\tBatch 4490\t0.1452s/batch\ttrain_loss = 2.8216\n",
      "Epoch 00\tBatch 4500\t0.1468s/batch\ttrain_loss = 3.5051\n",
      "Epoch 00\tBatch 4500\ttest_loss = 3.2584\n",
      "new best loss (3.274612 -> 3.258401)\n",
      "Epoch 00\tBatch 4510\t0.1468s/batch\ttrain_loss = 3.6587\n",
      "Epoch 00\tBatch 4520\t0.1450s/batch\ttrain_loss = 2.9133\n",
      "Epoch 00\tBatch 4530\t0.1472s/batch\ttrain_loss = 3.9078\n",
      "Epoch 00\tBatch 4530\ttest_loss = 3.3057\n",
      "Epoch 00\tBatch 4540\t0.1462s/batch\ttrain_loss = 3.4406\n",
      "Epoch 00\tBatch 4550\t0.1472s/batch\ttrain_loss = 3.7172\n",
      "Epoch 00\tBatch 4560\t0.1450s/batch\ttrain_loss = 2.9339\n",
      "Epoch 00\tBatch 4560\ttest_loss = 3.2424\n",
      "new best loss (3.258401 -> 3.242409)\n",
      "Epoch 00\tBatch 4570\t0.1490s/batch\ttrain_loss = 4.0910\n",
      "Epoch 00\tBatch 4580\t0.1480s/batch\ttrain_loss = 3.0761\n",
      "Epoch 00\tBatch 4590\t0.1459s/batch\ttrain_loss = 3.5933\n",
      "Epoch 00\tBatch 4590\ttest_loss = 3.2806\n",
      "Epoch 00\tBatch 4600\t0.1478s/batch\ttrain_loss = 4.3353\n",
      "Epoch 00\tBatch 4610\t0.1449s/batch\ttrain_loss = 2.8547\n",
      "Epoch 00\tBatch 4620\t0.1466s/batch\ttrain_loss = 3.3598\n",
      "Epoch 00\tBatch 4620\ttest_loss = 3.2557\n",
      "Epoch 00\tBatch 4630\t0.1488s/batch\ttrain_loss = 4.2152\n",
      "Epoch 00\tBatch 4640\t0.1450s/batch\ttrain_loss = 3.7218\n",
      "Epoch 00\tBatch 4650\t0.1462s/batch\ttrain_loss = 3.8199\n",
      "Epoch 00\tBatch 4650\ttest_loss = 3.2469\n",
      "Epoch 00\tBatch 4660\t0.1447s/batch\ttrain_loss = 3.2063\n",
      "Epoch 00\tBatch 4670\t0.1471s/batch\ttrain_loss = 3.4442\n",
      "Epoch 00\tBatch 4680\t0.1458s/batch\ttrain_loss = 3.2659\n",
      "Epoch 00\tBatch 4680\ttest_loss = 3.2731\n",
      "Epoch 00\tBatch 4690\t0.1457s/batch\ttrain_loss = 2.8383\n",
      "Epoch 00\tBatch 4700\t0.1461s/batch\ttrain_loss = 3.1543\n",
      "Epoch 00\tBatch 4710\t0.1455s/batch\ttrain_loss = 3.1867\n",
      "Epoch 00\tBatch 4710\ttest_loss = 3.2758\n",
      "Epoch 00\tBatch 4720\t0.1465s/batch\ttrain_loss = 3.2263\n",
      "Epoch 00\tBatch 4730\t0.1458s/batch\ttrain_loss = 2.9209\n",
      "Epoch 00\tBatch 4740\t0.1455s/batch\ttrain_loss = 3.0937\n",
      "Epoch 00\tBatch 4740\ttest_loss = 3.2233\n",
      "new best loss (3.242409 -> 3.223291)\n",
      "Epoch 00\tBatch 4750\t0.1466s/batch\ttrain_loss = 3.4136\n",
      "Epoch 00\tBatch 4760\t0.1471s/batch\ttrain_loss = 3.7845\n",
      "Epoch 00\tBatch 4770\t0.1465s/batch\ttrain_loss = 3.2754\n",
      "Epoch 00\tBatch 4770\ttest_loss = 3.3028\n",
      "Epoch 00\tBatch 4780\t0.1466s/batch\ttrain_loss = 3.0014\n",
      "Epoch 00\tBatch 4790\t0.1439s/batch\ttrain_loss = 2.4459\n",
      "Epoch 00\tBatch 4800\t0.1439s/batch\ttrain_loss = 2.6627\n",
      "Epoch 00\tBatch 4800\ttest_loss = 3.2501\n",
      "Epoch 00\tBatch 4810\t0.1441s/batch\ttrain_loss = 2.6130\n",
      "Epoch 00\tBatch 4820\t0.1449s/batch\ttrain_loss = 3.3357\n",
      "Epoch 00\tBatch 4830\t0.1453s/batch\ttrain_loss = 2.7435\n",
      "Epoch 00\tBatch 4830\ttest_loss = 3.3747\n",
      "Epoch 00\tBatch 4840\t0.1450s/batch\ttrain_loss = 2.7864\n",
      "Epoch 00\tBatch 4850\t0.1454s/batch\ttrain_loss = 3.0612\n",
      "Epoch 00\tBatch 4860\t0.1456s/batch\ttrain_loss = 3.3170\n",
      "Epoch 00\tBatch 4860\ttest_loss = 3.2574\n",
      "Epoch 00\tBatch 4870\t0.1450s/batch\ttrain_loss = 2.6065\n",
      "Epoch 00\tBatch 4880\t0.1476s/batch\ttrain_loss = 3.9257\n",
      "Epoch 00\tBatch 4890\t0.1470s/batch\ttrain_loss = 3.6173\n",
      "Epoch 00\tBatch 4890\ttest_loss = 3.2379\n",
      "Epoch 00\tBatch 4900\t0.1457s/batch\ttrain_loss = 3.2996\n",
      "Epoch 00\tBatch 4910\t0.1455s/batch\ttrain_loss = 2.7207\n",
      "Epoch 00\tBatch 4920\t0.1448s/batch\ttrain_loss = 2.5562\n",
      "Epoch 00\tBatch 4920\ttest_loss = 3.2390\n",
      "Epoch 00\tBatch 4930\t0.1451s/batch\ttrain_loss = 2.7522\n",
      "Epoch 00\tBatch 4940\t0.1458s/batch\ttrain_loss = 3.3003\n",
      "Epoch 00\tBatch 4950\t0.1460s/batch\ttrain_loss = 3.1097\n",
      "Epoch 00\tBatch 4950\ttest_loss = 3.2543\n",
      "Epoch 00\tBatch 4960\t0.1452s/batch\ttrain_loss = 2.9663\n",
      "Epoch 00\tBatch 4970\t0.1445s/batch\ttrain_loss = 2.8651\n",
      "Epoch 00\tBatch 4980\t0.1449s/batch\ttrain_loss = 2.9014\n",
      "Epoch 00\tBatch 4980\ttest_loss = 3.2012\n",
      "new best loss (3.223291 -> 3.201166)\n",
      "Epoch 00\tBatch 4990\t0.1458s/batch\ttrain_loss = 2.7554\n",
      "Epoch 00\tBatch 5000\t0.1468s/batch\ttrain_loss = 2.9543\n",
      "Epoch 00\tBatch 5010\t0.1444s/batch\ttrain_loss = 3.0192\n",
      "Epoch 00\tBatch 5010\ttest_loss = 3.2228\n",
      "Epoch 00\tBatch 5020\t0.1453s/batch\ttrain_loss = 3.2618\n",
      "Epoch 00\tBatch 5030\t0.1472s/batch\ttrain_loss = 3.6681\n",
      "Epoch 00\tBatch 5040\t0.1462s/batch\ttrain_loss = 3.7720\n",
      "Epoch 00\tBatch 5040\ttest_loss = 3.2247\n",
      "Epoch 00\tBatch 5050\t0.1460s/batch\ttrain_loss = 3.5064\n",
      "Epoch 00\tBatch 5060\t0.1456s/batch\ttrain_loss = 3.3855\n",
      "Epoch 00\tBatch 5070\t0.1461s/batch\ttrain_loss = 3.7593\n",
      "Epoch 00\tBatch 5070\ttest_loss = 3.2419\n",
      "Epoch 00\tBatch 5080\t0.1478s/batch\ttrain_loss = 4.1325\n",
      "Epoch 00\tBatch 5090\t0.1465s/batch\ttrain_loss = 3.9172\n",
      "Epoch 00\tBatch 5100\t0.1466s/batch\ttrain_loss = 3.7808\n",
      "Epoch 00\tBatch 5100\ttest_loss = 3.3088\n",
      "Epoch 00\tBatch 5110\t0.1458s/batch\ttrain_loss = 3.0109\n",
      "Epoch 00\tBatch 5120\t0.1444s/batch\ttrain_loss = 2.7727\n",
      "Epoch 00\tBatch 5130\t0.1474s/batch\ttrain_loss = 4.0670\n",
      "Epoch 00\tBatch 5130\ttest_loss = 3.1950\n",
      "new best loss (3.201166 -> 3.194989)\n",
      "Epoch 00\tBatch 5140\t0.1463s/batch\ttrain_loss = 3.2704\n",
      "Epoch 00\tBatch 5150\t0.1452s/batch\ttrain_loss = 2.6031\n",
      "Epoch 00\tBatch 5160\t0.1468s/batch\ttrain_loss = 3.0140\n",
      "Epoch 00\tBatch 5160\ttest_loss = 3.2080\n",
      "Epoch 00\tBatch 5170\t0.1452s/batch\ttrain_loss = 3.0279\n",
      "Epoch 00\tBatch 5180\t0.1450s/batch\ttrain_loss = 2.7991\n",
      "Epoch 00\tBatch 5190\t0.1447s/batch\ttrain_loss = 2.8857\n",
      "Epoch 00\tBatch 5190\ttest_loss = 3.1725\n",
      "new best loss (3.194989 -> 3.172477)\n",
      "Epoch 00\tBatch 5200\t0.1458s/batch\ttrain_loss = 3.2086\n",
      "Epoch 00\tBatch 5210\t0.1461s/batch\ttrain_loss = 3.3339\n",
      "Epoch 00\tBatch 5220\t0.1461s/batch\ttrain_loss = 3.5908\n",
      "Epoch 00\tBatch 5220\ttest_loss = 3.2447\n",
      "Epoch 00\tBatch 5230\t0.1453s/batch\ttrain_loss = 2.9944\n",
      "Epoch 00\tBatch 5240\t0.1459s/batch\ttrain_loss = 3.8968\n",
      "Epoch 00\tBatch 5250\t0.1467s/batch\ttrain_loss = 3.5782\n",
      "Epoch 00\tBatch 5250\ttest_loss = 3.2286\n",
      "Epoch 00\tBatch 5260\t0.1453s/batch\ttrain_loss = 2.7319\n",
      "Epoch 00\tBatch 5270\t0.1473s/batch\ttrain_loss = 3.3515\n",
      "Epoch 00\tBatch 5280\t0.1461s/batch\ttrain_loss = 3.5282\n",
      "Epoch 00\tBatch 5280\ttest_loss = 3.2485\n",
      "Epoch 00\tBatch 5290\t0.1462s/batch\ttrain_loss = 3.0034\n",
      "Epoch 00\tBatch 5300\t0.1469s/batch\ttrain_loss = 3.8314\n",
      "Epoch 00\tBatch 5310\t0.1466s/batch\ttrain_loss = 3.5333\n",
      "Epoch 00\tBatch 5310\ttest_loss = 3.2068\n",
      "Epoch 00\tBatch 5320\t0.1452s/batch\ttrain_loss = 3.0545\n",
      "Epoch 00\tBatch 5330\t0.1466s/batch\ttrain_loss = 3.5409\n",
      "Epoch 00\tBatch 5340\t0.1442s/batch\ttrain_loss = 2.6883\n",
      "Epoch 00\tBatch 5340\ttest_loss = 3.1975\n",
      "Epoch 00\tBatch 5350\t0.1451s/batch\ttrain_loss = 3.1881\n",
      "Epoch 00\tBatch 5360\t0.1449s/batch\ttrain_loss = 3.0026\n",
      "Epoch 00\tBatch 5370\t0.1447s/batch\ttrain_loss = 3.0747\n",
      "Epoch 00\tBatch 5370\ttest_loss = 3.1798\n",
      "Epoch 00\tBatch 5380\t0.1464s/batch\ttrain_loss = 3.2645\n",
      "Epoch 00\tBatch 5390\t0.1460s/batch\ttrain_loss = 3.3832\n",
      "Epoch 00\tBatch 5400\t0.1469s/batch\ttrain_loss = 3.8094\n",
      "Epoch 00\tBatch 5400\ttest_loss = 3.2280\n",
      "Epoch 00\tBatch 5410\t0.1461s/batch\ttrain_loss = 3.4299\n",
      "Epoch 00\tBatch 5420\t0.1449s/batch\ttrain_loss = 2.3569\n",
      "Epoch 00\tBatch 5430\t0.1463s/batch\ttrain_loss = 3.2366\n",
      "Epoch 00\tBatch 5430\ttest_loss = 3.1788\n",
      "Epoch 00\tBatch 5440\t0.1459s/batch\ttrain_loss = 3.3597\n",
      "Epoch 00\tBatch 5450\t0.1449s/batch\ttrain_loss = 2.7300\n",
      "Epoch 00\tBatch 5460\t0.1458s/batch\ttrain_loss = 3.0864\n",
      "Epoch 00\tBatch 5460\ttest_loss = 3.1695\n",
      "new best loss (3.172477 -> 3.169512)\n",
      "Epoch 00\tBatch 5470\t0.1465s/batch\ttrain_loss = 3.2721\n",
      "Epoch 00\tBatch 5480\t0.1441s/batch\ttrain_loss = 2.6901\n",
      "Epoch 00\tBatch 5490\t0.1447s/batch\ttrain_loss = 2.8539\n",
      "Epoch 00\tBatch 5490\ttest_loss = 3.2741\n",
      "Epoch 00\tBatch 5500\t0.1460s/batch\ttrain_loss = 3.1146\n",
      "Epoch 00\tBatch 5510\t0.1445s/batch\ttrain_loss = 2.6819\n",
      "Epoch 00\tBatch 5520\t0.1458s/batch\ttrain_loss = 2.9658\n",
      "Epoch 00\tBatch 5520\ttest_loss = 3.1856\n",
      "Epoch 00\tBatch 5530\t0.1466s/batch\ttrain_loss = 3.7152\n",
      "Epoch 00\tBatch 5540\t0.1463s/batch\ttrain_loss = 3.6617\n",
      "Epoch 00\tBatch 5550\t0.1441s/batch\ttrain_loss = 3.1169\n",
      "Epoch 00\tBatch 5550\ttest_loss = 3.2478\n",
      "Epoch 00\tBatch 5560\t0.1434s/batch\ttrain_loss = 2.3786\n",
      "Epoch 00\tBatch 5570\t0.1459s/batch\ttrain_loss = 2.9530\n",
      "Epoch 00\tBatch 5580\t0.1464s/batch\ttrain_loss = 3.3816\n",
      "Epoch 00\tBatch 5580\ttest_loss = 3.1905\n",
      "Epoch 00\tBatch 5590\t0.1438s/batch\ttrain_loss = 2.7020\n",
      "Epoch 00\tBatch 5600\t0.1459s/batch\ttrain_loss = 3.5073\n",
      "Epoch 00\tBatch 5610\t0.1453s/batch\ttrain_loss = 2.9528\n",
      "Epoch 00\tBatch 5610\ttest_loss = 3.2309\n",
      "Epoch 00\tBatch 5620\t0.1462s/batch\ttrain_loss = 2.9388\n",
      "Epoch 00\tBatch 5630\t0.1439s/batch\ttrain_loss = 2.3497\n",
      "Epoch 00\tBatch 5640\t0.1454s/batch\ttrain_loss = 2.7585\n",
      "Epoch 00\tBatch 5640\ttest_loss = 3.1862\n",
      "Epoch 00\tBatch 5650\t0.1450s/batch\ttrain_loss = 2.8294\n",
      "Epoch 00\tBatch 5660\t0.1468s/batch\ttrain_loss = 3.2170\n",
      "Epoch 00\tBatch 5670\t0.1472s/batch\ttrain_loss = 3.4239\n",
      "Epoch 00\tBatch 5670\ttest_loss = 3.1994\n",
      "Epoch 00\tBatch 5680\t0.1439s/batch\ttrain_loss = 2.6086\n",
      "Epoch 00\tBatch 5690\t0.1477s/batch\ttrain_loss = 3.8592\n",
      "Epoch 00\tBatch 5700\t0.1481s/batch\ttrain_loss = 3.9856\n",
      "Epoch 00\tBatch 5700\ttest_loss = 3.1649\n",
      "new best loss (3.169512 -> 3.164903)\n",
      "Epoch 00\tBatch 5710\t0.1488s/batch\ttrain_loss = 3.9056\n",
      "Epoch 00\tBatch 5720\t0.1481s/batch\ttrain_loss = 4.1054\n",
      "Epoch 00\tBatch 5730\t0.1470s/batch\ttrain_loss = 3.5867\n",
      "Epoch 00\tBatch 5730\ttest_loss = 3.2031\n",
      "Epoch 00\tBatch 5740\t0.1467s/batch\ttrain_loss = 3.1030\n",
      "Epoch 00\tBatch 5750\t0.1448s/batch\ttrain_loss = 2.8806\n",
      "Epoch 00\tBatch 5760\t0.1458s/batch\ttrain_loss = 3.0769\n",
      "Epoch 00\tBatch 5760\ttest_loss = 3.1939\n",
      "Epoch 00\tBatch 5770\t0.1471s/batch\ttrain_loss = 3.9459\n",
      "Epoch 00\tBatch 5780\t0.1468s/batch\ttrain_loss = 3.7767\n",
      "Epoch 00\tBatch 5790\t0.1447s/batch\ttrain_loss = 2.7818\n",
      "Epoch 00\tBatch 5790\ttest_loss = 3.1925\n",
      "Epoch 00\tBatch 5800\t0.1471s/batch\ttrain_loss = 3.6737\n",
      "Epoch 00\tBatch 5810\t0.1444s/batch\ttrain_loss = 2.5891\n",
      "Epoch 00\tBatch 5820\t0.1460s/batch\ttrain_loss = 3.1596\n",
      "Epoch 00\tBatch 5820\ttest_loss = 3.1695\n",
      "Epoch 00\tBatch 5830\t0.1468s/batch\ttrain_loss = 3.5186\n",
      "Epoch 00\tBatch 5840\t0.1452s/batch\ttrain_loss = 3.5173\n",
      "Epoch 00\tBatch 5850\t0.1442s/batch\ttrain_loss = 2.6865\n",
      "Epoch 00\tBatch 5850\ttest_loss = 3.1503\n",
      "new best loss (3.164903 -> 3.150275)\n",
      "Epoch 00\tBatch 5860\t0.1446s/batch\ttrain_loss = 2.8440\n",
      "Epoch 00\tBatch 5870\t0.1458s/batch\ttrain_loss = 3.2209\n",
      "Epoch 00\tBatch 5880\t0.1443s/batch\ttrain_loss = 2.6471\n",
      "Epoch 00\tBatch 5880\ttest_loss = 3.2070\n",
      "Epoch 00\tBatch 5890\t0.1459s/batch\ttrain_loss = 3.5841\n",
      "Epoch 00\tBatch 5900\t0.1456s/batch\ttrain_loss = 3.4982\n",
      "Epoch 00\tBatch 5910\t0.1444s/batch\ttrain_loss = 2.8034\n",
      "Epoch 00\tBatch 5910\ttest_loss = 3.2073\n",
      "Epoch 00\tBatch 5920\t0.1448s/batch\ttrain_loss = 2.9837\n",
      "Epoch 01\tBatch 5930\t0.1433s/batch\ttrain_loss = 3.6032\n",
      "Epoch 01\tBatch 5940\t0.1475s/batch\ttrain_loss = 3.8219\n",
      "Epoch 01\tBatch 5940\ttest_loss = 3.1813\n",
      "Epoch 01\tBatch 5950\t0.1469s/batch\ttrain_loss = 4.0646\n",
      "Epoch 01\tBatch 5960\t0.1462s/batch\ttrain_loss = 3.7599\n",
      "Epoch 01\tBatch 5970\t0.1467s/batch\ttrain_loss = 3.7290\n",
      "Epoch 01\tBatch 5970\ttest_loss = 3.1453\n",
      "new best loss (3.150275 -> 3.145262)\n",
      "Epoch 01\tBatch 5980\t0.1442s/batch\ttrain_loss = 2.7632\n",
      "Epoch 01\tBatch 5990\t0.1473s/batch\ttrain_loss = 3.2173\n",
      "Epoch 01\tBatch 6000\t0.1657s/batch\ttrain_loss = 2.9819\n",
      "Epoch 01\tBatch 6000\ttest_loss = 3.1395\n",
      "new best loss (3.145262 -> 3.139469)\n",
      "Epoch 01\tBatch 6010\t0.1471s/batch\ttrain_loss = 3.3091\n",
      "Epoch 01\tBatch 6020\t0.1520s/batch\ttrain_loss = 2.9563\n",
      "Epoch 01\tBatch 6030\t0.1550s/batch\ttrain_loss = 3.0642\n",
      "Epoch 01\tBatch 6030\ttest_loss = 3.1711\n",
      "Epoch 01\tBatch 6040\t0.1490s/batch\ttrain_loss = 3.6129\n",
      "Epoch 01\tBatch 6050\t0.1480s/batch\ttrain_loss = 2.8028\n",
      "Epoch 01\tBatch 6060\t0.1470s/batch\ttrain_loss = 2.8733\n",
      "Epoch 01\tBatch 6060\ttest_loss = 3.1497\n",
      "Epoch 01\tBatch 6070\t0.1494s/batch\ttrain_loss = 3.2553\n",
      "Epoch 01\tBatch 6080\t0.1515s/batch\ttrain_loss = 3.3564\n",
      "Epoch 01\tBatch 6090\t0.1532s/batch\ttrain_loss = 3.3856\n",
      "Epoch 01\tBatch 6090\ttest_loss = 3.2250\n",
      "Epoch 01\tBatch 6100\t0.1484s/batch\ttrain_loss = 3.4228\n",
      "Epoch 01\tBatch 6110\t0.1450s/batch\ttrain_loss = 2.5854\n",
      "Epoch 01\tBatch 6120\t0.1534s/batch\ttrain_loss = 2.9809\n",
      "Epoch 01\tBatch 6120\ttest_loss = 3.1807\n",
      "Epoch 01\tBatch 6130\t0.1487s/batch\ttrain_loss = 4.4581\n",
      "Epoch 01\tBatch 6140\t0.1507s/batch\ttrain_loss = 3.0062\n",
      "Epoch 01\tBatch 6150\t0.1459s/batch\ttrain_loss = 3.1914\n",
      "Epoch 01\tBatch 6150\ttest_loss = 3.1798\n",
      "Epoch 01\tBatch 6160\t0.1464s/batch\ttrain_loss = 3.1746\n",
      "Epoch 01\tBatch 6170\t0.1462s/batch\ttrain_loss = 3.8697\n",
      "Epoch 01\tBatch 6180\t0.1460s/batch\ttrain_loss = 2.4405\n",
      "Epoch 01\tBatch 6180\ttest_loss = 3.1847\n",
      "Epoch 01\tBatch 6190\t0.1483s/batch\ttrain_loss = 2.5412\n",
      "Epoch 01\tBatch 6200\t0.1467s/batch\ttrain_loss = 3.4776\n",
      "Epoch 01\tBatch 6210\t0.1459s/batch\ttrain_loss = 2.9488\n",
      "Epoch 01\tBatch 6210\ttest_loss = 3.1420\n",
      "Epoch 01\tBatch 6220\t0.1476s/batch\ttrain_loss = 3.4968\n",
      "Epoch 01\tBatch 6230\t0.1472s/batch\ttrain_loss = 3.4119\n",
      "Epoch 01\tBatch 6240\t0.1470s/batch\ttrain_loss = 3.6539\n",
      "Epoch 01\tBatch 6240\ttest_loss = 3.1941\n",
      "Epoch 01\tBatch 6250\t0.1448s/batch\ttrain_loss = 2.8449\n",
      "Epoch 01\tBatch 6260\t0.1448s/batch\ttrain_loss = 3.1776\n",
      "Epoch 01\tBatch 6270\t0.1484s/batch\ttrain_loss = 3.7203\n",
      "Epoch 01\tBatch 6270\ttest_loss = 3.1821\n",
      "Epoch 01\tBatch 6280\t0.1457s/batch\ttrain_loss = 3.4763\n",
      "Epoch 01\tBatch 6290\t0.1466s/batch\ttrain_loss = 3.7444\n",
      "Epoch 01\tBatch 6300\t0.1473s/batch\ttrain_loss = 4.2725\n",
      "Epoch 01\tBatch 6300\ttest_loss = 3.1494\n",
      "Epoch 01\tBatch 6310\t0.1468s/batch\ttrain_loss = 3.8163\n",
      "Epoch 01\tBatch 6320\t0.1491s/batch\ttrain_loss = 3.3028\n",
      "Epoch 01\tBatch 6330\t0.1472s/batch\ttrain_loss = 3.8458\n",
      "Epoch 01\tBatch 6330\ttest_loss = 3.1439\n",
      "Epoch 01\tBatch 6340\t0.1469s/batch\ttrain_loss = 3.6965\n",
      "Epoch 01\tBatch 6350\t0.1456s/batch\ttrain_loss = 3.2382\n",
      "Epoch 01\tBatch 6360\t0.1462s/batch\ttrain_loss = 3.2080\n",
      "Epoch 01\tBatch 6360\ttest_loss = 3.1995\n",
      "Epoch 01\tBatch 6370\t0.1462s/batch\ttrain_loss = 3.4447\n",
      "Epoch 01\tBatch 6380\t0.1456s/batch\ttrain_loss = 2.8076\n",
      "Epoch 01\tBatch 6390\t0.1469s/batch\ttrain_loss = 3.2935\n",
      "Epoch 01\tBatch 6390\ttest_loss = 3.1505\n",
      "Epoch 01\tBatch 6400\t0.1472s/batch\ttrain_loss = 3.3979\n",
      "Epoch 01\tBatch 6410\t0.1467s/batch\ttrain_loss = 3.7238\n",
      "Epoch 01\tBatch 6420\t0.1462s/batch\ttrain_loss = 3.2567\n",
      "Epoch 01\tBatch 6420\ttest_loss = 3.1106\n",
      "new best loss (3.139469 -> 3.110625)\n",
      "Epoch 01\tBatch 6430\t0.1517s/batch\ttrain_loss = 3.7301\n",
      "Epoch 01\tBatch 6440\t0.1466s/batch\ttrain_loss = 3.7137\n",
      "Epoch 01\tBatch 6450\t0.1447s/batch\ttrain_loss = 2.7404\n",
      "Epoch 01\tBatch 6450\ttest_loss = 3.1500\n",
      "Epoch 01\tBatch 6460\t0.1447s/batch\ttrain_loss = 2.8746\n",
      "Epoch 01\tBatch 6470\t0.1447s/batch\ttrain_loss = 2.8070\n",
      "Epoch 01\tBatch 6480\t0.1434s/batch\ttrain_loss = 2.2655\n",
      "Epoch 01\tBatch 6480\ttest_loss = 3.0924\n",
      "new best loss (3.110625 -> 3.092395)\n",
      "Epoch 01\tBatch 6490\t0.1476s/batch\ttrain_loss = 3.1689\n",
      "Epoch 01\tBatch 6500\t0.1459s/batch\ttrain_loss = 3.2744\n",
      "Epoch 01\tBatch 6510\t0.1477s/batch\ttrain_loss = 3.8674\n",
      "Epoch 01\tBatch 6510\ttest_loss = 3.1393\n",
      "Epoch 01\tBatch 6520\t0.1449s/batch\ttrain_loss = 3.0826\n",
      "Epoch 01\tBatch 6530\t0.1459s/batch\ttrain_loss = 3.3003\n",
      "Epoch 01\tBatch 6540\t0.1447s/batch\ttrain_loss = 2.6617\n",
      "Epoch 01\tBatch 6540\ttest_loss = 3.0815\n",
      "new best loss (3.092395 -> 3.081459)\n",
      "Epoch 01\tBatch 6550\t0.1487s/batch\ttrain_loss = 4.3181\n",
      "Epoch 01\tBatch 6560\t0.1460s/batch\ttrain_loss = 3.3421\n",
      "Epoch 01\tBatch 6570\t0.1470s/batch\ttrain_loss = 3.3882\n",
      "Epoch 01\tBatch 6570\ttest_loss = 3.1334\n",
      "Epoch 01\tBatch 6580\t0.1486s/batch\ttrain_loss = 4.0889\n",
      "Epoch 01\tBatch 6590\t0.1451s/batch\ttrain_loss = 2.9015\n",
      "Epoch 01\tBatch 6600\t0.1454s/batch\ttrain_loss = 3.2182\n",
      "Epoch 01\tBatch 6600\ttest_loss = 3.1220\n",
      "Epoch 01\tBatch 6610\t0.1525s/batch\ttrain_loss = 2.6733\n",
      "Epoch 01\tBatch 6620\t0.1471s/batch\ttrain_loss = 3.0488\n",
      "Epoch 01\tBatch 6630\t0.1482s/batch\ttrain_loss = 3.8385\n",
      "Epoch 01\tBatch 6630\ttest_loss = 3.1179\n",
      "Epoch 01\tBatch 6640\t0.1465s/batch\ttrain_loss = 2.9885\n",
      "Epoch 01\tBatch 6650\t0.1478s/batch\ttrain_loss = 3.5163\n",
      "Epoch 01\tBatch 6660\t0.1450s/batch\ttrain_loss = 2.9844\n",
      "Epoch 01\tBatch 6660\ttest_loss = 3.0764\n",
      "new best loss (3.081459 -> 3.076442)\n",
      "Epoch 01\tBatch 6670\t0.1507s/batch\ttrain_loss = 3.2597\n",
      "Epoch 01\tBatch 6680\t0.1449s/batch\ttrain_loss = 2.7040\n",
      "Epoch 01\tBatch 6690\t0.1459s/batch\ttrain_loss = 3.4863\n",
      "Epoch 01\tBatch 6690\ttest_loss = 3.1084\n",
      "Epoch 01\tBatch 6700\t0.1453s/batch\ttrain_loss = 3.4971\n",
      "Epoch 01\tBatch 6710\t0.1458s/batch\ttrain_loss = 3.1125\n",
      "Epoch 01\tBatch 6720\t0.1443s/batch\ttrain_loss = 2.7465\n",
      "Epoch 01\tBatch 6720\ttest_loss = 3.1140\n",
      "Epoch 01\tBatch 6730\t0.1472s/batch\ttrain_loss = 3.3588\n",
      "Epoch 01\tBatch 6740\t0.1451s/batch\ttrain_loss = 2.8224\n",
      "Epoch 01\tBatch 6750\t0.1475s/batch\ttrain_loss = 3.8492\n",
      "Epoch 01\tBatch 6750\ttest_loss = 3.1012\n",
      "Epoch 01\tBatch 6760\t0.1472s/batch\ttrain_loss = 3.8120\n",
      "Epoch 01\tBatch 6770\t0.1461s/batch\ttrain_loss = 3.2128\n",
      "Epoch 01\tBatch 6780\t0.1449s/batch\ttrain_loss = 2.7324\n",
      "Epoch 01\tBatch 6780\ttest_loss = 3.1313\n",
      "Epoch 01\tBatch 6790\t0.1465s/batch\ttrain_loss = 3.4807\n",
      "Epoch 01\tBatch 6800\t0.1443s/batch\ttrain_loss = 2.7814\n",
      "Epoch 01\tBatch 6810\t0.1475s/batch\ttrain_loss = 2.7905\n",
      "Epoch 01\tBatch 6810\ttest_loss = 3.1276\n",
      "Epoch 01\tBatch 6820\t0.1446s/batch\ttrain_loss = 2.8567\n",
      "Epoch 01\tBatch 6830\t0.1498s/batch\ttrain_loss = 3.1339\n",
      "Epoch 01\tBatch 6840\t0.1464s/batch\ttrain_loss = 3.0307\n",
      "Epoch 01\tBatch 6840\ttest_loss = 3.1292\n",
      "Epoch 01\tBatch 6850\t0.1457s/batch\ttrain_loss = 2.8719\n",
      "Epoch 01\tBatch 6860\t0.1452s/batch\ttrain_loss = 2.7780\n",
      "Epoch 01\tBatch 6870\t0.1464s/batch\ttrain_loss = 3.0889\n",
      "Epoch 01\tBatch 6870\ttest_loss = 3.1073\n",
      "Epoch 01\tBatch 6880\t0.1512s/batch\ttrain_loss = 3.2365\n",
      "Epoch 01\tBatch 6890\t0.1504s/batch\ttrain_loss = 3.2632\n",
      "Epoch 01\tBatch 6900\t0.1467s/batch\ttrain_loss = 2.7917\n",
      "Epoch 01\tBatch 6900\ttest_loss = 3.1161\n",
      "Epoch 01\tBatch 6910\t0.1455s/batch\ttrain_loss = 2.7805\n",
      "Epoch 01\tBatch 6920\t0.1463s/batch\ttrain_loss = 3.3251\n",
      "Epoch 01\tBatch 6930\t0.1465s/batch\ttrain_loss = 3.5499\n",
      "Epoch 01\tBatch 6930\ttest_loss = 3.1304\n",
      "Epoch 01\tBatch 6940\t0.1470s/batch\ttrain_loss = 3.8875\n",
      "Epoch 01\tBatch 6950\t0.1456s/batch\ttrain_loss = 3.2616\n",
      "Epoch 01\tBatch 6960\t0.1447s/batch\ttrain_loss = 2.9866\n",
      "Epoch 01\tBatch 6960\ttest_loss = 3.1053\n",
      "Epoch 01\tBatch 6970\t0.1468s/batch\ttrain_loss = 3.6210\n",
      "Epoch 01\tBatch 6980\t0.1455s/batch\ttrain_loss = 3.2911\n",
      "Epoch 01\tBatch 6990\t0.1475s/batch\ttrain_loss = 3.5759\n",
      "Epoch 01\tBatch 6990\ttest_loss = 3.0772\n",
      "Epoch 01\tBatch 7000\t0.1465s/batch\ttrain_loss = 3.4325\n",
      "Epoch 01\tBatch 7010\t0.1468s/batch\ttrain_loss = 3.5288\n",
      "Epoch 01\tBatch 7020\t0.1456s/batch\ttrain_loss = 3.4035\n",
      "Epoch 01\tBatch 7020\ttest_loss = 3.0909\n",
      "Epoch 01\tBatch 7030\t0.1466s/batch\ttrain_loss = 3.0994\n",
      "Epoch 01\tBatch 7040\t0.1459s/batch\ttrain_loss = 2.9266\n",
      "Epoch 01\tBatch 7050\t0.1459s/batch\ttrain_loss = 3.5320\n",
      "Epoch 01\tBatch 7050\ttest_loss = 3.0690\n",
      "new best loss (3.076442 -> 3.068988)\n",
      "Epoch 01\tBatch 7060\t0.1519s/batch\ttrain_loss = 3.9994\n",
      "Epoch 01\tBatch 7070\t0.1499s/batch\ttrain_loss = 2.9827\n",
      "Epoch 01\tBatch 7080\t0.1473s/batch\ttrain_loss = 2.6722\n",
      "Epoch 01\tBatch 7080\ttest_loss = 3.0956\n",
      "Epoch 01\tBatch 7090\t0.1438s/batch\ttrain_loss = 2.3394\n",
      "Epoch 01\tBatch 7100\t0.1447s/batch\ttrain_loss = 2.9430\n",
      "Epoch 01\tBatch 7110\t0.1466s/batch\ttrain_loss = 3.5377\n",
      "Epoch 01\tBatch 7110\ttest_loss = 3.1002\n",
      "Epoch 01\tBatch 7120\t0.1515s/batch\ttrain_loss = 3.5945\n",
      "Epoch 01\tBatch 7130\t0.1440s/batch\ttrain_loss = 2.9588\n",
      "Epoch 01\tBatch 7140\t0.1447s/batch\ttrain_loss = 2.8797\n",
      "Epoch 01\tBatch 7140\ttest_loss = 3.0823\n",
      "Epoch 01\tBatch 7150\t0.1437s/batch\ttrain_loss = 2.6478\n",
      "Epoch 01\tBatch 7160\t0.1449s/batch\ttrain_loss = 3.1063\n",
      "Epoch 01\tBatch 7170\t0.1453s/batch\ttrain_loss = 2.7182\n",
      "Epoch 01\tBatch 7170\ttest_loss = 3.0991\n",
      "Epoch 01\tBatch 7180\t0.1453s/batch\ttrain_loss = 3.1819\n",
      "Epoch 01\tBatch 7190\t0.1470s/batch\ttrain_loss = 3.7021\n",
      "Epoch 01\tBatch 7200\t0.1462s/batch\ttrain_loss = 3.7086\n",
      "Epoch 01\tBatch 7200\ttest_loss = 3.1327\n",
      "Epoch 01\tBatch 7210\t0.1447s/batch\ttrain_loss = 2.5761\n",
      "Epoch 01\tBatch 7220\t0.1457s/batch\ttrain_loss = 2.5976\n",
      "Epoch 01\tBatch 7230\t0.1426s/batch\ttrain_loss = 1.9749\n",
      "Epoch 01\tBatch 7230\ttest_loss = 3.1235\n",
      "Epoch 01\tBatch 7240\t0.1456s/batch\ttrain_loss = 3.0583\n",
      "Epoch 01\tBatch 7250\t0.1460s/batch\ttrain_loss = 3.3110\n",
      "Epoch 01\tBatch 7260\t0.1466s/batch\ttrain_loss = 3.4914\n",
      "Epoch 01\tBatch 7260\ttest_loss = 3.1514\n",
      "Epoch 01\tBatch 7270\t0.1466s/batch\ttrain_loss = 3.4804\n",
      "Epoch 01\tBatch 7280\t0.1440s/batch\ttrain_loss = 2.3153\n",
      "Epoch 01\tBatch 7290\t0.1440s/batch\ttrain_loss = 2.5508\n",
      "Epoch 01\tBatch 7290\ttest_loss = 3.1043\n",
      "Epoch 01\tBatch 7300\t0.1438s/batch\ttrain_loss = 2.5232\n",
      "Epoch 01\tBatch 7310\t0.1454s/batch\ttrain_loss = 3.3106\n",
      "Epoch 01\tBatch 7320\t0.1448s/batch\ttrain_loss = 2.8235\n",
      "Epoch 01\tBatch 7320\ttest_loss = 3.1243\n",
      "Epoch 01\tBatch 7330\t0.1543s/batch\ttrain_loss = 3.8455\n",
      "Epoch 01\tBatch 7340\t0.1473s/batch\ttrain_loss = 3.2753\n",
      "Epoch 01\tBatch 7350\t0.1489s/batch\ttrain_loss = 3.8627\n",
      "Epoch 01\tBatch 7350\ttest_loss = 3.0828\n",
      "Epoch 01\tBatch 7360\t0.1462s/batch\ttrain_loss = 3.2363\n",
      "Epoch 01\tBatch 7370\t0.1448s/batch\ttrain_loss = 3.0390\n",
      "Epoch 01\tBatch 7380\t0.1464s/batch\ttrain_loss = 3.2097\n",
      "Epoch 01\tBatch 7380\ttest_loss = 3.1430\n",
      "Epoch 01\tBatch 7390\t0.1440s/batch\ttrain_loss = 2.7438\n",
      "Epoch 01\tBatch 7400\t0.1442s/batch\ttrain_loss = 2.5766\n",
      "Epoch 01\tBatch 7410\t0.1428s/batch\ttrain_loss = 2.1535\n",
      "Epoch 01\tBatch 7410\ttest_loss = 3.0964\n",
      "Epoch 01\tBatch 7420\t0.1448s/batch\ttrain_loss = 3.1393\n",
      "Epoch 01\tBatch 7430\t0.1446s/batch\ttrain_loss = 3.0258\n",
      "Epoch 01\tBatch 7440\t0.1461s/batch\ttrain_loss = 3.1588\n",
      "Epoch 01\tBatch 7440\ttest_loss = 3.1481\n",
      "Epoch 01\tBatch 7450\t0.1452s/batch\ttrain_loss = 3.0933\n",
      "Epoch 01\tBatch 7460\t0.1429s/batch\ttrain_loss = 2.5371\n",
      "Epoch 01\tBatch 7470\t0.1442s/batch\ttrain_loss = 3.0237\n",
      "Epoch 01\tBatch 7470\ttest_loss = 3.1606\n",
      "Epoch 01\tBatch 7480\t0.1440s/batch\ttrain_loss = 2.9841\n",
      "Epoch 01\tBatch 7490\t0.1437s/batch\ttrain_loss = 2.6543\n",
      "Epoch 01\tBatch 7500\t0.1427s/batch\ttrain_loss = 2.0226\n",
      "Epoch 01\tBatch 7500\ttest_loss = 3.1191\n",
      "Epoch 01\tBatch 7510\t0.1461s/batch\ttrain_loss = 2.5043\n",
      "Epoch 01\tBatch 7520\t0.1454s/batch\ttrain_loss = 2.8878\n",
      "Epoch 01\tBatch 7530\t0.1459s/batch\ttrain_loss = 3.0315\n",
      "Epoch 01\tBatch 7530\ttest_loss = 3.0638\n",
      "new best loss (3.068988 -> 3.063839)\n",
      "Epoch 01\tBatch 7540\t0.1517s/batch\ttrain_loss = 3.3666\n",
      "Epoch 01\tBatch 7550\t0.1497s/batch\ttrain_loss = 2.9973\n",
      "Epoch 01\tBatch 7560\t0.1502s/batch\ttrain_loss = 3.7271\n",
      "Epoch 01\tBatch 7560\ttest_loss = 3.0885\n",
      "Epoch 01\tBatch 7570\t0.1535s/batch\ttrain_loss = 3.4685\n",
      "Epoch 01\tBatch 7580\t0.1461s/batch\ttrain_loss = 2.9381\n",
      "Epoch 01\tBatch 7590\t0.1477s/batch\ttrain_loss = 2.8302\n",
      "Epoch 01\tBatch 7590\ttest_loss = 3.0842\n",
      "Epoch 01\tBatch 7600\t0.1444s/batch\ttrain_loss = 2.4655\n",
      "Epoch 01\tBatch 7610\t0.1486s/batch\ttrain_loss = 2.7079\n",
      "Epoch 01\tBatch 7620\t0.1482s/batch\ttrain_loss = 2.9491\n",
      "Epoch 01\tBatch 7620\ttest_loss = 3.1410\n",
      "Epoch 01\tBatch 7630\t0.1473s/batch\ttrain_loss = 3.9699\n",
      "Epoch 01\tBatch 7640\t0.1496s/batch\ttrain_loss = 3.5754\n",
      "Epoch 01\tBatch 7650\t0.1529s/batch\ttrain_loss = 3.4145\n",
      "Epoch 01\tBatch 7650\ttest_loss = 3.0631\n",
      "new best loss (3.063839 -> 3.063138)\n",
      "Epoch 01\tBatch 7660\t0.1523s/batch\ttrain_loss = 3.1076\n",
      "Epoch 01\tBatch 7670\t0.1566s/batch\ttrain_loss = 3.6468\n",
      "Epoch 01\tBatch 7680\t0.1513s/batch\ttrain_loss = 3.3452\n",
      "Epoch 01\tBatch 7680\ttest_loss = 3.0618\n",
      "new best loss (3.063138 -> 3.061798)\n",
      "Epoch 01\tBatch 7690\t0.1562s/batch\ttrain_loss = 2.6997\n",
      "Epoch 01\tBatch 7700\t0.1482s/batch\ttrain_loss = 2.8279\n",
      "Epoch 01\tBatch 7710\t0.1489s/batch\ttrain_loss = 3.6245\n",
      "Epoch 01\tBatch 7710\ttest_loss = 3.0502\n",
      "new best loss (3.061798 -> 3.050232)\n",
      "Epoch 01\tBatch 7720\t0.1472s/batch\ttrain_loss = 4.1336\n",
      "Epoch 01\tBatch 7730\t0.1466s/batch\ttrain_loss = 3.7082\n",
      "Epoch 01\tBatch 7740\t0.1456s/batch\ttrain_loss = 3.1969\n",
      "Epoch 01\tBatch 7740\ttest_loss = 3.0992\n",
      "Epoch 01\tBatch 7750\t0.1446s/batch\ttrain_loss = 2.9280\n",
      "Epoch 01\tBatch 7760\t0.1463s/batch\ttrain_loss = 3.6305\n",
      "Epoch 01\tBatch 7770\t0.1453s/batch\ttrain_loss = 3.3024\n",
      "Epoch 01\tBatch 7770\ttest_loss = 3.0619\n",
      "Epoch 01\tBatch 7780\t0.1450s/batch\ttrain_loss = 3.3344\n",
      "Epoch 01\tBatch 7790\t0.1451s/batch\ttrain_loss = 2.9518\n",
      "Epoch 01\tBatch 7800\t0.1429s/batch\ttrain_loss = 2.4443\n",
      "Epoch 01\tBatch 7800\ttest_loss = 3.0824\n",
      "Epoch 01\tBatch 7810\t0.1431s/batch\ttrain_loss = 2.4460\n",
      "Epoch 01\tBatch 7820\t0.1445s/batch\ttrain_loss = 2.9389\n",
      "Epoch 01\tBatch 7830\t0.1460s/batch\ttrain_loss = 3.2967\n",
      "Epoch 01\tBatch 7830\ttest_loss = 3.0456\n",
      "new best loss (3.050232 -> 3.045605)\n",
      "Epoch 01\tBatch 7840\t0.1443s/batch\ttrain_loss = 2.6930\n",
      "Epoch 01\tBatch 7850\t0.1441s/batch\ttrain_loss = 2.6377\n",
      "Epoch 01\tBatch 7860\t0.1445s/batch\ttrain_loss = 2.6148\n",
      "Epoch 01\tBatch 7860\ttest_loss = 3.0455\n",
      "new best loss (3.045605 -> 3.045549)\n",
      "Epoch 01\tBatch 7870\t0.1427s/batch\ttrain_loss = 2.2999\n",
      "Epoch 01\tBatch 7880\t0.1423s/batch\ttrain_loss = 2.3985\n",
      "Epoch 01\tBatch 7890\t0.1462s/batch\ttrain_loss = 3.3110\n",
      "Epoch 01\tBatch 7890\ttest_loss = 3.0366\n",
      "new best loss (3.045549 -> 3.036551)\n",
      "Epoch 01\tBatch 7900\t0.1446s/batch\ttrain_loss = 2.9133\n",
      "Epoch 01\tBatch 7910\t0.1457s/batch\ttrain_loss = 3.8742\n",
      "Epoch 01\tBatch 7920\t0.1453s/batch\ttrain_loss = 3.6083\n",
      "Epoch 01\tBatch 7920\ttest_loss = 3.0654\n",
      "Epoch 01\tBatch 7930\t0.1427s/batch\ttrain_loss = 2.5446\n",
      "Epoch 01\tBatch 7940\t0.1442s/batch\ttrain_loss = 2.7566\n",
      "Epoch 01\tBatch 7950\t0.1453s/batch\ttrain_loss = 2.7895\n",
      "Epoch 01\tBatch 7950\ttest_loss = 3.0668\n",
      "Epoch 01\tBatch 7960\t0.1454s/batch\ttrain_loss = 3.4219\n",
      "Epoch 01\tBatch 7970\t0.1459s/batch\ttrain_loss = 3.3586\n",
      "Epoch 01\tBatch 7980\t0.1448s/batch\ttrain_loss = 2.7692\n",
      "Epoch 01\tBatch 7980\ttest_loss = 3.0610\n",
      "Epoch 01\tBatch 7990\t0.1621s/batch\ttrain_loss = 2.5162\n",
      "Epoch 01\tBatch 8000\t0.1438s/batch\ttrain_loss = 2.0918\n",
      "Epoch 01\tBatch 8010\t0.1457s/batch\ttrain_loss = 3.3000\n",
      "Epoch 01\tBatch 8010\ttest_loss = 3.0526\n",
      "Epoch 01\tBatch 8020\t0.1444s/batch\ttrain_loss = 2.6858\n",
      "Epoch 01\tBatch 8030\t0.1445s/batch\ttrain_loss = 2.7388\n",
      "Epoch 01\tBatch 8040\t0.1462s/batch\ttrain_loss = 3.8838\n",
      "Epoch 01\tBatch 8040\ttest_loss = 3.0658\n",
      "Epoch 01\tBatch 8050\t0.1458s/batch\ttrain_loss = 3.9100\n",
      "Epoch 01\tBatch 8060\t0.1445s/batch\ttrain_loss = 2.8281\n",
      "Epoch 01\tBatch 8070\t0.1450s/batch\ttrain_loss = 3.6809\n",
      "Epoch 01\tBatch 8070\ttest_loss = 3.0643\n",
      "Epoch 01\tBatch 8080\t0.1465s/batch\ttrain_loss = 3.6005\n",
      "Epoch 01\tBatch 8090\t0.1444s/batch\ttrain_loss = 2.6808\n",
      "Epoch 01\tBatch 8100\t0.1486s/batch\ttrain_loss = 3.5054\n",
      "Epoch 01\tBatch 8100\ttest_loss = 3.1406\n",
      "Epoch 01\tBatch 8110\t0.1444s/batch\ttrain_loss = 3.0187\n",
      "Epoch 01\tBatch 8120\t0.1457s/batch\ttrain_loss = 3.4115\n",
      "Epoch 01\tBatch 8130\t0.1480s/batch\ttrain_loss = 4.4272\n",
      "Epoch 01\tBatch 8130\ttest_loss = 3.0961\n",
      "Epoch 01\tBatch 8140\t0.1440s/batch\ttrain_loss = 2.8238\n",
      "Epoch 01\tBatch 8150\t0.1456s/batch\ttrain_loss = 3.5901\n",
      "Epoch 01\tBatch 8160\t0.1453s/batch\ttrain_loss = 3.0009\n",
      "Epoch 01\tBatch 8160\ttest_loss = 3.0640\n",
      "Epoch 01\tBatch 8170\t0.1451s/batch\ttrain_loss = 2.9545\n",
      "Epoch 01\tBatch 8180\t0.1459s/batch\ttrain_loss = 3.5883\n",
      "Epoch 01\tBatch 8190\t0.1450s/batch\ttrain_loss = 2.7303\n",
      "Epoch 01\tBatch 8190\ttest_loss = 3.1085\n",
      "Epoch 01\tBatch 8200\t0.1448s/batch\ttrain_loss = 2.9733\n",
      "Epoch 01\tBatch 8210\t0.1457s/batch\ttrain_loss = 3.3359\n",
      "Epoch 01\tBatch 8220\t0.1459s/batch\ttrain_loss = 3.4608\n",
      "Epoch 01\tBatch 8220\ttest_loss = 3.0132\n",
      "new best loss (3.036551 -> 3.013168)\n",
      "Epoch 01\tBatch 8230\t0.1462s/batch\ttrain_loss = 3.5681\n",
      "Epoch 01\tBatch 8240\t0.1467s/batch\ttrain_loss = 3.8176\n",
      "Epoch 01\tBatch 8250\t0.1467s/batch\ttrain_loss = 3.2277\n",
      "Epoch 01\tBatch 8250\ttest_loss = 3.0917\n",
      "Epoch 01\tBatch 8260\t0.1456s/batch\ttrain_loss = 3.3469\n",
      "Epoch 01\tBatch 8270\t0.1455s/batch\ttrain_loss = 3.5907\n",
      "Epoch 01\tBatch 8280\t0.1453s/batch\ttrain_loss = 3.0333\n",
      "Epoch 01\tBatch 8280\ttest_loss = 3.1294\n",
      "Epoch 01\tBatch 8290\t0.1447s/batch\ttrain_loss = 2.8789\n",
      "Epoch 01\tBatch 8300\t0.1452s/batch\ttrain_loss = 3.0006\n",
      "Epoch 01\tBatch 8310\t0.1446s/batch\ttrain_loss = 2.8674\n",
      "Epoch 01\tBatch 8310\ttest_loss = 3.0680\n",
      "Epoch 01\tBatch 8320\t0.1447s/batch\ttrain_loss = 2.8062\n",
      "Epoch 01\tBatch 8330\t0.1455s/batch\ttrain_loss = 3.2235\n",
      "Epoch 01\tBatch 8340\t0.1462s/batch\ttrain_loss = 3.8710\n",
      "Epoch 01\tBatch 8340\ttest_loss = 3.0444\n",
      "Epoch 01\tBatch 8350\t0.1450s/batch\ttrain_loss = 2.6681\n",
      "Epoch 01\tBatch 8360\t0.1450s/batch\ttrain_loss = 2.9704\n",
      "Epoch 01\tBatch 8370\t0.1455s/batch\ttrain_loss = 2.7192\n",
      "Epoch 01\tBatch 8370\ttest_loss = 3.0875\n",
      "Epoch 01\tBatch 8380\t0.1452s/batch\ttrain_loss = 3.0313\n",
      "Epoch 01\tBatch 8390\t0.1444s/batch\ttrain_loss = 2.6812\n",
      "Epoch 01\tBatch 8400\t0.1448s/batch\ttrain_loss = 2.8333\n",
      "Epoch 01\tBatch 8400\ttest_loss = 3.0959\n",
      "Epoch 01\tBatch 8410\t0.1436s/batch\ttrain_loss = 2.5196\n",
      "Epoch 01\tBatch 8420\t0.1463s/batch\ttrain_loss = 3.3329\n",
      "Epoch 01\tBatch 8430\t0.1454s/batch\ttrain_loss = 2.9501\n",
      "Epoch 01\tBatch 8430\ttest_loss = 3.0167\n",
      "Epoch 01\tBatch 8440\t0.1474s/batch\ttrain_loss = 3.5735\n",
      "Epoch 01\tBatch 8450\t0.1466s/batch\ttrain_loss = 3.4682\n",
      "Epoch 01\tBatch 8460\t0.1433s/batch\ttrain_loss = 2.4920\n",
      "Epoch 01\tBatch 8460\ttest_loss = 3.0084\n",
      "new best loss (3.013168 -> 3.008373)\n",
      "Epoch 01\tBatch 8470\t0.1448s/batch\ttrain_loss = 3.1033\n",
      "Epoch 01\tBatch 8480\t0.1464s/batch\ttrain_loss = 2.8493\n",
      "Epoch 01\tBatch 8490\t0.1477s/batch\ttrain_loss = 3.9377\n",
      "Epoch 01\tBatch 8490\ttest_loss = 3.0549\n",
      "Epoch 01\tBatch 8500\t0.1461s/batch\ttrain_loss = 3.5502\n",
      "Epoch 01\tBatch 8510\t0.1497s/batch\ttrain_loss = 2.6720\n",
      "Epoch 01\tBatch 8520\t0.1459s/batch\ttrain_loss = 3.8059\n",
      "Epoch 01\tBatch 8520\ttest_loss = 3.0226\n",
      "Epoch 01\tBatch 8530\t0.1471s/batch\ttrain_loss = 3.5192\n",
      "Epoch 01\tBatch 8540\t0.1450s/batch\ttrain_loss = 3.0359\n",
      "Epoch 01\tBatch 8550\t0.1463s/batch\ttrain_loss = 3.2759\n",
      "Epoch 01\tBatch 8550\ttest_loss = 3.0252\n",
      "Epoch 01\tBatch 8560\t0.1458s/batch\ttrain_loss = 3.3489\n",
      "Epoch 01\tBatch 8570\t0.1448s/batch\ttrain_loss = 2.8008\n",
      "Epoch 01\tBatch 8580\t0.1459s/batch\ttrain_loss = 3.3202\n",
      "Epoch 01\tBatch 8580\ttest_loss = 3.0418\n",
      "Epoch 01\tBatch 8590\t0.1451s/batch\ttrain_loss = 2.9309\n",
      "Epoch 01\tBatch 8600\t0.1462s/batch\ttrain_loss = 3.5134\n",
      "Epoch 01\tBatch 8610\t0.1445s/batch\ttrain_loss = 2.6277\n",
      "Epoch 01\tBatch 8610\ttest_loss = 3.0205\n",
      "Epoch 01\tBatch 8620\t0.1463s/batch\ttrain_loss = 3.4552\n",
      "Epoch 01\tBatch 8630\t0.1446s/batch\ttrain_loss = 3.0664\n",
      "Epoch 01\tBatch 8640\t0.1449s/batch\ttrain_loss = 2.8665\n",
      "Epoch 01\tBatch 8640\ttest_loss = 3.0523\n",
      "Epoch 01\tBatch 8650\t0.1466s/batch\ttrain_loss = 3.2584\n",
      "Epoch 01\tBatch 8660\t0.1458s/batch\ttrain_loss = 3.1353\n",
      "Epoch 01\tBatch 8670\t0.1456s/batch\ttrain_loss = 2.8745\n",
      "Epoch 01\tBatch 8670\ttest_loss = 2.9959\n",
      "new best loss (3.008373 -> 2.995902)\n",
      "Epoch 01\tBatch 8680\t0.1450s/batch\ttrain_loss = 2.6534\n",
      "Epoch 01\tBatch 8690\t0.1454s/batch\ttrain_loss = 2.8709\n",
      "Epoch 01\tBatch 8700\t0.1445s/batch\ttrain_loss = 2.7762\n",
      "Epoch 01\tBatch 8700\ttest_loss = 3.0070\n",
      "Epoch 01\tBatch 8710\t0.1460s/batch\ttrain_loss = 3.7145\n",
      "Epoch 01\tBatch 8720\t0.1477s/batch\ttrain_loss = 3.8159\n",
      "Epoch 01\tBatch 8730\t0.1463s/batch\ttrain_loss = 2.9768\n",
      "Epoch 01\tBatch 8730\ttest_loss = 3.0023\n",
      "Epoch 01\tBatch 8740\t0.1455s/batch\ttrain_loss = 3.2289\n",
      "Epoch 01\tBatch 8750\t0.1446s/batch\ttrain_loss = 2.8790\n",
      "Epoch 01\tBatch 8760\t0.1453s/batch\ttrain_loss = 2.8380\n",
      "Epoch 01\tBatch 8760\ttest_loss = 3.0423\n",
      "Epoch 01\tBatch 8770\t0.1436s/batch\ttrain_loss = 2.2680\n",
      "Epoch 01\tBatch 8780\t0.1466s/batch\ttrain_loss = 3.2273\n",
      "Epoch 01\tBatch 8790\t0.1462s/batch\ttrain_loss = 3.2618\n",
      "Epoch 01\tBatch 8790\ttest_loss = 3.0262\n",
      "Epoch 01\tBatch 8800\t0.1462s/batch\ttrain_loss = 3.2328\n",
      "Epoch 01\tBatch 8810\t0.1455s/batch\ttrain_loss = 3.3124\n",
      "Epoch 01\tBatch 8820\t0.1460s/batch\ttrain_loss = 3.0229\n",
      "Epoch 01\tBatch 8820\ttest_loss = 3.0106\n",
      "Epoch 01\tBatch 8830\t0.1463s/batch\ttrain_loss = 3.1645\n",
      "Epoch 01\tBatch 8840\t0.1449s/batch\ttrain_loss = 3.0077\n",
      "Epoch 01\tBatch 8850\t0.1444s/batch\ttrain_loss = 2.5440\n",
      "Epoch 01\tBatch 8850\ttest_loss = 2.9910\n",
      "new best loss (2.995902 -> 2.991034)\n",
      "Epoch 01\tBatch 8860\t0.1459s/batch\ttrain_loss = 3.0211\n",
      "Epoch 01\tBatch 8870\t0.1502s/batch\ttrain_loss = 3.4117\n",
      "Epoch 01\tBatch 8880\t0.1447s/batch\ttrain_loss = 2.9593\n",
      "Epoch 01\tBatch 8880\ttest_loss = 3.0939\n",
      "Epoch 01\tBatch 8890\t0.1556s/batch\ttrain_loss = 2.5052\n",
      "Epoch 01\tBatch 8900\t0.1531s/batch\ttrain_loss = 2.9917\n",
      "Epoch 01\tBatch 8910\t0.1529s/batch\ttrain_loss = 2.5596\n",
      "Epoch 01\tBatch 8910\ttest_loss = 3.0516\n",
      "Epoch 01\tBatch 8920\t0.1523s/batch\ttrain_loss = 2.4546\n",
      "Epoch 01\tBatch 8930\t0.1535s/batch\ttrain_loss = 3.1911\n",
      "Epoch 01\tBatch 8940\t0.1531s/batch\ttrain_loss = 2.9477\n",
      "Epoch 01\tBatch 8940\ttest_loss = 3.0689\n",
      "Epoch 01\tBatch 8950\t0.1538s/batch\ttrain_loss = 2.6081\n",
      "Epoch 01\tBatch 8960\t0.1534s/batch\ttrain_loss = 3.2182\n",
      "Epoch 01\tBatch 8970\t0.1547s/batch\ttrain_loss = 3.7312\n",
      "Epoch 01\tBatch 8970\ttest_loss = 2.9948\n",
      "Epoch 01\tBatch 8980\t0.1522s/batch\ttrain_loss = 3.0660\n",
      "Epoch 01\tBatch 8990\t0.1501s/batch\ttrain_loss = 2.5240\n",
      "Epoch 01\tBatch 9000\t0.1540s/batch\ttrain_loss = 3.6243\n",
      "Epoch 01\tBatch 9000\ttest_loss = 2.9709\n",
      "new best loss (2.991034 -> 2.970938)\n",
      "Epoch 01\tBatch 9010\t0.1545s/batch\ttrain_loss = 3.7120\n",
      "Epoch 01\tBatch 9020\t0.1527s/batch\ttrain_loss = 2.9970\n",
      "Epoch 01\tBatch 9030\t0.1529s/batch\ttrain_loss = 2.6329\n",
      "Epoch 01\tBatch 9030\ttest_loss = 3.0186\n",
      "Epoch 01\tBatch 9040\t0.1524s/batch\ttrain_loss = 2.8678\n",
      "Epoch 01\tBatch 9050\t0.1531s/batch\ttrain_loss = 2.6876\n",
      "Epoch 01\tBatch 9060\t0.1528s/batch\ttrain_loss = 2.8298\n",
      "Epoch 01\tBatch 9060\ttest_loss = 2.9848\n",
      "Epoch 01\tBatch 9070\t0.1521s/batch\ttrain_loss = 2.8763\n",
      "Epoch 01\tBatch 9080\t0.1518s/batch\ttrain_loss = 3.3069\n",
      "Epoch 01\tBatch 9090\t0.1501s/batch\ttrain_loss = 2.3915\n",
      "Epoch 01\tBatch 9090\ttest_loss = 3.0286\n",
      "Epoch 01\tBatch 9100\t0.1518s/batch\ttrain_loss = 2.5551\n",
      "Epoch 01\tBatch 9110\t0.1516s/batch\ttrain_loss = 3.1690\n",
      "Epoch 01\tBatch 9120\t0.1513s/batch\ttrain_loss = 2.6167\n",
      "Epoch 01\tBatch 9120\ttest_loss = 3.0043\n",
      "Epoch 01\tBatch 9130\t0.1518s/batch\ttrain_loss = 3.0771\n",
      "Epoch 01\tBatch 9140\t0.1509s/batch\ttrain_loss = 2.1047\n",
      "Epoch 01\tBatch 9150\t0.1536s/batch\ttrain_loss = 3.3461\n",
      "Epoch 01\tBatch 9150\ttest_loss = 2.9814\n",
      "Epoch 01\tBatch 9160\t0.1528s/batch\ttrain_loss = 2.7289\n",
      "Epoch 01\tBatch 9170\t0.1539s/batch\ttrain_loss = 3.1388\n",
      "Epoch 01\tBatch 9180\t0.1528s/batch\ttrain_loss = 2.9133\n",
      "Epoch 01\tBatch 9180\ttest_loss = 2.9777\n",
      "Epoch 01\tBatch 9190\t0.1530s/batch\ttrain_loss = 3.1885\n",
      "Epoch 01\tBatch 9200\t0.1540s/batch\ttrain_loss = 3.4596\n",
      "Epoch 01\tBatch 9210\t0.1534s/batch\ttrain_loss = 3.2479\n",
      "Epoch 01\tBatch 9210\ttest_loss = 2.9967\n",
      "Epoch 01\tBatch 9220\t0.1544s/batch\ttrain_loss = 2.8212\n",
      "Epoch 01\tBatch 9230\t0.1561s/batch\ttrain_loss = 3.3443\n",
      "Epoch 01\tBatch 9240\t0.1603s/batch\ttrain_loss = 2.4614\n",
      "Epoch 01\tBatch 9240\ttest_loss = 3.0104\n",
      "Epoch 01\tBatch 9250\t0.1529s/batch\ttrain_loss = 2.6863\n",
      "Epoch 01\tBatch 9260\t0.1538s/batch\ttrain_loss = 3.0879\n",
      "Epoch 01\tBatch 9270\t0.1548s/batch\ttrain_loss = 3.3189\n",
      "Epoch 01\tBatch 9270\ttest_loss = 2.9589\n",
      "new best loss (2.970938 -> 2.958924)\n",
      "Epoch 01\tBatch 9280\t0.1533s/batch\ttrain_loss = 2.7312\n",
      "Epoch 01\tBatch 9290\t0.1528s/batch\ttrain_loss = 2.4517\n",
      "Epoch 01\tBatch 9300\t0.1582s/batch\ttrain_loss = 3.5208\n",
      "Epoch 01\tBatch 9300\ttest_loss = 2.9965\n",
      "Epoch 01\tBatch 9310\t0.1530s/batch\ttrain_loss = 2.6083\n",
      "Epoch 01\tBatch 9320\t0.1552s/batch\ttrain_loss = 2.7196\n",
      "Epoch 01\tBatch 9330\t0.1551s/batch\ttrain_loss = 2.7215\n",
      "Epoch 01\tBatch 9330\ttest_loss = 3.0002\n",
      "Epoch 01\tBatch 9340\t0.1546s/batch\ttrain_loss = 2.6633\n",
      "Epoch 01\tBatch 9350\t0.1520s/batch\ttrain_loss = 2.8436\n",
      "Epoch 01\tBatch 9360\t0.1505s/batch\ttrain_loss = 2.4780\n",
      "Epoch 01\tBatch 9360\ttest_loss = 3.0906\n",
      "Epoch 01\tBatch 9370\t0.1530s/batch\ttrain_loss = 3.2531\n",
      "Epoch 01\tBatch 9380\t0.1524s/batch\ttrain_loss = 3.0290\n",
      "Epoch 01\tBatch 9390\t0.1528s/batch\ttrain_loss = 3.1012\n",
      "Epoch 01\tBatch 9390\ttest_loss = 2.9805\n",
      "Epoch 01\tBatch 9400\t0.1535s/batch\ttrain_loss = 3.3979\n",
      "Epoch 01\tBatch 9410\t0.1526s/batch\ttrain_loss = 2.4564\n",
      "Epoch 01\tBatch 9420\t0.1531s/batch\ttrain_loss = 2.5887\n",
      "Epoch 01\tBatch 9420\ttest_loss = 2.9737\n",
      "Epoch 01\tBatch 9430\t0.1520s/batch\ttrain_loss = 2.5328\n",
      "Epoch 01\tBatch 9440\t0.1520s/batch\ttrain_loss = 2.9412\n",
      "Epoch 01\tBatch 9450\t0.1532s/batch\ttrain_loss = 3.1922\n",
      "Epoch 01\tBatch 9450\ttest_loss = 3.0017\n",
      "Epoch 01\tBatch 9460\t0.1521s/batch\ttrain_loss = 2.9353\n",
      "Epoch 01\tBatch 9470\t0.1528s/batch\ttrain_loss = 3.3200\n",
      "Epoch 01\tBatch 9480\t0.1543s/batch\ttrain_loss = 2.7589\n",
      "Epoch 01\tBatch 9480\ttest_loss = 2.9969\n",
      "Epoch 01\tBatch 9490\t0.1553s/batch\ttrain_loss = 2.9633\n",
      "Epoch 01\tBatch 9500\t0.1583s/batch\ttrain_loss = 4.0786\n",
      "Epoch 01\tBatch 9510\t0.1577s/batch\ttrain_loss = 2.8794\n",
      "Epoch 01\tBatch 9510\ttest_loss = 2.9974\n",
      "Epoch 01\tBatch 9520\t0.1533s/batch\ttrain_loss = 3.2228\n",
      "Epoch 01\tBatch 9530\t0.1557s/batch\ttrain_loss = 3.0186\n",
      "Epoch 01\tBatch 9540\t0.1543s/batch\ttrain_loss = 3.5130\n",
      "Epoch 01\tBatch 9540\ttest_loss = 3.0029\n",
      "Epoch 01\tBatch 9550\t0.1498s/batch\ttrain_loss = 2.2614\n",
      "Epoch 01\tBatch 9560\t0.1549s/batch\ttrain_loss = 3.5166\n",
      "Epoch 01\tBatch 9570\t0.1527s/batch\ttrain_loss = 3.0459\n",
      "Epoch 01\tBatch 9570\ttest_loss = 2.9849\n",
      "Epoch 01\tBatch 9580\t0.1510s/batch\ttrain_loss = 2.4680\n",
      "Epoch 01\tBatch 9590\t0.1590s/batch\ttrain_loss = 3.1656\n",
      "Epoch 01\tBatch 9600\t0.1571s/batch\ttrain_loss = 3.2823\n",
      "Epoch 01\tBatch 9600\ttest_loss = 3.0866\n",
      "Epoch 01\tBatch 9610\t0.1552s/batch\ttrain_loss = 3.3290\n",
      "Epoch 01\tBatch 9620\t0.1529s/batch\ttrain_loss = 3.5257\n",
      "Epoch 01\tBatch 9630\t0.1523s/batch\ttrain_loss = 3.1810\n",
      "Epoch 01\tBatch 9630\ttest_loss = 3.0116\n",
      "Epoch 01\tBatch 9640\t0.1530s/batch\ttrain_loss = 3.1604\n",
      "Epoch 01\tBatch 9650\t0.1524s/batch\ttrain_loss = 2.9638\n",
      "Epoch 01\tBatch 9660\t0.1507s/batch\ttrain_loss = 2.4615\n",
      "Epoch 01\tBatch 9660\ttest_loss = 2.9520\n",
      "new best loss (2.958924 -> 2.951991)\n",
      "Epoch 01\tBatch 9670\t0.1531s/batch\ttrain_loss = 2.8339\n",
      "Epoch 01\tBatch 9680\t0.1508s/batch\ttrain_loss = 2.7085\n",
      "Epoch 01\tBatch 9690\t0.1543s/batch\ttrain_loss = 3.2373\n",
      "Epoch 01\tBatch 9690\ttest_loss = 2.9713\n",
      "Epoch 01\tBatch 9700\t0.1540s/batch\ttrain_loss = 3.5082\n",
      "Epoch 01\tBatch 9710\t0.1523s/batch\ttrain_loss = 2.9784\n",
      "Epoch 01\tBatch 9720\t0.1546s/batch\ttrain_loss = 3.6443\n",
      "Epoch 01\tBatch 9720\ttest_loss = 3.0072\n",
      "Epoch 01\tBatch 9730\t0.1519s/batch\ttrain_loss = 3.3908\n",
      "Epoch 01\tBatch 9740\t0.1518s/batch\ttrain_loss = 2.8778\n",
      "Epoch 01\tBatch 9750\t0.1526s/batch\ttrain_loss = 3.0470\n",
      "Epoch 01\tBatch 9750\ttest_loss = 3.0030\n",
      "Epoch 01\tBatch 9760\t0.1546s/batch\ttrain_loss = 3.5786\n",
      "Epoch 01\tBatch 9770\t0.1513s/batch\ttrain_loss = 2.9681\n",
      "Epoch 01\tBatch 9780\t0.1551s/batch\ttrain_loss = 3.9329\n",
      "Epoch 01\tBatch 9780\ttest_loss = 3.0824\n",
      "Epoch 01\tBatch 9790\t0.1523s/batch\ttrain_loss = 2.8729\n",
      "Epoch 01\tBatch 9800\t0.1528s/batch\ttrain_loss = 2.8744\n",
      "Epoch 01\tBatch 9810\t0.1557s/batch\ttrain_loss = 3.9187\n",
      "Epoch 01\tBatch 9810\ttest_loss = 3.0034\n",
      "Epoch 01\tBatch 9820\t0.1532s/batch\ttrain_loss = 3.0775\n",
      "Epoch 01\tBatch 9830\t0.1537s/batch\ttrain_loss = 3.4335\n",
      "Epoch 01\tBatch 9840\t0.1546s/batch\ttrain_loss = 3.1918\n",
      "Epoch 01\tBatch 9840\ttest_loss = 3.0359\n",
      "Epoch 01\tBatch 9850\t0.1541s/batch\ttrain_loss = 3.3251\n",
      "Epoch 01\tBatch 9860\t0.1546s/batch\ttrain_loss = 3.6510\n",
      "Epoch 01\tBatch 9870\t0.1546s/batch\ttrain_loss = 3.4534\n",
      "Epoch 01\tBatch 9870\ttest_loss = 2.9944\n",
      "Epoch 01\tBatch 9880\t0.1591s/batch\ttrain_loss = 3.6276\n",
      "Epoch 01\tBatch 9890\t0.1545s/batch\ttrain_loss = 3.1282\n",
      "Epoch 01\tBatch 9900\t0.1527s/batch\ttrain_loss = 2.8000\n",
      "Epoch 01\tBatch 9900\ttest_loss = 3.0617\n",
      "Epoch 01\tBatch 9910\t0.1524s/batch\ttrain_loss = 2.9486\n",
      "Epoch 01\tBatch 9920\t0.1512s/batch\ttrain_loss = 2.5082\n",
      "Epoch 01\tBatch 9930\t0.1530s/batch\ttrain_loss = 3.0777\n",
      "Epoch 01\tBatch 9930\ttest_loss = 3.0032\n",
      "Epoch 01\tBatch 9940\t0.1526s/batch\ttrain_loss = 2.4476\n",
      "Epoch 01\tBatch 9950\t0.1533s/batch\ttrain_loss = 3.1802\n",
      "Epoch 01\tBatch 9960\t0.1519s/batch\ttrain_loss = 2.8127\n",
      "Epoch 01\tBatch 9960\ttest_loss = 2.9597\n",
      "Epoch 01\tBatch 9970\t0.1525s/batch\ttrain_loss = 3.2988\n",
      "Epoch 01\tBatch 9980\t0.1511s/batch\ttrain_loss = 2.8995\n",
      "Epoch 01\tBatch 9990\t0.1515s/batch\ttrain_loss = 2.4867\n",
      "Epoch 01\tBatch 9990\ttest_loss = 3.0953\n",
      "Epoch 01\tBatch 10000\t0.1547s/batch\ttrain_loss = 3.4175\n",
      "Epoch 01\tBatch 10010\t0.1549s/batch\ttrain_loss = 3.1689\n",
      "Epoch 01\tBatch 10020\t0.1547s/batch\ttrain_loss = 3.3354\n",
      "Epoch 01\tBatch 10020\ttest_loss = 3.0111\n",
      "Epoch 01\tBatch 10030\t0.1539s/batch\ttrain_loss = 3.2286\n",
      "Epoch 01\tBatch 10040\t0.1526s/batch\ttrain_loss = 2.6743\n",
      "Epoch 01\tBatch 10050\t0.1548s/batch\ttrain_loss = 3.4647\n",
      "Epoch 01\tBatch 10050\ttest_loss = 3.0162\n",
      "Epoch 01\tBatch 10060\t0.1551s/batch\ttrain_loss = 3.4284\n",
      "Epoch 01\tBatch 10070\t0.1544s/batch\ttrain_loss = 2.9424\n",
      "Epoch 01\tBatch 10080\t0.1536s/batch\ttrain_loss = 3.0009\n",
      "Epoch 01\tBatch 10080\ttest_loss = 2.9839\n",
      "Epoch 01\tBatch 10090\t0.1536s/batch\ttrain_loss = 2.6719\n",
      "Epoch 01\tBatch 10100\t0.1549s/batch\ttrain_loss = 3.6935\n",
      "Epoch 01\tBatch 10110\t0.1540s/batch\ttrain_loss = 3.1801\n",
      "Epoch 01\tBatch 10110\ttest_loss = 2.9972\n",
      "Epoch 01\tBatch 10120\t0.1552s/batch\ttrain_loss = 3.6948\n",
      "Epoch 01\tBatch 10130\t0.1561s/batch\ttrain_loss = 3.8142\n",
      "Epoch 01\tBatch 10140\t0.1556s/batch\ttrain_loss = 3.3652\n",
      "Epoch 01\tBatch 10140\ttest_loss = 2.9876\n",
      "Epoch 01\tBatch 10150\t0.1550s/batch\ttrain_loss = 3.0336\n",
      "Epoch 01\tBatch 10160\t0.1556s/batch\ttrain_loss = 2.8758\n",
      "Epoch 01\tBatch 10170\t0.1555s/batch\ttrain_loss = 2.9689\n",
      "Epoch 01\tBatch 10170\ttest_loss = 2.9398\n",
      "new best loss (2.951991 -> 2.939783)\n",
      "Epoch 01\tBatch 10180\t0.1551s/batch\ttrain_loss = 3.0885\n",
      "Epoch 01\tBatch 10190\t0.1561s/batch\ttrain_loss = 4.0627\n",
      "Epoch 01\tBatch 10200\t0.1541s/batch\ttrain_loss = 3.0527\n",
      "Epoch 01\tBatch 10200\ttest_loss = 3.0036\n",
      "Epoch 01\tBatch 10210\t0.1559s/batch\ttrain_loss = 3.2265\n",
      "Epoch 01\tBatch 10220\t0.1561s/batch\ttrain_loss = 3.2981\n",
      "Epoch 01\tBatch 10230\t0.1546s/batch\ttrain_loss = 2.4043\n",
      "Epoch 01\tBatch 10230\ttest_loss = 2.9579\n",
      "Epoch 01\tBatch 10240\t0.1544s/batch\ttrain_loss = 2.6204\n",
      "Epoch 01\tBatch 10250\t0.1539s/batch\ttrain_loss = 2.4537\n",
      "Epoch 01\tBatch 10260\t0.1530s/batch\ttrain_loss = 2.2161\n",
      "Epoch 01\tBatch 10260\ttest_loss = 2.9937\n",
      "Epoch 01\tBatch 10270\t0.1563s/batch\ttrain_loss = 3.5598\n",
      "Epoch 01\tBatch 10280\t0.1541s/batch\ttrain_loss = 2.9029\n",
      "Epoch 01\tBatch 10290\t0.1564s/batch\ttrain_loss = 3.1391\n",
      "Epoch 01\tBatch 10290\ttest_loss = 3.0184\n",
      "Epoch 01\tBatch 10300\t0.1543s/batch\ttrain_loss = 3.0243\n",
      "Epoch 01\tBatch 10310\t0.1551s/batch\ttrain_loss = 3.7789\n",
      "Epoch 01\tBatch 10320\t0.1525s/batch\ttrain_loss = 2.3105\n",
      "Epoch 01\tBatch 10320\ttest_loss = 3.0243\n",
      "Epoch 01\tBatch 10330\t0.1563s/batch\ttrain_loss = 3.2768\n",
      "Epoch 01\tBatch 10340\t0.1545s/batch\ttrain_loss = 3.2067\n",
      "Epoch 01\tBatch 10350\t0.1530s/batch\ttrain_loss = 2.5079\n",
      "Epoch 01\tBatch 10350\ttest_loss = 3.0210\n",
      "Epoch 01\tBatch 10360\t0.1532s/batch\ttrain_loss = 2.8772\n",
      "Epoch 01\tBatch 10370\t0.1558s/batch\ttrain_loss = 3.0837\n",
      "Epoch 01\tBatch 10380\t0.1525s/batch\ttrain_loss = 2.4327\n",
      "Epoch 01\tBatch 10380\ttest_loss = 2.9994\n",
      "Epoch 01\tBatch 10390\t0.1547s/batch\ttrain_loss = 3.5131\n",
      "Epoch 01\tBatch 10400\t0.1521s/batch\ttrain_loss = 2.7364\n",
      "Epoch 01\tBatch 10410\t0.1523s/batch\ttrain_loss = 3.0266\n",
      "Epoch 01\tBatch 10410\ttest_loss = 2.9768\n",
      "Epoch 01\tBatch 10420\t0.1521s/batch\ttrain_loss = 2.8707\n",
      "Epoch 01\tBatch 10430\t0.1521s/batch\ttrain_loss = 3.1160\n",
      "Epoch 01\tBatch 10440\t0.1532s/batch\ttrain_loss = 3.0115\n",
      "Epoch 01\tBatch 10440\ttest_loss = 3.0105\n",
      "Epoch 01\tBatch 10450\t0.1521s/batch\ttrain_loss = 2.7765\n",
      "Epoch 01\tBatch 10460\t0.1662s/batch\ttrain_loss = 3.4880\n",
      "Epoch 01\tBatch 10470\t0.1641s/batch\ttrain_loss = 3.3696\n",
      "Epoch 01\tBatch 10470\ttest_loss = 2.9501\n",
      "Epoch 01\tBatch 10480\t0.1466s/batch\ttrain_loss = 2.6650\n",
      "Epoch 01\tBatch 10490\t0.1524s/batch\ttrain_loss = 3.2319\n",
      "Epoch 01\tBatch 10500\t0.1486s/batch\ttrain_loss = 3.6975\n",
      "Epoch 01\tBatch 10500\ttest_loss = 2.9898\n",
      "Epoch 01\tBatch 10510\t0.1501s/batch\ttrain_loss = 2.6539\n",
      "Epoch 01\tBatch 10520\t0.1578s/batch\ttrain_loss = 3.4714\n",
      "Epoch 01\tBatch 10530\t0.1521s/batch\ttrain_loss = 3.6328\n",
      "Epoch 01\tBatch 10530\ttest_loss = 2.9305\n",
      "new best loss (2.939783 -> 2.930515)\n",
      "Epoch 01\tBatch 10540\t0.1466s/batch\ttrain_loss = 2.4813\n",
      "Epoch 01\tBatch 10550\t0.1503s/batch\ttrain_loss = 3.8806\n",
      "Epoch 01\tBatch 10560\t0.1484s/batch\ttrain_loss = 3.5550\n",
      "Epoch 01\tBatch 10560\ttest_loss = 2.9394\n",
      "Epoch 01\tBatch 10570\t0.1480s/batch\ttrain_loss = 3.4310\n",
      "Epoch 01\tBatch 10580\t0.1464s/batch\ttrain_loss = 3.0560\n",
      "Epoch 01\tBatch 10590\t0.1458s/batch\ttrain_loss = 3.0686\n",
      "Epoch 01\tBatch 10590\ttest_loss = 2.9765\n",
      "Epoch 01\tBatch 10600\t0.1473s/batch\ttrain_loss = 3.0972\n",
      "Epoch 01\tBatch 10610\t0.1457s/batch\ttrain_loss = 2.8827\n",
      "Epoch 01\tBatch 10620\t0.1457s/batch\ttrain_loss = 2.6609\n",
      "Epoch 01\tBatch 10620\ttest_loss = 2.9756\n",
      "Epoch 01\tBatch 10630\t0.1463s/batch\ttrain_loss = 2.8804\n",
      "Epoch 01\tBatch 10640\t0.1450s/batch\ttrain_loss = 3.0311\n",
      "Epoch 01\tBatch 10650\t0.1460s/batch\ttrain_loss = 2.6198\n",
      "Epoch 01\tBatch 10650\ttest_loss = 3.0286\n",
      "Epoch 01\tBatch 10660\t0.1462s/batch\ttrain_loss = 2.5946\n",
      "Epoch 01\tBatch 10670\t0.1492s/batch\ttrain_loss = 3.2429\n",
      "Epoch 01\tBatch 10680\t0.1480s/batch\ttrain_loss = 3.4778\n",
      "Epoch 01\tBatch 10680\ttest_loss = 2.9450\n",
      "Epoch 01\tBatch 10690\t0.1463s/batch\ttrain_loss = 3.0306\n",
      "Epoch 01\tBatch 10700\t0.1462s/batch\ttrain_loss = 2.9294\n",
      "Epoch 01\tBatch 10710\t0.1451s/batch\ttrain_loss = 2.8215\n",
      "Epoch 01\tBatch 10710\ttest_loss = 3.0589\n",
      "Epoch 01\tBatch 10720\t0.1446s/batch\ttrain_loss = 2.2882\n",
      "Epoch 01\tBatch 10730\t0.1468s/batch\ttrain_loss = 2.4788\n",
      "Epoch 01\tBatch 10740\t0.1459s/batch\ttrain_loss = 2.7940\n",
      "Epoch 01\tBatch 10740\ttest_loss = 2.9618\n",
      "Epoch 01\tBatch 10750\t0.1533s/batch\ttrain_loss = 2.6121\n",
      "Epoch 01\tBatch 10760\t0.1450s/batch\ttrain_loss = 2.3376\n",
      "Epoch 01\tBatch 10770\t0.1454s/batch\ttrain_loss = 2.8105\n",
      "Epoch 01\tBatch 10770\ttest_loss = 2.9830\n",
      "Epoch 01\tBatch 10780\t0.1466s/batch\ttrain_loss = 3.0423\n",
      "Epoch 01\tBatch 10790\t0.1440s/batch\ttrain_loss = 2.3543\n",
      "Epoch 01\tBatch 10800\t0.1463s/batch\ttrain_loss = 3.0255\n",
      "Epoch 01\tBatch 10800\ttest_loss = 2.9489\n",
      "Epoch 01\tBatch 10810\t0.1467s/batch\ttrain_loss = 3.5163\n",
      "Epoch 01\tBatch 10820\t0.1486s/batch\ttrain_loss = 3.3255\n",
      "Epoch 01\tBatch 10830\t0.1433s/batch\ttrain_loss = 2.7063\n",
      "Epoch 01\tBatch 10830\ttest_loss = 3.0921\n",
      "Epoch 01\tBatch 10840\t0.1440s/batch\ttrain_loss = 2.7215\n",
      "Epoch 01\tBatch 10850\t0.1444s/batch\ttrain_loss = 2.6401\n",
      "Epoch 01\tBatch 10860\t0.1446s/batch\ttrain_loss = 2.4347\n",
      "Epoch 01\tBatch 10860\ttest_loss = 2.9833\n",
      "Epoch 01\tBatch 10870\t0.1459s/batch\ttrain_loss = 3.1175\n",
      "Epoch 01\tBatch 10880\t0.1427s/batch\ttrain_loss = 2.2594\n",
      "Epoch 01\tBatch 10890\t0.1448s/batch\ttrain_loss = 3.0439\n",
      "Epoch 01\tBatch 10890\ttest_loss = 3.0813\n",
      "Epoch 01\tBatch 10900\t0.1438s/batch\ttrain_loss = 2.5560\n",
      "Epoch 01\tBatch 10910\t0.1473s/batch\ttrain_loss = 2.7946\n",
      "Epoch 01\tBatch 10920\t0.1467s/batch\ttrain_loss = 2.7648\n",
      "Epoch 01\tBatch 10920\ttest_loss = 2.9715\n",
      "Epoch 01\tBatch 10930\t0.1532s/batch\ttrain_loss = 2.5204\n",
      "Epoch 01\tBatch 10940\t0.1498s/batch\ttrain_loss = 3.0440\n",
      "Epoch 01\tBatch 10950\t0.1462s/batch\ttrain_loss = 2.8057\n",
      "Epoch 01\tBatch 10950\ttest_loss = 2.9662\n",
      "Epoch 01\tBatch 10960\t0.1485s/batch\ttrain_loss = 3.8265\n",
      "Epoch 01\tBatch 10970\t0.1455s/batch\ttrain_loss = 3.3820\n",
      "Epoch 01\tBatch 10980\t0.1460s/batch\ttrain_loss = 3.0875\n",
      "Epoch 01\tBatch 10980\ttest_loss = 2.9687\n",
      "Epoch 01\tBatch 10990\t0.1454s/batch\ttrain_loss = 3.1583\n",
      "Epoch 01\tBatch 11000\t0.1454s/batch\ttrain_loss = 3.5706\n",
      "Epoch 01\tBatch 11010\t0.1463s/batch\ttrain_loss = 3.9209\n",
      "Epoch 01\tBatch 11010\ttest_loss = 2.9574\n",
      "Epoch 01\tBatch 11020\t0.1485s/batch\ttrain_loss = 3.4374\n",
      "Epoch 01\tBatch 11030\t0.1453s/batch\ttrain_loss = 3.2643\n",
      "Epoch 01\tBatch 11040\t0.1439s/batch\ttrain_loss = 2.8061\n",
      "Epoch 01\tBatch 11040\ttest_loss = 2.9639\n",
      "Epoch 01\tBatch 11050\t0.1443s/batch\ttrain_loss = 2.7421\n",
      "Epoch 01\tBatch 11060\t0.1475s/batch\ttrain_loss = 4.0466\n",
      "Epoch 01\tBatch 11070\t0.1440s/batch\ttrain_loss = 2.5551\n",
      "Epoch 01\tBatch 11070\ttest_loss = 2.9581\n",
      "Epoch 01\tBatch 11080\t0.1444s/batch\ttrain_loss = 2.6675\n",
      "Epoch 01\tBatch 11090\t0.1451s/batch\ttrain_loss = 2.9999\n",
      "Epoch 01\tBatch 11100\t0.1428s/batch\ttrain_loss = 2.1556\n",
      "Epoch 01\tBatch 11100\ttest_loss = 2.9358\n",
      "Epoch 01\tBatch 11110\t0.1464s/batch\ttrain_loss = 2.9488\n",
      "Epoch 01\tBatch 11120\t0.1454s/batch\ttrain_loss = 2.4717\n",
      "Epoch 01\tBatch 11130\t0.1477s/batch\ttrain_loss = 3.1751\n",
      "Epoch 01\tBatch 11130\ttest_loss = 2.9274\n",
      "new best loss (2.930515 -> 2.927417)\n",
      "Epoch 01\tBatch 11140\t0.1495s/batch\ttrain_loss = 3.3585\n",
      "Epoch 01\tBatch 11150\t0.1449s/batch\ttrain_loss = 2.6666\n",
      "Epoch 01\tBatch 11160\t0.1455s/batch\ttrain_loss = 3.0270\n",
      "Epoch 01\tBatch 11160\ttest_loss = 2.9469\n",
      "Epoch 01\tBatch 11170\t0.1473s/batch\ttrain_loss = 4.0722\n",
      "Epoch 01\tBatch 11180\t0.1489s/batch\ttrain_loss = 2.5239\n",
      "Epoch 01\tBatch 11190\t0.1487s/batch\ttrain_loss = 2.9030\n",
      "Epoch 01\tBatch 11190\ttest_loss = 2.9374\n",
      "Epoch 01\tBatch 11200\t0.1456s/batch\ttrain_loss = 2.9727\n",
      "Epoch 01\tBatch 11210\t0.1465s/batch\ttrain_loss = 3.4351\n",
      "Epoch 01\tBatch 11220\t0.1458s/batch\ttrain_loss = 3.2535\n",
      "Epoch 01\tBatch 11220\ttest_loss = 2.9473\n",
      "Epoch 01\tBatch 11230\t0.1440s/batch\ttrain_loss = 2.9532\n",
      "Epoch 01\tBatch 11240\t0.1505s/batch\ttrain_loss = 3.7503\n",
      "Epoch 01\tBatch 11250\t0.1434s/batch\ttrain_loss = 2.5131\n",
      "Epoch 01\tBatch 11250\ttest_loss = 2.9320\n",
      "Epoch 01\tBatch 11260\t0.1440s/batch\ttrain_loss = 2.6864\n",
      "Epoch 01\tBatch 11270\t0.1438s/batch\ttrain_loss = 2.6823\n",
      "Epoch 01\tBatch 11280\t0.1459s/batch\ttrain_loss = 2.9303\n",
      "Epoch 01\tBatch 11280\ttest_loss = 2.9569\n",
      "Epoch 01\tBatch 11290\t0.1446s/batch\ttrain_loss = 2.8217\n",
      "Epoch 01\tBatch 11300\t0.1446s/batch\ttrain_loss = 2.9184\n",
      "Epoch 01\tBatch 11310\t0.1464s/batch\ttrain_loss = 3.1255\n",
      "Epoch 01\tBatch 11310\ttest_loss = 2.9273\n",
      "new best loss (2.927417 -> 2.927256)\n",
      "Epoch 01\tBatch 11320\t0.1462s/batch\ttrain_loss = 3.6398\n",
      "Epoch 01\tBatch 11330\t0.1450s/batch\ttrain_loss = 2.9154\n",
      "Epoch 01\tBatch 11340\t0.1447s/batch\ttrain_loss = 2.7888\n",
      "Epoch 01\tBatch 11340\ttest_loss = 2.9538\n",
      "Epoch 01\tBatch 11350\t0.1474s/batch\ttrain_loss = 2.6955\n",
      "Epoch 01\tBatch 11360\t0.1449s/batch\ttrain_loss = 2.4582\n",
      "Epoch 01\tBatch 11370\t0.1449s/batch\ttrain_loss = 3.2111\n",
      "Epoch 01\tBatch 11370\ttest_loss = 3.0210\n",
      "Epoch 01\tBatch 11380\t0.1445s/batch\ttrain_loss = 2.5660\n",
      "Epoch 01\tBatch 11390\t0.1457s/batch\ttrain_loss = 3.1793\n",
      "Epoch 01\tBatch 11400\t0.1439s/batch\ttrain_loss = 2.6999\n",
      "Epoch 01\tBatch 11400\ttest_loss = 3.0063\n",
      "Epoch 01\tBatch 11410\t0.1443s/batch\ttrain_loss = 2.5000\n",
      "Epoch 01\tBatch 11420\t0.1454s/batch\ttrain_loss = 2.7805\n",
      "Epoch 01\tBatch 11430\t0.1450s/batch\ttrain_loss = 2.8234\n",
      "Epoch 01\tBatch 11430\ttest_loss = 2.9930\n",
      "Epoch 01\tBatch 11440\t0.1430s/batch\ttrain_loss = 2.3669\n",
      "Epoch 01\tBatch 11450\t0.1461s/batch\ttrain_loss = 3.7864\n",
      "Epoch 01\tBatch 11460\t0.1450s/batch\ttrain_loss = 3.1002\n",
      "Epoch 01\tBatch 11460\ttest_loss = 3.1608\n",
      "Epoch 01\tBatch 11470\t0.1479s/batch\ttrain_loss = 4.0635\n",
      "Epoch 01\tBatch 11480\t0.1434s/batch\ttrain_loss = 2.2534\n",
      "Epoch 01\tBatch 11490\t0.1478s/batch\ttrain_loss = 2.5535\n",
      "Epoch 01\tBatch 11490\ttest_loss = 3.0300\n",
      "Epoch 01\tBatch 11500\t0.1455s/batch\ttrain_loss = 2.7244\n",
      "Epoch 01\tBatch 11510\t0.1450s/batch\ttrain_loss = 3.0812\n",
      "Epoch 01\tBatch 11520\t0.1454s/batch\ttrain_loss = 2.3067\n",
      "Epoch 01\tBatch 11520\ttest_loss = 2.9340\n",
      "Epoch 01\tBatch 11530\t0.1479s/batch\ttrain_loss = 3.4427\n",
      "Epoch 01\tBatch 11540\t0.1437s/batch\ttrain_loss = 2.5756\n",
      "Epoch 01\tBatch 11550\t0.1445s/batch\ttrain_loss = 2.4713\n",
      "Epoch 01\tBatch 11550\ttest_loss = 2.9799\n",
      "Epoch 01\tBatch 11560\t0.1440s/batch\ttrain_loss = 2.2463\n",
      "Epoch 01\tBatch 11570\t0.1438s/batch\ttrain_loss = 2.5235\n",
      "Epoch 01\tBatch 11580\t0.1439s/batch\ttrain_loss = 2.6124\n",
      "Epoch 01\tBatch 11580\ttest_loss = 2.9409\n",
      "Epoch 01\tBatch 11590\t0.1491s/batch\ttrain_loss = 3.3573\n",
      "Epoch 01\tBatch 11600\t0.1464s/batch\ttrain_loss = 2.7143\n",
      "Epoch 01\tBatch 11610\t0.1443s/batch\ttrain_loss = 2.7149\n",
      "Epoch 01\tBatch 11610\ttest_loss = 2.9382\n",
      "Epoch 01\tBatch 11620\t0.1479s/batch\ttrain_loss = 4.0718\n",
      "Epoch 01\tBatch 11630\t0.1463s/batch\ttrain_loss = 3.3927\n",
      "Epoch 01\tBatch 11640\t0.1475s/batch\ttrain_loss = 3.8174\n",
      "Epoch 01\tBatch 11640\ttest_loss = 2.9822\n",
      "Epoch 01\tBatch 11650\t0.1470s/batch\ttrain_loss = 3.8503\n",
      "Epoch 01\tBatch 11660\t0.1461s/batch\ttrain_loss = 3.0395\n",
      "Epoch 01\tBatch 11670\t0.1449s/batch\ttrain_loss = 2.8470\n",
      "Epoch 01\tBatch 11670\ttest_loss = 2.9568\n",
      "Epoch 01\tBatch 11680\t0.1443s/batch\ttrain_loss = 2.7863\n",
      "Epoch 01\tBatch 11690\t0.1451s/batch\ttrain_loss = 2.8107\n",
      "Epoch 01\tBatch 11700\t0.1471s/batch\ttrain_loss = 3.5936\n",
      "Epoch 01\tBatch 11700\ttest_loss = 2.9843\n",
      "Epoch 01\tBatch 11710\t0.1462s/batch\ttrain_loss = 3.3437\n",
      "Epoch 01\tBatch 11720\t0.1437s/batch\ttrain_loss = 2.4894\n",
      "Epoch 01\tBatch 11730\t0.1472s/batch\ttrain_loss = 3.3058\n",
      "Epoch 01\tBatch 11730\ttest_loss = 2.9650\n",
      "Epoch 01\tBatch 11740\t0.1445s/batch\ttrain_loss = 2.7837\n",
      "Epoch 01\tBatch 11750\t0.1445s/batch\ttrain_loss = 2.7714\n",
      "Epoch 01\tBatch 11760\t0.1463s/batch\ttrain_loss = 3.5537\n",
      "Epoch 01\tBatch 11760\ttest_loss = 2.9354\n",
      "Epoch 01\tBatch 11770\t0.1445s/batch\ttrain_loss = 2.8685\n",
      "Epoch 01\tBatch 11780\t0.1429s/batch\ttrain_loss = 2.1973\n",
      "Epoch 01\tBatch 11790\t0.1461s/batch\ttrain_loss = 3.2521\n",
      "Epoch 01\tBatch 11790\ttest_loss = 2.9287\n",
      "Epoch 01\tBatch 11800\t0.1436s/batch\ttrain_loss = 2.4765\n",
      "Epoch 01\tBatch 11810\t0.1446s/batch\ttrain_loss = 2.8097\n",
      "Epoch 01\tBatch 11820\t0.1460s/batch\ttrain_loss = 3.1976\n",
      "Epoch 01\tBatch 11820\ttest_loss = 2.9403\n",
      "Epoch 01\tBatch 11830\t0.1462s/batch\ttrain_loss = 3.1215\n",
      "Epoch 01\tBatch 11840\t0.1451s/batch\ttrain_loss = 2.5259\n",
      "Epoch 01\tBatch 11850\t0.1455s/batch\ttrain_loss = 2.8442\n",
      "Epoch 01\tBatch 11850\ttest_loss = 2.9458\n",
      "Epoch 02\tBatch 11860\t0.1450s/batch\ttrain_loss = 3.6191\n",
      "Epoch 02\tBatch 11870\t0.1464s/batch\ttrain_loss = 3.8800\n",
      "Epoch 02\tBatch 11880\t0.1475s/batch\ttrain_loss = 3.7766\n",
      "Epoch 02\tBatch 11880\ttest_loss = 2.9537\n",
      "Epoch 02\tBatch 11890\t0.1458s/batch\ttrain_loss = 3.2177\n",
      "Epoch 02\tBatch 11900\t0.1465s/batch\ttrain_loss = 3.4437\n",
      "Epoch 02\tBatch 11910\t0.1448s/batch\ttrain_loss = 2.6683\n",
      "Epoch 02\tBatch 11910\ttest_loss = 2.9262\n",
      "new best loss (2.927256 -> 2.926214)\n",
      "Epoch 02\tBatch 11920\t0.1459s/batch\ttrain_loss = 2.8395\n",
      "Epoch 02\tBatch 11930\t0.1453s/batch\ttrain_loss = 2.8001\n",
      "Epoch 02\tBatch 11940\t0.1451s/batch\ttrain_loss = 2.9446\n",
      "Epoch 02\tBatch 11940\ttest_loss = 2.9147\n",
      "new best loss (2.926214 -> 2.914672)\n",
      "Epoch 02\tBatch 11950\t0.1446s/batch\ttrain_loss = 2.5159\n",
      "Epoch 02\tBatch 11960\t0.1467s/batch\ttrain_loss = 3.4706\n",
      "Epoch 02\tBatch 11970\t0.1456s/batch\ttrain_loss = 2.8850\n",
      "Epoch 02\tBatch 11970\ttest_loss = 2.9920\n",
      "Epoch 02\tBatch 11980\t0.1443s/batch\ttrain_loss = 2.5754\n",
      "Epoch 02\tBatch 11990\t0.1442s/batch\ttrain_loss = 2.6191\n",
      "Epoch 02\tBatch 12000\t0.1458s/batch\ttrain_loss = 3.0916\n",
      "Epoch 02\tBatch 12000\ttest_loss = 2.9091\n",
      "new best loss (2.914672 -> 2.909126)\n",
      "Epoch 02\tBatch 12010\t0.1471s/batch\ttrain_loss = 3.3542\n",
      "Epoch 02\tBatch 12020\t0.1456s/batch\ttrain_loss = 3.0034\n",
      "Epoch 02\tBatch 12030\t0.1453s/batch\ttrain_loss = 2.8818\n",
      "Epoch 02\tBatch 12030\ttest_loss = 2.9777\n",
      "Epoch 02\tBatch 12040\t0.1470s/batch\ttrain_loss = 2.5716\n",
      "Epoch 02\tBatch 12050\t0.1471s/batch\ttrain_loss = 3.5712\n",
      "Epoch 02\tBatch 12060\t0.1473s/batch\ttrain_loss = 3.5307\n",
      "Epoch 02\tBatch 12060\ttest_loss = 2.9278\n",
      "Epoch 02\tBatch 12070\t0.1455s/batch\ttrain_loss = 2.4161\n",
      "Epoch 02\tBatch 12080\t0.1467s/batch\ttrain_loss = 3.2185\n",
      "Epoch 02\tBatch 12090\t0.1472s/batch\ttrain_loss = 3.5902\n",
      "Epoch 02\tBatch 12090\ttest_loss = 2.9251\n",
      "Epoch 02\tBatch 12100\t0.1448s/batch\ttrain_loss = 2.7199\n",
      "Epoch 02\tBatch 12110\t0.1430s/batch\ttrain_loss = 2.0840\n",
      "Epoch 02\tBatch 12120\t0.1453s/batch\ttrain_loss = 2.9230\n",
      "Epoch 02\tBatch 12120\ttest_loss = 2.9128\n",
      "Epoch 02\tBatch 12130\t0.1468s/batch\ttrain_loss = 3.0933\n",
      "Epoch 02\tBatch 12140\t0.1452s/batch\ttrain_loss = 2.5910\n",
      "Epoch 02\tBatch 12150\t0.1472s/batch\ttrain_loss = 3.5803\n",
      "Epoch 02\tBatch 12150\ttest_loss = 2.9469\n",
      "Epoch 02\tBatch 12160\t0.1462s/batch\ttrain_loss = 3.3404\n",
      "Epoch 02\tBatch 12170\t0.1458s/batch\ttrain_loss = 3.0560\n",
      "Epoch 02\tBatch 12180\t0.1445s/batch\ttrain_loss = 2.5551\n",
      "Epoch 02\tBatch 12180\ttest_loss = 2.9133\n",
      "Epoch 02\tBatch 12190\t0.1473s/batch\ttrain_loss = 3.5302\n",
      "Epoch 02\tBatch 12200\t0.1464s/batch\ttrain_loss = 3.2776\n",
      "Epoch 02\tBatch 12210\t0.1455s/batch\ttrain_loss = 3.0683\n",
      "Epoch 02\tBatch 12210\ttest_loss = 2.8842\n",
      "new best loss (2.909126 -> 2.884184)\n",
      "Epoch 02\tBatch 12220\t0.1479s/batch\ttrain_loss = 4.1949\n",
      "Epoch 02\tBatch 12230\t0.1468s/batch\ttrain_loss = 3.7508\n",
      "Epoch 02\tBatch 12240\t0.1459s/batch\ttrain_loss = 3.2911\n",
      "Epoch 02\tBatch 12240\ttest_loss = 3.0232\n",
      "Epoch 02\tBatch 12250\t0.1460s/batch\ttrain_loss = 3.4424\n",
      "Epoch 02\tBatch 12260\t0.1472s/batch\ttrain_loss = 3.4309\n",
      "Epoch 02\tBatch 12270\t0.1461s/batch\ttrain_loss = 3.4376\n",
      "Epoch 02\tBatch 12270\ttest_loss = 2.9895\n",
      "Epoch 02\tBatch 12280\t0.1453s/batch\ttrain_loss = 2.8330\n",
      "Epoch 02\tBatch 12290\t0.1462s/batch\ttrain_loss = 3.2255\n",
      "Epoch 02\tBatch 12300\t0.1462s/batch\ttrain_loss = 3.1280\n",
      "Epoch 02\tBatch 12300\ttest_loss = 2.9441\n",
      "Epoch 02\tBatch 12310\t0.1453s/batch\ttrain_loss = 2.5559\n",
      "Epoch 02\tBatch 12320\t0.1483s/batch\ttrain_loss = 3.2016\n",
      "Epoch 02\tBatch 12330\t0.1459s/batch\ttrain_loss = 3.1675\n",
      "Epoch 02\tBatch 12330\ttest_loss = 2.9706\n",
      "Epoch 02\tBatch 12340\t0.1466s/batch\ttrain_loss = 3.5648\n",
      "Epoch 02\tBatch 12350\t0.1455s/batch\ttrain_loss = 2.9581\n",
      "Epoch 02\tBatch 12360\t0.1470s/batch\ttrain_loss = 3.7629\n",
      "Epoch 02\tBatch 12360\ttest_loss = 2.9172\n",
      "Epoch 02\tBatch 12370\t0.1465s/batch\ttrain_loss = 3.2852\n",
      "Epoch 02\tBatch 12380\t0.1454s/batch\ttrain_loss = 2.4940\n",
      "Epoch 02\tBatch 12390\t0.1452s/batch\ttrain_loss = 2.6575\n",
      "Epoch 02\tBatch 12390\ttest_loss = 2.9092\n",
      "Epoch 02\tBatch 12400\t0.1441s/batch\ttrain_loss = 2.3290\n",
      "Epoch 02\tBatch 12410\t0.1442s/batch\ttrain_loss = 2.7300\n",
      "Epoch 02\tBatch 12420\t0.1442s/batch\ttrain_loss = 2.5903\n",
      "Epoch 02\tBatch 12420\ttest_loss = 2.9056\n",
      "Epoch 02\tBatch 12430\t0.1468s/batch\ttrain_loss = 3.6359\n",
      "Epoch 02\tBatch 12440\t0.1484s/batch\ttrain_loss = 3.2581\n",
      "Epoch 02\tBatch 12450\t0.1452s/batch\ttrain_loss = 3.0644\n",
      "Epoch 02\tBatch 12450\ttest_loss = 2.9655\n",
      "Epoch 02\tBatch 12460\t0.1449s/batch\ttrain_loss = 2.7026\n",
      "Epoch 02\tBatch 12470\t0.1457s/batch\ttrain_loss = 2.8880\n",
      "Epoch 02\tBatch 12480\t0.1574s/batch\ttrain_loss = 4.1443\n",
      "Epoch 02\tBatch 12480\ttest_loss = 2.9402\n",
      "Epoch 02\tBatch 12490\t0.1493s/batch\ttrain_loss = 2.7968\n",
      "Epoch 02\tBatch 12500\t0.1515s/batch\ttrain_loss = 3.5812\n",
      "Epoch 02\tBatch 12510\t0.1507s/batch\ttrain_loss = 3.5590\n",
      "Epoch 02\tBatch 12510\ttest_loss = 2.9244\n",
      "Epoch 02\tBatch 12520\t0.1486s/batch\ttrain_loss = 2.7580\n",
      "Epoch 02\tBatch 12530\t0.1470s/batch\ttrain_loss = 2.9506\n",
      "Epoch 02\tBatch 12540\t0.1480s/batch\ttrain_loss = 2.4704\n",
      "Epoch 02\tBatch 12540\ttest_loss = 2.9001\n",
      "Epoch 02\tBatch 12550\t0.1473s/batch\ttrain_loss = 3.3140\n",
      "Epoch 02\tBatch 12560\t0.1473s/batch\ttrain_loss = 3.3441\n",
      "Epoch 02\tBatch 12570\t0.1463s/batch\ttrain_loss = 2.7124\n",
      "Epoch 02\tBatch 12570\ttest_loss = 2.9151\n",
      "Epoch 02\tBatch 12580\t0.1545s/batch\ttrain_loss = 3.1130\n",
      "Epoch 02\tBatch 12590\t0.1504s/batch\ttrain_loss = 3.0812\n",
      "Epoch 02\tBatch 12600\t0.1470s/batch\ttrain_loss = 2.9158\n",
      "Epoch 02\tBatch 12600\ttest_loss = 2.9002\n",
      "Epoch 02\tBatch 12610\t0.1461s/batch\ttrain_loss = 2.6150\n",
      "Epoch 02\tBatch 12620\t0.1477s/batch\ttrain_loss = 3.1369\n",
      "Epoch 02\tBatch 12630\t0.1481s/batch\ttrain_loss = 3.4569\n",
      "Epoch 02\tBatch 12630\ttest_loss = 2.9383\n",
      "Epoch 02\tBatch 12640\t0.1442s/batch\ttrain_loss = 2.3158\n",
      "Epoch 02\tBatch 12650\t0.1538s/batch\ttrain_loss = 3.0200\n",
      "Epoch 02\tBatch 12660\t0.1469s/batch\ttrain_loss = 3.2039\n",
      "Epoch 02\tBatch 12660\ttest_loss = 2.9478\n",
      "Epoch 02\tBatch 12670\t0.1474s/batch\ttrain_loss = 3.0524\n",
      "Epoch 02\tBatch 12680\t0.1472s/batch\ttrain_loss = 3.4199\n",
      "Epoch 02\tBatch 12690\t0.1526s/batch\ttrain_loss = 3.6729\n",
      "Epoch 02\tBatch 12690\ttest_loss = 2.9529\n",
      "Epoch 02\tBatch 12700\t0.1530s/batch\ttrain_loss = 2.6850\n",
      "Epoch 02\tBatch 12710\t0.1484s/batch\ttrain_loss = 3.3197\n",
      "Epoch 02\tBatch 12720\t0.1472s/batch\ttrain_loss = 2.5583\n",
      "Epoch 02\tBatch 12720\ttest_loss = 2.9756\n",
      "Epoch 02\tBatch 12730\t0.1453s/batch\ttrain_loss = 2.7773\n",
      "Epoch 02\tBatch 12740\t0.1445s/batch\ttrain_loss = 2.6194\n",
      "Epoch 02\tBatch 12750\t0.1452s/batch\ttrain_loss = 2.5767\n",
      "Epoch 02\tBatch 12750\ttest_loss = 2.9121\n",
      "Epoch 02\tBatch 12760\t0.1458s/batch\ttrain_loss = 3.0034\n",
      "Epoch 02\tBatch 12770\t0.1462s/batch\ttrain_loss = 2.9393\n",
      "Epoch 02\tBatch 12780\t0.1453s/batch\ttrain_loss = 2.6300\n",
      "Epoch 02\tBatch 12780\ttest_loss = 2.9377\n",
      "Epoch 02\tBatch 12790\t0.1458s/batch\ttrain_loss = 2.6166\n",
      "Epoch 02\tBatch 12800\t0.1457s/batch\ttrain_loss = 2.6562\n",
      "Epoch 02\tBatch 12810\t0.1473s/batch\ttrain_loss = 3.6591\n",
      "Epoch 02\tBatch 12810\ttest_loss = 2.9041\n",
      "converged after 2 epochs (lowest achieved loss: 2.8842)\n",
      "loading best checkpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([10.908362197875977,\n",
       "  7.960996103286743,\n",
       "  7.473121881484985,\n",
       "  6.43770432472229,\n",
       "  5.8635783195495605,\n",
       "  5.67886176109314,\n",
       "  5.700396394729614,\n",
       "  5.604948711395264,\n",
       "  5.64202241897583,\n",
       "  5.015197682380676,\n",
       "  6.447308301925659,\n",
       "  5.487941932678223,\n",
       "  4.994870853424072,\n",
       "  5.2878772735595705,\n",
       "  5.577517032623291,\n",
       "  5.978104734420777,\n",
       "  5.560517168045044,\n",
       "  4.919181227684021,\n",
       "  5.108117723464966,\n",
       "  6.589050149917602,\n",
       "  5.573911523818969,\n",
       "  4.809597206115723,\n",
       "  5.549636125564575,\n",
       "  5.71639997959137,\n",
       "  4.929488158226013,\n",
       "  4.116895723342895,\n",
       "  5.124610590934753,\n",
       "  5.227909851074219,\n",
       "  4.887757086753846,\n",
       "  5.389058804512024,\n",
       "  5.828646898269653,\n",
       "  4.889252758026123,\n",
       "  4.511975026130676,\n",
       "  5.610156583786011,\n",
       "  5.362890672683716,\n",
       "  4.957969427108765,\n",
       "  6.647210216522216,\n",
       "  5.781266880035401,\n",
       "  4.842542123794556,\n",
       "  5.532192373275757,\n",
       "  5.650252103805542,\n",
       "  5.113083553314209,\n",
       "  4.639243340492248,\n",
       "  5.073835611343384,\n",
       "  4.904958629608155,\n",
       "  4.416230010986328,\n",
       "  5.251512670516968,\n",
       "  4.993745183944702,\n",
       "  5.434408187866211,\n",
       "  4.982915019989013,\n",
       "  5.5601152896881105,\n",
       "  4.6329988718032835,\n",
       "  4.264933705329895,\n",
       "  4.117644548416138,\n",
       "  4.107652354240417,\n",
       "  4.529441595077515,\n",
       "  4.030236172676086,\n",
       "  5.711321020126343,\n",
       "  4.574063014984131,\n",
       "  4.65837836265564,\n",
       "  4.308151268959046,\n",
       "  4.759126138687134,\n",
       "  5.665521574020386,\n",
       "  4.364042520523071,\n",
       "  5.552338194847107,\n",
       "  4.976407361030579,\n",
       "  4.516760849952698,\n",
       "  4.322614073753357,\n",
       "  4.064244890213013,\n",
       "  4.879856395721435,\n",
       "  4.850086903572082,\n",
       "  4.584285593032837,\n",
       "  4.496483278274536,\n",
       "  4.886938667297363,\n",
       "  4.21206567287445,\n",
       "  4.299493432044983,\n",
       "  4.76278669834137,\n",
       "  5.156318283081054,\n",
       "  3.443008637428284,\n",
       "  4.6749810695648195,\n",
       "  4.30517213344574,\n",
       "  4.8797667741775514,\n",
       "  4.991884350776672,\n",
       "  5.267019939422608,\n",
       "  3.992453694343567,\n",
       "  4.896264982223511,\n",
       "  3.751645040512085,\n",
       "  3.817043161392212,\n",
       "  4.031593227386475,\n",
       "  3.971409296989441,\n",
       "  4.404537916183472,\n",
       "  4.291010236740112,\n",
       "  3.841662788391113,\n",
       "  4.135462713241577,\n",
       "  3.895313763618469,\n",
       "  4.959276700019837,\n",
       "  3.6334806442260743,\n",
       "  3.9945653915405273,\n",
       "  4.055153441429138,\n",
       "  4.693620491027832,\n",
       "  4.699931263923645,\n",
       "  5.160374426841736,\n",
       "  3.593681573867798,\n",
       "  5.079303669929504,\n",
       "  4.155178713798523,\n",
       "  4.533427381515503,\n",
       "  4.771486902236939,\n",
       "  4.4787369012832645,\n",
       "  4.127382040023804,\n",
       "  4.47814257144928,\n",
       "  4.266776847839355,\n",
       "  4.20877161026001,\n",
       "  4.899814963340759,\n",
       "  4.274425029754639,\n",
       "  3.7919575929641725,\n",
       "  3.338553762435913,\n",
       "  3.4074349641799926,\n",
       "  4.7469346761703495,\n",
       "  4.357293701171875,\n",
       "  4.262309932708741,\n",
       "  3.7234099864959718,\n",
       "  3.6742172479629516,\n",
       "  4.0931254625320435,\n",
       "  4.0308098793029785,\n",
       "  3.8295255661010743,\n",
       "  4.532924938201904,\n",
       "  4.759078121185302,\n",
       "  4.0096210718154905,\n",
       "  3.6168121099472046,\n",
       "  3.2075390577316285,\n",
       "  3.2511426210403442,\n",
       "  4.246242666244507,\n",
       "  3.9509256362915037,\n",
       "  5.147746610641479,\n",
       "  3.376445245742798,\n",
       "  2.984686517715454,\n",
       "  3.479785752296448,\n",
       "  3.4901225090026857,\n",
       "  4.465428066253662,\n",
       "  4.2865235090255736,\n",
       "  4.402325582504273,\n",
       "  4.581750082969665,\n",
       "  4.2641295194625854,\n",
       "  4.178743076324463,\n",
       "  4.465504050254822,\n",
       "  3.0772112607955933,\n",
       "  3.7992518663406374,\n",
       "  2.5532971262931823,\n",
       "  4.041341257095337,\n",
       "  4.164496278762817,\n",
       "  3.643903613090515,\n",
       "  4.310335350036621,\n",
       "  3.191846323013306,\n",
       "  3.242390441894531,\n",
       "  4.072101616859436,\n",
       "  3.3294023990631105,\n",
       "  3.300532031059265,\n",
       "  2.8161609172821045,\n",
       "  3.197401022911072,\n",
       "  3.897203803062439,\n",
       "  4.189962482452392,\n",
       "  3.8341073513031008,\n",
       "  3.6195871114730833,\n",
       "  5.205766677856445,\n",
       "  3.694935178756714,\n",
       "  4.012834572792054,\n",
       "  3.04227032661438,\n",
       "  3.5787240266799927,\n",
       "  3.489456129074097,\n",
       "  4.564638829231262,\n",
       "  4.180290937423706,\n",
       "  4.729341077804565,\n",
       "  3.5101922035217283,\n",
       "  4.582018566131592,\n",
       "  4.311473727226257,\n",
       "  3.154783034324646,\n",
       "  3.7361491918563843,\n",
       "  4.18193244934082,\n",
       "  4.8876076459884645,\n",
       "  4.807451367378235,\n",
       "  4.0219562292099,\n",
       "  3.738630270957947,\n",
       "  4.058467769622803,\n",
       "  4.163198041915893,\n",
       "  4.24140350818634,\n",
       "  3.9140740633010864,\n",
       "  3.212453532218933,\n",
       "  2.786876606941223,\n",
       "  3.7723825693130495,\n",
       "  4.016790747642517,\n",
       "  3.160154414176941,\n",
       "  3.4804991245269776,\n",
       "  3.26954607963562,\n",
       "  3.1119163036346436,\n",
       "  2.7915162682533263,\n",
       "  3.6835659027099608,\n",
       "  3.4808364868164063,\n",
       "  4.451229906082153,\n",
       "  4.698537921905517,\n",
       "  3.551853156089783,\n",
       "  2.9015183806419373,\n",
       "  3.3195811033248903,\n",
       "  3.896727442741394,\n",
       "  4.532797908782959,\n",
       "  3.529363012313843,\n",
       "  3.228608798980713,\n",
       "  2.659020984172821,\n",
       "  3.821609950065613,\n",
       "  3.378930449485779,\n",
       "  3.4634017944335938,\n",
       "  4.176012563705444,\n",
       "  4.173456501960755,\n",
       "  4.0586347579956055,\n",
       "  4.032058238983154,\n",
       "  3.9733436822891237,\n",
       "  4.011102390289307,\n",
       "  3.9581945657730104,\n",
       "  4.017230868339539,\n",
       "  3.3720778465270995,\n",
       "  5.2388794898986815,\n",
       "  4.331333494186401,\n",
       "  3.484653413295746,\n",
       "  3.746327829360962,\n",
       "  3.645853281021118,\n",
       "  3.981732225418091,\n",
       "  4.062470293045044,\n",
       "  3.7325420141220094,\n",
       "  3.6747830867767335,\n",
       "  3.952384090423584,\n",
       "  4.4553789615631105,\n",
       "  4.0487521409988405,\n",
       "  4.222215032577514,\n",
       "  3.523137927055359,\n",
       "  4.4314371109008786,\n",
       "  3.356289005279541,\n",
       "  3.686431050300598,\n",
       "  3.1448944568634034,\n",
       "  3.776440978050232,\n",
       "  3.256214714050293,\n",
       "  3.5502487659454345,\n",
       "  4.13582718372345,\n",
       "  3.6503081560134887,\n",
       "  3.5083375930786134,\n",
       "  3.7106082916259764,\n",
       "  3.2726970672607423,\n",
       "  3.154784917831421,\n",
       "  3.4938561797142027,\n",
       "  3.113529992103577,\n",
       "  3.3586881875991823,\n",
       "  3.9069051504135133,\n",
       "  3.6658867955207826,\n",
       "  4.931845998764038,\n",
       "  2.8170903444290163,\n",
       "  3.2465855360031126,\n",
       "  3.6375661373138426,\n",
       "  4.018023896217346,\n",
       "  4.562080955505371,\n",
       "  3.461459589004517,\n",
       "  3.5986324548721313,\n",
       "  4.716811776161194,\n",
       "  3.7388443470001222,\n",
       "  3.702539086341858,\n",
       "  3.9769427299499513,\n",
       "  3.6604620456695556,\n",
       "  3.5966182470321657,\n",
       "  3.3777625799179076,\n",
       "  4.459763407707214,\n",
       "  3.2183898210525514,\n",
       "  3.694767713546753,\n",
       "  3.7751450061798097,\n",
       "  3.3929859399795532,\n",
       "  3.6614452481269835,\n",
       "  4.343869209289551,\n",
       "  2.841029953956604,\n",
       "  3.35235595703125,\n",
       "  3.2263221740722656,\n",
       "  3.715371322631836,\n",
       "  3.517901587486267,\n",
       "  4.581947088241577,\n",
       "  3.8057782649993896,\n",
       "  3.9371373653411865,\n",
       "  3.5961873054504396,\n",
       "  2.867555284500122,\n",
       "  3.3921084642410277,\n",
       "  3.517481529712677,\n",
       "  3.7489236116409304,\n",
       "  3.508606767654419,\n",
       "  4.14220769405365,\n",
       "  3.6384538888931273,\n",
       "  3.807678985595703,\n",
       "  3.7414711475372315,\n",
       "  3.187675154209137,\n",
       "  2.807308721542358,\n",
       "  4.223495745658875,\n",
       "  3.658151888847351,\n",
       "  3.260277819633484,\n",
       "  3.0446865558624268,\n",
       "  3.4904321908950804,\n",
       "  2.5102030992507935,\n",
       "  3.611587572097778,\n",
       "  3.8208689212799074,\n",
       "  3.230270767211914,\n",
       "  3.198969268798828,\n",
       "  4.247521615028381,\n",
       "  3.447399306297302,\n",
       "  3.384202551841736,\n",
       "  3.7073020935058594,\n",
       "  4.269488763809204,\n",
       "  4.0416666030883786,\n",
       "  3.2571356296539307,\n",
       "  3.3089584589004515,\n",
       "  2.929717516899109,\n",
       "  3.4996387481689455,\n",
       "  3.1213228702545166,\n",
       "  3.900606155395508,\n",
       "  3.3723955392837524,\n",
       "  2.6357243061065674,\n",
       "  3.760469579696655,\n",
       "  2.91554479598999,\n",
       "  3.802274966239929,\n",
       "  2.7403458952903748,\n",
       "  3.425157296657562,\n",
       "  3.3895564794540407,\n",
       "  3.1578328609466553,\n",
       "  3.7121715784072875,\n",
       "  3.594244456291199,\n",
       "  3.9276074171066284,\n",
       "  3.669847011566162,\n",
       "  3.5532207012176515,\n",
       "  3.45274658203125,\n",
       "  3.4741066336631774,\n",
       "  2.871429753303528,\n",
       "  3.545394277572632,\n",
       "  3.4174176454544067,\n",
       "  4.1468147277832035,\n",
       "  2.5407747626304626,\n",
       "  3.6965030908584593,\n",
       "  3.327851963043213,\n",
       "  3.197171187400818,\n",
       "  2.9985644698143004,\n",
       "  3.070323646068573,\n",
       "  3.2708556175231935,\n",
       "  3.0272947549819946,\n",
       "  3.275847387313843,\n",
       "  3.5043798685073853,\n",
       "  3.579832899570465,\n",
       "  3.628770899772644,\n",
       "  3.3677131652832033,\n",
       "  2.971725010871887,\n",
       "  3.2342328786849976,\n",
       "  2.9717602372169494,\n",
       "  3.7849915504455565,\n",
       "  3.3788936138153076,\n",
       "  3.537379789352417,\n",
       "  3.4339296579360963,\n",
       "  3.281995475292206,\n",
       "  4.298057603836059,\n",
       "  4.131611800193786,\n",
       "  3.294472169876099,\n",
       "  3.560884475708008,\n",
       "  3.663141894340515,\n",
       "  3.1528356075286865,\n",
       "  3.3635573387145996,\n",
       "  3.8842631816864013,\n",
       "  2.815725326538086,\n",
       "  3.458205318450928,\n",
       "  3.7715773582458496,\n",
       "  3.2517444133758544,\n",
       "  4.2294398784637455,\n",
       "  3.6056071639060976,\n",
       "  3.599914050102234,\n",
       "  3.2621630787849427,\n",
       "  3.282893204689026,\n",
       "  2.8166802287101746,\n",
       "  3.217094564437866,\n",
       "  3.470629930496216,\n",
       "  3.892892861366272,\n",
       "  3.230195331573486,\n",
       "  3.802329659461975,\n",
       "  3.9928800106048583,\n",
       "  3.818475914001465,\n",
       "  2.850970482826233,\n",
       "  3.87358763217926,\n",
       "  3.7164202451705934,\n",
       "  3.8337072372436523,\n",
       "  3.4695577144622805,\n",
       "  3.419894003868103,\n",
       "  3.8047503232955933,\n",
       "  3.6587989568710326,\n",
       "  3.6714382648468016,\n",
       "  3.737077474594116,\n",
       "  3.8874285221099854,\n",
       "  3.5045246124267577,\n",
       "  3.981335926055908,\n",
       "  4.237629365921021,\n",
       "  3.8574837923049925,\n",
       "  3.22615647315979,\n",
       "  3.444275712966919,\n",
       "  2.612563097476959,\n",
       "  3.25662248134613,\n",
       "  3.024567699432373,\n",
       "  3.2567196011543276,\n",
       "  3.1512805700302122,\n",
       "  3.614047038555145,\n",
       "  3.4420628190040587,\n",
       "  2.626998269557953,\n",
       "  3.154823327064514,\n",
       "  3.7388691902160645,\n",
       "  3.4235889196395872,\n",
       "  3.765421438217163,\n",
       "  3.2529919385910033,\n",
       "  3.520415496826172,\n",
       "  3.7068488121032717,\n",
       "  3.6273008823394775,\n",
       "  3.2873350739479066,\n",
       "  3.2222601652145384,\n",
       "  3.1279052257537843,\n",
       "  4.2610520124435425,\n",
       "  3.7197913408279417,\n",
       "  4.019343543052673,\n",
       "  4.295186233520508,\n",
       "  3.179428815841675,\n",
       "  3.2179012537002563,\n",
       "  3.2712592124938964,\n",
       "  3.4878901481628417,\n",
       "  4.051962375640869,\n",
       "  3.7322402238845824,\n",
       "  3.579692506790161,\n",
       "  3.319094681739807,\n",
       "  3.343688356876373,\n",
       "  2.8486900687217713,\n",
       "  2.5710196495056152,\n",
       "  2.599786841869354,\n",
       "  3.270298457145691,\n",
       "  3.510992693901062,\n",
       "  3.469572973251343,\n",
       "  3.088328647613525,\n",
       "  3.84097580909729,\n",
       "  3.4333993673324583,\n",
       "  3.11425496339798,\n",
       "  3.786226725578308,\n",
       "  2.9859795570373535,\n",
       "  2.511073088645935,\n",
       "  3.6852692127227784,\n",
       "  2.8542897939682006,\n",
       "  3.2015527367591856,\n",
       "  3.6747567892074584,\n",
       "  3.1923893213272097,\n",
       "  2.821640157699585,\n",
       "  3.5051353693008425,\n",
       "  3.658673906326294,\n",
       "  2.913317060470581,\n",
       "  3.907836842536926,\n",
       "  3.440621829032898,\n",
       "  3.7172206163406374,\n",
       "  2.933862769603729,\n",
       "  4.09096018075943,\n",
       "  3.076060676574707,\n",
       "  3.5933308601379395,\n",
       "  4.335296964645385,\n",
       "  2.8546637654304505,\n",
       "  3.359754240512848,\n",
       "  4.215202593803406,\n",
       "  3.721810221672058,\n",
       "  3.8199430465698243,\n",
       "  3.206312894821167,\n",
       "  3.444230055809021,\n",
       "  3.265868353843689,\n",
       "  2.8382599115371705,\n",
       "  3.1543143033981322,\n",
       "  3.186747145652771,\n",
       "  3.226300764083862,\n",
       "  2.920857548713684,\n",
       "  3.093718409538269,\n",
       "  3.41359658241272,\n",
       "  3.784549593925476,\n",
       "  3.275448179244995,\n",
       "  3.0014425039291384,\n",
       "  2.44591600894928,\n",
       "  2.662738525867462,\n",
       "  2.6129658818244934,\n",
       "  3.3357019901275633,\n",
       "  2.7435449838638304,\n",
       "  2.7864405035972597,\n",
       "  3.0611573696136474,\n",
       "  3.3170314908027647,\n",
       "  2.6065048813819884,\n",
       "  3.925665593147278,\n",
       "  3.617333984375,\n",
       "  3.2995557308197023,\n",
       "  2.7206538319587708,\n",
       "  2.556174159049988,\n",
       "  2.752163088321686,\n",
       "  3.300316262245178,\n",
       "  3.109658980369568,\n",
       "  2.966306966543198,\n",
       "  2.86512234210968,\n",
       "  2.9013949394226075,\n",
       "  2.755355620384216,\n",
       "  2.954274129867554,\n",
       "  3.0192139387130736,\n",
       "  3.2618376493453978,\n",
       "  3.6680740833282472,\n",
       "  3.7720351696014403,\n",
       "  3.5064032077789307,\n",
       "  3.3854636430740355,\n",
       "  3.7592667818069456,\n",
       "  4.132475280761719,\n",
       "  3.917150926589966,\n",
       "  3.78077495098114,\n",
       "  3.0108978509902955,\n",
       "  2.7727380752563477,\n",
       "  4.06697678565979,\n",
       "  3.2704232215881346,\n",
       "  2.6030598521232604,\n",
       "  3.0139927744865416,\n",
       "  3.0279106855392457,\n",
       "  2.7991021633148194,\n",
       "  2.8857206106185913,\n",
       "  3.208611845970154,\n",
       "  3.3339420318603517,\n",
       "  3.5908216714859007,\n",
       "  2.9944324016571047,\n",
       "  3.8967594623565676,\n",
       "  3.5782363176345826,\n",
       "  2.7319143533706667,\n",
       "  3.351521134376526,\n",
       "  3.5281632423400877,\n",
       "  3.003395140171051,\n",
       "  3.83142991065979,\n",
       "  3.533261775970459,\n",
       "  3.054473173618317,\n",
       "  3.5409411907196047,\n",
       "  2.6883276224136354,\n",
       "  3.1880579471588133,\n",
       "  3.002571868896484,\n",
       "  3.0746596813201905,\n",
       "  3.264537501335144,\n",
       "  3.383221983909607,\n",
       "  3.809396195411682,\n",
       "  3.4298930406570434,\n",
       "  2.3568601846694945,\n",
       "  3.236647295951843,\n",
       "  3.3596540212631227,\n",
       "  2.7300001621246337,\n",
       "  3.0863683462142943,\n",
       "  3.272115969657898,\n",
       "  2.6901078462600707,\n",
       "  2.8539299726486207,\n",
       "  3.114579510688782,\n",
       "  2.6819265127182006,\n",
       "  2.9658310651779174,\n",
       "  3.7151663064956666,\n",
       "  3.6617370367050173,\n",
       "  3.1168603777885435,\n",
       "  2.3785956621170046,\n",
       "  2.953006315231323,\n",
       "  3.3815850019454956,\n",
       "  2.7019635319709776,\n",
       "  3.5073010444641115,\n",
       "  2.95275297164917,\n",
       "  2.938809370994568,\n",
       "  2.3496891260147095,\n",
       "  2.7584672570228577,\n",
       "  2.829382073879242,\n",
       "  3.2169865131378175,\n",
       "  3.423876714706421,\n",
       "  2.608568346500397,\n",
       "  3.859182333946228,\n",
       "  3.9855838060379027,\n",
       "  3.9055955171585084,\n",
       "  4.105435848236084,\n",
       "  3.586670088768005,\n",
       "  3.103006196022034,\n",
       "  2.880623936653137,\n",
       "  3.0769107818603514,\n",
       "  3.9459099769592285,\n",
       "  3.776651346683502,\n",
       "  2.7817612648010255,\n",
       "  3.673703932762146,\n",
       "  2.5890751481056213,\n",
       "  3.1596307277679445,\n",
       "  3.5185550451278687,\n",
       "  3.5172794103622436,\n",
       "  2.6865278244018556,\n",
       "  2.84396755695343,\n",
       "  3.2209001898765566,\n",
       "  2.6471198320388796,\n",
       "  3.584142827987671,\n",
       "  3.498198366165161,\n",
       "  2.8034476399421693,\n",
       "  2.983728277683258,\n",
       "  3.603167772293091,\n",
       "  3.8219321727752686,\n",
       "  4.0646239757537845,\n",
       "  3.759883451461792,\n",
       "  3.7290457487106323,\n",
       "  2.7632335424423218,\n",
       "  3.217337393760681,\n",
       "  2.9818570613861084,\n",
       "  3.309086036682129,\n",
       "  2.956259775161743,\n",
       "  3.06415114402771,\n",
       "  3.612948274612427,\n",
       "  2.8027545690536497,\n",
       "  2.8733271598815917,\n",
       "  3.2552692174911497,\n",
       "  3.356403851509094,\n",
       "  3.385603594779968,\n",
       "  3.422776532173157,\n",
       "  2.5853724002838137,\n",
       "  2.9808719158172607,\n",
       "  4.4581342220306395,\n",
       "  3.0061738014221193,\n",
       "  3.191427230834961,\n",
       "  3.1746238470077515,\n",
       "  3.8697190046310426,\n",
       "  2.440479838848114,\n",
       "  2.541220557689667,\n",
       "  3.47760865688324,\n",
       "  2.948784852027893,\n",
       "  3.4967519998550416,\n",
       "  3.41191771030426,\n",
       "  3.653906774520874,\n",
       "  2.8449381589889526,\n",
       "  3.1775796175003053,\n",
       "  3.7203334093093874,\n",
       "  3.476335334777832,\n",
       "  3.7444092988967896,\n",
       "  4.27250862121582,\n",
       "  3.816312885284424,\n",
       "  3.3027743339538573,\n",
       "  3.8458298444747925,\n",
       "  3.6965161085128786,\n",
       "  3.2381500720977785,\n",
       "  3.207963252067566,\n",
       "  3.4446919679641725,\n",
       "  2.8075844049453735,\n",
       "  3.2935118198394777,\n",
       "  3.397873783111572,\n",
       "  3.7237958192825316,\n",
       "  3.256677269935608,\n",
       "  3.7301430702209473,\n",
       "  3.7137039422988893,\n",
       "  2.74041268825531,\n",
       "  2.874640703201294,\n",
       "  2.806974542140961,\n",
       "  2.265519917011261,\n",
       "  3.1688717126846315,\n",
       "  3.274421286582947,\n",
       "  3.867373299598694,\n",
       "  3.082638454437256,\n",
       "  3.3003320693969727,\n",
       "  2.6617000818252565,\n",
       "  4.318066167831421,\n",
       "  3.3421387910842895,\n",
       "  3.388247883319855,\n",
       "  4.088941264152527,\n",
       "  2.9015204668045045,\n",
       "  3.218236875534058,\n",
       "  2.673262321949005,\n",
       "  3.0487714767456056,\n",
       "  3.8385234594345095,\n",
       "  2.9885127425193785,\n",
       "  3.516320085525513,\n",
       "  2.9843512058258055,\n",
       "  3.2597293615341187,\n",
       "  2.703978180885315,\n",
       "  3.4863319039344787,\n",
       "  3.497132086753845,\n",
       "  3.1125421285629273,\n",
       "  2.7465120434761046,\n",
       "  3.3588284969329836,\n",
       "  2.8223594665527343,\n",
       "  3.849175238609314,\n",
       "  3.811970090866089,\n",
       "  3.212793254852295,\n",
       "  2.7324092388153076,\n",
       "  3.480728578567505,\n",
       "  2.781416821479797,\n",
       "  2.790480363368988,\n",
       "  2.8567092418670654,\n",
       "  3.133881390094757,\n",
       "  3.030697560310364,\n",
       "  2.871918058395386,\n",
       "  2.7780396699905396,\n",
       "  3.0888732433319093,\n",
       "  3.236459994316101,\n",
       "  3.2632251024246215,\n",
       "  2.791681718826294,\n",
       "  2.7805060029029844,\n",
       "  3.3250826597213745,\n",
       "  3.549895238876343,\n",
       "  3.887549114227295,\n",
       "  3.261574411392212,\n",
       "  2.9865598917007445,\n",
       "  3.6209731101989746,\n",
       "  3.2910722732543944,\n",
       "  3.575933575630188,\n",
       "  3.432523822784424,\n",
       "  3.5288017272949217,\n",
       "  3.403501272201538,\n",
       "  3.099368119239807,\n",
       "  2.9265695214271545,\n",
       "  3.5319873571395872,\n",
       "  3.999416184425354,\n",
       "  2.9826777935028077,\n",
       "  2.6722479104995727,\n",
       "  2.339428198337555,\n",
       "  2.9430206775665284,\n",
       "  3.5377421140670777,\n",
       "  3.5944761753082277,\n",
       "  2.958777117729187,\n",
       "  2.879703962802887,\n",
       "  2.647774910926819,\n",
       "  3.1062856197357176,\n",
       "  2.7182073831558227,\n",
       "  3.181860423088074,\n",
       "  3.70206618309021,\n",
       "  3.7086410760879516,\n",
       "  2.576064133644104,\n",
       "  2.597623682022095,\n",
       "  1.974885380268097,\n",
       "  3.0582818031311034,\n",
       "  3.3109798550605776,\n",
       "  3.491357684135437,\n",
       "  3.4803937792778017,\n",
       "  2.3152971386909487,\n",
       "  2.5508409261703493,\n",
       "  2.5232012033462525,\n",
       "  3.3105539083480835,\n",
       "  2.8235005736351013,\n",
       "  3.8455242395401,\n",
       "  3.2753144979476927,\n",
       "  3.8626953840255736,\n",
       "  3.2363385438919066,\n",
       "  3.0389870524406435,\n",
       "  3.209719944000244,\n",
       "  2.743776500225067,\n",
       "  2.57662433385849,\n",
       "  2.1534834265708924,\n",
       "  3.139296019077301,\n",
       "  3.0257505536079408,\n",
       "  3.158812952041626,\n",
       "  3.09330792427063,\n",
       "  2.5371415495872496,\n",
       "  3.0237141370773317,\n",
       "  2.984074568748474,\n",
       "  2.654294764995575,\n",
       "  2.0225512862205504,\n",
       "  2.50431512594223,\n",
       "  2.8878278970718383,\n",
       "  3.0314587116241456,\n",
       "  3.3666220903396606,\n",
       "  2.9973222494125364,\n",
       "  3.7271475911140444,\n",
       "  3.4685157775878905,\n",
       "  2.938113045692444,\n",
       "  2.8302012205123903,\n",
       "  2.4654744625091554,\n",
       "  2.7078569293022157,\n",
       "  2.949143171310425,\n",
       "  3.96990168094635,\n",
       "  3.5754327297210695,\n",
       "  3.414505648612976,\n",
       "  3.10759242773056,\n",
       "  3.6468206882476806,\n",
       "  3.3452142238616944,\n",
       "  2.6997083187103272,\n",
       "  2.8278971910476685,\n",
       "  3.624543237686157,\n",
       "  4.133639192581176,\n",
       "  3.7082122802734374,\n",
       "  3.196946358680725,\n",
       "  2.9279895067214965,\n",
       "  3.630544328689575,\n",
       "  3.302353858947754,\n",
       "  3.3344380617141725,\n",
       "  2.9517648816108704,\n",
       "  2.444348728656769,\n",
       "  2.44597190618515,\n",
       "  2.9388588190078737,\n",
       "  3.296728992462158,\n",
       "  2.6930103063583375,\n",
       "  2.6377090573310853,\n",
       "  2.614754855632782,\n",
       "  2.299923002719879,\n",
       "  2.3985485434532166,\n",
       "  3.310988187789917,\n",
       "  2.913269019126892,\n",
       "  3.874151659011841,\n",
       "  3.608295774459839,\n",
       "  2.544575595855713,\n",
       "  2.75663343667984,\n",
       "  2.789503288269043,\n",
       "  3.421921706199646,\n",
       "  3.3585573196411134,\n",
       "  2.7691864728927613,\n",
       "  2.516225814819336,\n",
       "  2.091794490814209,\n",
       "  3.299967122077942,\n",
       "  2.6857553243637087,\n",
       "  2.738759756088257,\n",
       "  3.8837846517562866,\n",
       "  3.909994959831238,\n",
       "  2.8280689477920533,\n",
       "  3.6809192657470704,\n",
       "  3.600476622581482,\n",
       "  2.680819058418274,\n",
       "  3.5053528785705566,\n",
       "  3.0186513662338257,\n",
       "  3.411482572555542,\n",
       "  4.427209281921387,\n",
       "  2.8237708449363708,\n",
       "  3.59008994102478,\n",
       "  3.0009363889694214,\n",
       "  2.954535722732544,\n",
       "  3.588278126716614,\n",
       "  2.7303022384643554,\n",
       "  2.973345160484314,\n",
       "  3.3358702659606934,\n",
       "  3.46078474521637,\n",
       "  3.568077635765076,\n",
       "  3.8175861120223997,\n",
       "  3.227678418159485,\n",
       "  3.3469052791595457,\n",
       "  3.5907443046569822,\n",
       "  3.0333454847335815,\n",
       "  2.8788694381713866,\n",
       "  3.0006352066993713,\n",
       "  2.867376685142517,\n",
       "  2.806183338165283,\n",
       "  3.2234640598297117,\n",
       "  3.870972418785095,\n",
       "  2.6680687427520753,\n",
       "  2.970422101020813,\n",
       "  2.7192151665687563,\n",
       "  3.0313051462173464,\n",
       "  2.6811595797538756,\n",
       "  2.833333134651184,\n",
       "  2.5196309328079223,\n",
       "  3.3329296827316286,\n",
       "  2.9500634908676147,\n",
       "  3.573483645915985,\n",
       "  3.46824187040329,\n",
       "  2.4919536232948305,\n",
       "  3.1033092498779298,\n",
       "  2.849270796775818,\n",
       "  3.937701153755188,\n",
       "  3.550237536430359,\n",
       "  2.6719828486442565,\n",
       "  3.8059308528900146,\n",
       "  3.519229865074158,\n",
       "  3.0358901500701903,\n",
       "  3.2758607149124144,\n",
       "  3.348929929733276,\n",
       "  2.8008095026016235,\n",
       "  3.320209765434265,\n",
       "  2.930886697769165,\n",
       "  3.513386344909668,\n",
       "  2.627734065055847,\n",
       "  3.4551500558853148,\n",
       "  3.0664361476898194,\n",
       "  2.8664791345596314,\n",
       "  3.25838987827301,\n",
       "  3.135266101360321,\n",
       "  2.8744625806808473,\n",
       "  2.653406095504761,\n",
       "  2.870886540412903,\n",
       "  2.776191222667694,\n",
       "  3.7144988059997557,\n",
       "  3.8159335374832155,\n",
       "  2.9768412113189697,\n",
       "  3.228861904144287,\n",
       "  2.879047656059265,\n",
       "  2.837959384918213,\n",
       "  2.268012058734894,\n",
       "  3.2272660732269287,\n",
       "  3.2618107318878176,\n",
       "  3.2328307151794435,\n",
       "  3.3124025106430053,\n",
       "  3.0229490995407104,\n",
       "  3.1644944190979003,\n",
       "  3.007719838619232,\n",
       "  2.543969917297363,\n",
       "  3.021125340461731,\n",
       "  3.411665511131287,\n",
       "  2.959290659427643,\n",
       "  2.505235028266907,\n",
       "  2.9916727542877197,\n",
       "  2.559630823135376,\n",
       "  2.4545842170715333,\n",
       "  3.1911313772201537,\n",
       "  2.947685217857361,\n",
       "  2.608106255531311,\n",
       "  3.218164587020874,\n",
       "  3.7312237501144407,\n",
       "  3.0659626603126524,\n",
       "  2.5240281343460085,\n",
       "  3.624318981170654,\n",
       "  3.7119781970977783,\n",
       "  2.9969508409500123,\n",
       "  2.632907176017761,\n",
       "  2.8677838325500487,\n",
       "  2.6876230120658873,\n",
       "  2.8298174858093263,\n",
       "  2.876295745372772,\n",
       "  3.306891441345215,\n",
       "  2.391542446613312,\n",
       "  2.555051851272583,\n",
       "  3.1689857840538025,\n",
       "  2.616735017299652,\n",
       "  3.0771284341812133,\n",
       "  2.1047380566596985,\n",
       "  3.346065640449524,\n",
       "  2.7289410829544067,\n",
       "  3.1387530326843263,\n",
       "  2.913319802284241,\n",
       "  3.1885122537612913,\n",
       "  3.4595836639404296,\n",
       "  3.247909116744995,\n",
       "  2.821155333518982,\n",
       "  3.344346058368683,\n",
       "  2.461383068561554,\n",
       "  2.6862812876701354,\n",
       "  3.087931180000305,\n",
       "  3.3188525319099424,\n",
       "  2.731231319904327,\n",
       "  2.4517094373703,\n",
       "  3.5208441495895384,\n",
       "  2.60831937789917,\n",
       "  2.7196213960647584,\n",
       "  2.7215200901031493,\n",
       "  2.663282573223114,\n",
       "  2.8436259508132933,\n",
       "  2.478045105934143,\n",
       "  3.253059458732605,\n",
       "  3.0290496826171873,\n",
       "  3.101166582107544,\n",
       "  3.3979063749313356,\n",
       "  2.4563855171203612,\n",
       "  2.588749659061432,\n",
       "  2.532754623889923,\n",
       "  2.941235160827637,\n",
       "  3.1921520471572875,\n",
       "  2.9352814674377443,\n",
       "  3.3200039863586426,\n",
       "  2.758941316604614,\n",
       "  2.963333523273468,\n",
       "  4.078572416305542,\n",
       "  2.8793516874313356,\n",
       "  3.222768497467041,\n",
       "  3.0186029434204102,\n",
       "  3.5130282640457153,\n",
       "  2.2614169120788574,\n",
       "  3.516589093208313,\n",
       "  3.0459433794021606,\n",
       "  2.467964839935303,\n",
       "  3.165610432624817,\n",
       "  3.2822510480880736,\n",
       "  3.3289701342582703,\n",
       "  3.5256532311439512,\n",
       "  3.180971097946167,\n",
       "  3.16038920879364,\n",
       "  2.963757872581482,\n",
       "  2.461501896381378,\n",
       "  2.833945870399475,\n",
       "  2.708456349372864,\n",
       "  3.237332272529602,\n",
       "  3.508227598667145,\n",
       "  2.9784158945083616,\n",
       "  3.6442758798599244,\n",
       "  3.3908463954925536,\n",
       "  2.8777633190155028,\n",
       "  3.0469745635986327,\n",
       "  3.57857825756073,\n",
       "  2.9680809497833254,\n",
       "  3.9329460144042967,\n",
       "  2.872867727279663,\n",
       "  2.874405574798584,\n",
       "  3.918675446510315,\n",
       "  3.0774516105651855,\n",
       "  3.4334527015686036,\n",
       "  3.191770648956299,\n",
       "  3.3250592231750487,\n",
       "  3.650974154472351,\n",
       "  3.4534088373184204,\n",
       "  3.6276373624801637,\n",
       "  3.128162455558777,\n",
       "  2.8000215530395507,\n",
       "  2.9485570669174193,\n",
       "  2.5082416772842406,\n",
       "  3.0777347326278686,\n",
       "  2.44762636423111,\n",
       "  3.1801915645599363,\n",
       "  2.8127050042152404,\n",
       "  3.2988119602203367,\n",
       "  2.8995041847229004,\n",
       "  2.4866507291793822,\n",
       "  3.417469024658203,\n",
       "  ...],\n",
       " [6.20669487118721,\n",
       "  5.82707741856575,\n",
       "  5.706403583288193,\n",
       "  5.6296074986457825,\n",
       "  5.49046716094017,\n",
       "  5.368061989545822,\n",
       "  5.364450618624687,\n",
       "  5.236259192228317,\n",
       "  5.1108783185482025,\n",
       "  5.1018587201833725,\n",
       "  5.0288911908864975,\n",
       "  5.058780327439308,\n",
       "  4.926200687885284,\n",
       "  4.919364228844643,\n",
       "  4.8019727021455765,\n",
       "  4.793634429574013,\n",
       "  4.747813895344734,\n",
       "  4.693734630942345,\n",
       "  4.635618180036545,\n",
       "  4.634697690606117,\n",
       "  4.624984934926033,\n",
       "  4.622199982404709,\n",
       "  4.56707526743412,\n",
       "  4.553176671266556,\n",
       "  4.471227616071701,\n",
       "  4.440236046910286,\n",
       "  4.474855601787567,\n",
       "  4.432888880372047,\n",
       "  4.366241872310638,\n",
       "  4.312155485153198,\n",
       "  4.385612308979034,\n",
       "  4.2893118262290955,\n",
       "  4.2798310071229935,\n",
       "  4.265285894274712,\n",
       "  4.209589645266533,\n",
       "  4.17549541592598,\n",
       "  4.140173062682152,\n",
       "  4.125817775726318,\n",
       "  4.137291327118874,\n",
       "  4.102877840399742,\n",
       "  4.1526515781879425,\n",
       "  4.161893412470818,\n",
       "  4.123653218150139,\n",
       "  4.086199849843979,\n",
       "  4.035862594842911,\n",
       "  4.054976835846901,\n",
       "  4.067499220371246,\n",
       "  4.012132704257965,\n",
       "  4.062786176800728,\n",
       "  4.027566701173782,\n",
       "  3.942203640937805,\n",
       "  3.8877224028110504,\n",
       "  3.879753068089485,\n",
       "  3.914387509226799,\n",
       "  3.8735688030719757,\n",
       "  3.8598846793174744,\n",
       "  3.8350184857845306,\n",
       "  3.8750905245542526,\n",
       "  3.879962220788002,\n",
       "  3.8454429358243942,\n",
       "  3.780254766345024,\n",
       "  3.8413891047239304,\n",
       "  3.778399184346199,\n",
       "  3.7852677553892136,\n",
       "  3.7643150985240936,\n",
       "  3.7208905816078186,\n",
       "  3.7552481293678284,\n",
       "  3.753164678812027,\n",
       "  3.7647484987974167,\n",
       "  3.7212038934230804,\n",
       "  3.7040560990571976,\n",
       "  3.966831848025322,\n",
       "  3.7791301906108856,\n",
       "  3.7604445070028305,\n",
       "  3.745751053094864,\n",
       "  3.752886950969696,\n",
       "  3.657158389687538,\n",
       "  3.6762066185474396,\n",
       "  3.6670339703559875,\n",
       "  3.766734704375267,\n",
       "  3.698106959462166,\n",
       "  3.613343670964241,\n",
       "  3.653208151459694,\n",
       "  3.6187217980623245,\n",
       "  3.6325189620256424,\n",
       "  3.5984328389167786,\n",
       "  3.621328890323639,\n",
       "  3.6330484449863434,\n",
       "  3.6379052102565765,\n",
       "  3.6608946323394775,\n",
       "  3.5829802751541138,\n",
       "  3.58600389957428,\n",
       "  3.6062323600053787,\n",
       "  3.5843693166971207,\n",
       "  3.6125832945108414,\n",
       "  3.6441029459238052,\n",
       "  3.5578311681747437,\n",
       "  3.548954799771309,\n",
       "  3.555711343884468,\n",
       "  3.575316220521927,\n",
       "  3.5547880828380585,\n",
       "  3.576553553342819,\n",
       "  3.547426849603653,\n",
       "  3.544395998120308,\n",
       "  3.572984129190445,\n",
       "  3.529541537165642,\n",
       "  3.503434330224991,\n",
       "  3.4381678998470306,\n",
       "  3.534460559487343,\n",
       "  3.5528545677661896,\n",
       "  3.6097270399332047,\n",
       "  3.5274238288402557,\n",
       "  3.469531700015068,\n",
       "  3.5176604986190796,\n",
       "  3.399723321199417,\n",
       "  3.468080058693886,\n",
       "  3.451411262154579,\n",
       "  3.5094600319862366,\n",
       "  3.5085418224334717,\n",
       "  3.4001218527555466,\n",
       "  3.413950502872467,\n",
       "  3.4211275428533554,\n",
       "  3.457005023956299,\n",
       "  3.3755923956632614,\n",
       "  3.3987803608179092,\n",
       "  3.3453063666820526,\n",
       "  3.438063234090805,\n",
       "  3.3661227375268936,\n",
       "  3.3718218952417374,\n",
       "  3.3484296202659607,\n",
       "  3.4116818606853485,\n",
       "  3.419915124773979,\n",
       "  3.339851528406143,\n",
       "  3.381575107574463,\n",
       "  3.3556830883026123,\n",
       "  3.36529441177845,\n",
       "  3.3758962899446487,\n",
       "  3.352043002843857,\n",
       "  3.300167143344879,\n",
       "  3.337491661310196,\n",
       "  3.2984219789505005,\n",
       "  3.3410166054964066,\n",
       "  3.298577845096588,\n",
       "  3.2868190854787827,\n",
       "  3.305260330438614,\n",
       "  3.396641969680786,\n",
       "  3.3633617907762527,\n",
       "  3.28429739177227,\n",
       "  3.274611532688141,\n",
       "  3.258401021361351,\n",
       "  3.305695354938507,\n",
       "  3.2424093782901764,\n",
       "  3.2806336581707,\n",
       "  3.255690038204193,\n",
       "  3.246890962123871,\n",
       "  3.2731223553419113,\n",
       "  3.2757970094680786,\n",
       "  3.223290830850601,\n",
       "  3.302795931696892,\n",
       "  3.2501230686903,\n",
       "  3.374670624732971,\n",
       "  3.2574258148670197,\n",
       "  3.2379342317581177,\n",
       "  3.2390122562646866,\n",
       "  3.2543426901102066,\n",
       "  3.201165556907654,\n",
       "  3.222848355770111,\n",
       "  3.2246572971343994,\n",
       "  3.24191090464592,\n",
       "  3.3088153153657913,\n",
       "  3.194988951086998,\n",
       "  3.207980141043663,\n",
       "  3.172477439045906,\n",
       "  3.2447011321783066,\n",
       "  3.2285720854997635,\n",
       "  3.2485377937555313,\n",
       "  3.2068126648664474,\n",
       "  3.1975355744361877,\n",
       "  3.179810017347336,\n",
       "  3.2279950380325317,\n",
       "  3.1788115203380585,\n",
       "  3.1695124953985214,\n",
       "  3.2740827202796936,\n",
       "  3.185599073767662,\n",
       "  3.2478135228157043,\n",
       "  3.190542906522751,\n",
       "  3.2308741211891174,\n",
       "  3.186180979013443,\n",
       "  3.199427992105484,\n",
       "  3.164902985095978,\n",
       "  3.2030899077653885,\n",
       "  3.1938794404268265,\n",
       "  3.192520022392273,\n",
       "  3.169459193944931,\n",
       "  3.1502745151519775,\n",
       "  3.2070489525794983,\n",
       "  3.2073233872652054,\n",
       "  3.1813023537397385,\n",
       "  3.1452620327472687,\n",
       "  3.1394693851470947,\n",
       "  3.171062797307968,\n",
       "  3.149708718061447,\n",
       "  3.2249539345502853,\n",
       "  3.1807111650705338,\n",
       "  3.1798201352357864,\n",
       "  3.1846871376037598,\n",
       "  3.1420344561338425,\n",
       "  3.194119915366173,\n",
       "  3.182133600115776,\n",
       "  3.149434521794319,\n",
       "  3.143886223435402,\n",
       "  3.1995227485895157,\n",
       "  3.150537073612213,\n",
       "  3.1106250286102295,\n",
       "  3.1499648690223694,\n",
       "  3.092394530773163,\n",
       "  3.1393393725156784,\n",
       "  3.0814592093229294,\n",
       "  3.133439004421234,\n",
       "  3.121959298849106,\n",
       "  3.1178905963897705,\n",
       "  3.076442450284958,\n",
       "  3.1083934754133224,\n",
       "  3.1139575392007828,\n",
       "  3.101162701845169,\n",
       "  3.131287232041359,\n",
       "  3.1275510787963867,\n",
       "  3.1291512101888657,\n",
       "  3.1073285341262817,\n",
       "  3.1161325573921204,\n",
       "  3.130431830883026,\n",
       "  3.10529725253582,\n",
       "  3.07722470164299,\n",
       "  3.090879037976265,\n",
       "  3.0689879208803177,\n",
       "  3.095559537410736,\n",
       "  3.1001902669668198,\n",
       "  3.0822958201169968,\n",
       "  3.0991034358739853,\n",
       "  3.1327138543128967,\n",
       "  3.1234770119190216,\n",
       "  3.151403531432152,\n",
       "  3.104274347424507,\n",
       "  3.1242758333683014,\n",
       "  3.082755744457245,\n",
       "  3.1429605334997177,\n",
       "  3.096360832452774,\n",
       "  3.1480538696050644,\n",
       "  3.1605694591999054,\n",
       "  3.119126573204994,\n",
       "  3.063839480280876,\n",
       "  3.0885423570871353,\n",
       "  3.084215745329857,\n",
       "  3.1409777104854584,\n",
       "  3.0631382018327713,\n",
       "  3.0617976933717728,\n",
       "  3.0502324998378754,\n",
       "  3.099218413233757,\n",
       "  3.061949610710144,\n",
       "  3.0823945850133896,\n",
       "  3.0456051379442215,\n",
       "  3.0455487817525864,\n",
       "  3.0365510433912277,\n",
       "  3.0653936713933945,\n",
       "  3.0667572170495987,\n",
       "  3.061041221022606,\n",
       "  3.0525743067264557,\n",
       "  3.0657509863376617,\n",
       "  3.0642824470996857,\n",
       "  3.1405849754810333,\n",
       "  3.096053436398506,\n",
       "  3.063965395092964,\n",
       "  3.108493000268936,\n",
       "  3.013167604804039,\n",
       "  3.091694474220276,\n",
       "  3.1294174641370773,\n",
       "  3.0679946541786194,\n",
       "  3.0443949699401855,\n",
       "  3.087534785270691,\n",
       "  3.095935568213463,\n",
       "  3.0167452841997147,\n",
       "  3.0083726942539215,\n",
       "  3.0549279153347015,\n",
       "  3.0225993245840073,\n",
       "  3.0251874178647995,\n",
       "  3.041827529668808,\n",
       "  3.0205393731594086,\n",
       "  3.0522589832544327,\n",
       "  2.995901718735695,\n",
       "  3.006972372531891,\n",
       "  3.0022777915000916,\n",
       "  3.0423108637332916,\n",
       "  3.0262183845043182,\n",
       "  3.010612428188324,\n",
       "  2.9910338073968887,\n",
       "  3.093944326043129,\n",
       "  3.0515688061714172,\n",
       "  3.068926587700844,\n",
       "  2.9948412477970123,\n",
       "  2.9709383994340897,\n",
       "  3.018558830022812,\n",
       "  2.984754964709282,\n",
       "  3.028635188937187,\n",
       "  3.0042820870876312,\n",
       "  2.981365606188774,\n",
       "  2.977692633867264,\n",
       "  2.9966818392276764,\n",
       "  3.010352686047554,\n",
       "  2.958923652768135,\n",
       "  2.9964683651924133,\n",
       "  3.00015589594841,\n",
       "  3.0905962586402893,\n",
       "  2.9804615527391434,\n",
       "  2.9737022072076797,\n",
       "  3.001660853624344,\n",
       "  2.996869131922722,\n",
       "  2.9974363148212433,\n",
       "  3.0028724372386932,\n",
       "  2.9848884493112564,\n",
       "  3.0865676552057266,\n",
       "  3.011622801423073,\n",
       "  2.9519907236099243,\n",
       "  2.971312075853348,\n",
       "  3.0072449296712875,\n",
       "  3.003022477030754,\n",
       "  3.0824041217565536,\n",
       "  3.0034462362527847,\n",
       "  3.0359310805797577,\n",
       "  2.9943600594997406,\n",
       "  3.0617072135210037,\n",
       "  3.0031740069389343,\n",
       "  2.959697589278221,\n",
       "  3.0952801555395126,\n",
       "  3.0110977590084076,\n",
       "  3.016198366880417,\n",
       "  2.9839425534009933,\n",
       "  2.9972048848867416,\n",
       "  2.9876081496477127,\n",
       "  2.9397833943367004,\n",
       "  3.003623843193054,\n",
       "  2.9578996747732162,\n",
       "  2.993731811642647,\n",
       "  3.0183507055044174,\n",
       "  3.0243383795022964,\n",
       "  3.020973816514015,\n",
       "  2.9993694722652435,\n",
       "  2.9768090546131134,\n",
       "  3.0105265378952026,\n",
       "  2.9500615000724792,\n",
       "  2.989829942584038,\n",
       "  2.9305152222514153,\n",
       "  2.939428001642227,\n",
       "  2.976539894938469,\n",
       "  2.9756258577108383,\n",
       "  3.028585895895958,\n",
       "  2.944959059357643,\n",
       "  3.0589137077331543,\n",
       "  2.9618230015039444,\n",
       "  2.982996702194214,\n",
       "  2.9488947838544846,\n",
       "  3.0920532941818237,\n",
       "  2.983272448182106,\n",
       "  3.081336423754692,\n",
       "  2.9714850038290024,\n",
       "  2.966161921620369,\n",
       "  2.9687290713191032,\n",
       "  2.9573660641908646,\n",
       "  2.9638509303331375,\n",
       "  2.9581237137317657,\n",
       "  2.9358055740594864,\n",
       "  2.9274170994758606,\n",
       "  2.9469470977783203,\n",
       "  2.937442108988762,\n",
       "  2.9472789019346237,\n",
       "  2.9320184513926506,\n",
       "  2.9568909108638763,\n",
       "  2.927255615592003,\n",
       "  2.953751415014267,\n",
       "  3.021030932664871,\n",
       "  3.006291389465332,\n",
       "  2.9930376559495926,\n",
       "  3.1608155220746994,\n",
       "  3.0299509316682816,\n",
       "  2.9339507818222046,\n",
       "  2.9798830151557922,\n",
       "  2.9409018754959106,\n",
       "  2.9382031112909317,\n",
       "  2.9821660220623016,\n",
       "  2.956791490316391,\n",
       "  2.9843219220638275,\n",
       "  2.9650032073259354,\n",
       "  2.9353890120983124,\n",
       "  2.928704082965851,\n",
       "  2.940273031592369,\n",
       "  2.945846438407898,\n",
       "  2.9536724537611008,\n",
       "  2.9262140691280365,\n",
       "  2.914672389626503,\n",
       "  2.9919933527708054,\n",
       "  2.9091263487935066,\n",
       "  2.9776716232299805,\n",
       "  2.9277753084897995,\n",
       "  2.92505843937397,\n",
       "  2.912770688533783,\n",
       "  2.9468709230422974,\n",
       "  2.913339748978615,\n",
       "  2.88418447971344,\n",
       "  3.023178532719612,\n",
       "  2.9894945323467255,\n",
       "  2.9441204220056534,\n",
       "  2.970588967204094,\n",
       "  2.9171559810638428,\n",
       "  2.909160539507866,\n",
       "  2.905578300356865,\n",
       "  2.965473562479019,\n",
       "  2.9401803761720657,\n",
       "  2.9243710190057755,\n",
       "  2.9000848084688187,\n",
       "  2.915135905146599,\n",
       "  2.9001583084464073,\n",
       "  2.9383162558078766,\n",
       "  2.947821408510208,\n",
       "  2.9529459178447723,\n",
       "  2.9756062775850296,\n",
       "  2.912103235721588,\n",
       "  2.937650367617607,\n",
       "  2.904084749519825],\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     lr: 0.003\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ae.train_batches(\n",
    "    batches_train=batches_train,\n",
    "    batches_test=batches_dev,\n",
    "    epochs=-1, \n",
    "    lr=3e-3, \n",
    "    print_every=10,\n",
    "    test_every=30,\n",
    "    patience=20,\n",
    "    min_improvement=.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lrcv10_acc': 0.6932877694806564, 'tree10_acc': 0.6740496983504197}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_test = it.RestartableBatchIterator(list(idx_test), batch_size*4)\n",
    "batches_test = it.RestartableMapIterator(batches_test, lambda batch: T(batch).long().transpose(0, 1))\n",
    "\n",
    "embs_test=[]\n",
    "for batch in batches_test: \n",
    "    batch = emb_layer(batch)\n",
    "    with torch.no_grad(): \n",
    "        model_ae.eval()\n",
    "        batch = model_ae.encoder(batch)\n",
    "    embs_test.append(batch)\n",
    "embs_test = torch.vstack(embs_test)\n",
    "embs_test.shape, len(simple_targets_test)\n",
    "\n",
    "batches_dev = it.RestartableBatchIterator(list(idx_dev), batch_size*4)\n",
    "batches_dev = it.RestartableMapIterator(batches_dev, lambda batch: T(batch).long().transpose(0, 1))\n",
    "\n",
    "embs_dev=[]\n",
    "for batch in batches_dev: \n",
    "    batch = emb_layer(batch)\n",
    "    with torch.no_grad(): \n",
    "        model_ae.eval()\n",
    "        batch = model_ae.encoder(batch)\n",
    "    embs_dev.append(batch)\n",
    "embs_dev = torch.vstack(embs_dev)\n",
    "embs_dev.shape, len(simple_targets_dev)\n",
    "\n",
    "result={}\n",
    "for clf_name, clf_init in classifiers.items():\n",
    "    clf = clf_init()\n",
    "    clf.fit(embs_dev[:n_samples], simple_targets_dev[:n_samples])\n",
    "    preds = clf.predict(embs_test)\n",
    "    acc = accuracy_score(simple_targets_test, preds)\n",
    "    f1_micro = f1_score(simple_targets_test, preds, average='micro')\n",
    "    f1_macro = f1_score(simple_targets_test, preds, average='macro')\n",
    "    #print(f'{clf_name}\\tacc={acc}\\tf1_micro={f1_micro}\\tf1_macro={f1_macro}')\n",
    "    result[f'{clf_name}_acc'] = acc\n",
    "    #result[f'{clf_name}_f1ma'] = f1_macro\n",
    "    #result[f'{clf_name}_f1mi'] = f1_micro\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8f5a8577b8cf413169496226e4485bf91d05ebdcdf9e1588db361085a732ed9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ml_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
